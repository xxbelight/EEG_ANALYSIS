{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd71aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor \n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\"\"\"k fold 대신 stratified k fold\"\"\"\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import itertools\n",
    "#from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import sys\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "import torch.utils.data as td\n",
    "#import utilities\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.distributions import Gamma\n",
    "from utilities import log_sum_exp\n",
    "import math\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from utilities import R2_value, log_sum_exp, parameters_to_vector, vector_to_parameters, getOpt\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR, ReduceLROnPlateau\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "#bayesean Tabnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa30904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1a3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7af249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>delta_E1</th>\n",
       "      <th>delta_E10</th>\n",
       "      <th>delta_E100</th>\n",
       "      <th>delta_E101</th>\n",
       "      <th>delta_E102</th>\n",
       "      <th>delta_E103</th>\n",
       "      <th>delta_E104</th>\n",
       "      <th>delta_E105</th>\n",
       "      <th>delta_E106</th>\n",
       "      <th>...</th>\n",
       "      <th>gamma_E91</th>\n",
       "      <th>gamma_E92</th>\n",
       "      <th>gamma_E93</th>\n",
       "      <th>gamma_E94</th>\n",
       "      <th>gamma_E95</th>\n",
       "      <th>gamma_E96</th>\n",
       "      <th>gamma_E97</th>\n",
       "      <th>gamma_E98</th>\n",
       "      <th>gamma_E99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.186840</td>\n",
       "      <td>10.606450</td>\n",
       "      <td>44.688722</td>\n",
       "      <td>12.757568</td>\n",
       "      <td>4.279060</td>\n",
       "      <td>6.252396</td>\n",
       "      <td>7.404375</td>\n",
       "      <td>8.745515</td>\n",
       "      <td>8.742263</td>\n",
       "      <td>...</td>\n",
       "      <td>61.988657</td>\n",
       "      <td>29.291054</td>\n",
       "      <td>29.744932</td>\n",
       "      <td>136.190847</td>\n",
       "      <td>212.924658</td>\n",
       "      <td>81.294987</td>\n",
       "      <td>48.675846</td>\n",
       "      <td>24.726053</td>\n",
       "      <td>142.232636</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25.939918</td>\n",
       "      <td>19.104370</td>\n",
       "      <td>16.145237</td>\n",
       "      <td>13.580193</td>\n",
       "      <td>5.161970</td>\n",
       "      <td>11.025384</td>\n",
       "      <td>7.649788</td>\n",
       "      <td>9.213589</td>\n",
       "      <td>6.471280</td>\n",
       "      <td>...</td>\n",
       "      <td>23.900143</td>\n",
       "      <td>14.315390</td>\n",
       "      <td>15.506638</td>\n",
       "      <td>94.855550</td>\n",
       "      <td>59.283250</td>\n",
       "      <td>44.557439</td>\n",
       "      <td>16.220062</td>\n",
       "      <td>13.037046</td>\n",
       "      <td>65.670841</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>153.159664</td>\n",
       "      <td>15.066106</td>\n",
       "      <td>6.662611</td>\n",
       "      <td>5.487291</td>\n",
       "      <td>4.630674</td>\n",
       "      <td>4.649062</td>\n",
       "      <td>4.682885</td>\n",
       "      <td>4.240100</td>\n",
       "      <td>3.813276</td>\n",
       "      <td>...</td>\n",
       "      <td>44.393759</td>\n",
       "      <td>59.016298</td>\n",
       "      <td>33.794492</td>\n",
       "      <td>70.474366</td>\n",
       "      <td>78.776522</td>\n",
       "      <td>34.908377</td>\n",
       "      <td>61.082163</td>\n",
       "      <td>45.893154</td>\n",
       "      <td>93.348860</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7.089047</td>\n",
       "      <td>10.054340</td>\n",
       "      <td>25.154236</td>\n",
       "      <td>23.959194</td>\n",
       "      <td>14.029636</td>\n",
       "      <td>10.943584</td>\n",
       "      <td>8.306384</td>\n",
       "      <td>4.749801</td>\n",
       "      <td>1.830242</td>\n",
       "      <td>...</td>\n",
       "      <td>20.186578</td>\n",
       "      <td>11.595557</td>\n",
       "      <td>11.295946</td>\n",
       "      <td>64.130118</td>\n",
       "      <td>22.663702</td>\n",
       "      <td>21.716074</td>\n",
       "      <td>12.985209</td>\n",
       "      <td>11.825560</td>\n",
       "      <td>29.928711</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>33.236915</td>\n",
       "      <td>12.426685</td>\n",
       "      <td>6.418845</td>\n",
       "      <td>4.376580</td>\n",
       "      <td>4.513896</td>\n",
       "      <td>4.290389</td>\n",
       "      <td>4.220892</td>\n",
       "      <td>4.660313</td>\n",
       "      <td>6.759117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439690</td>\n",
       "      <td>0.102942</td>\n",
       "      <td>0.091856</td>\n",
       "      <td>0.881043</td>\n",
       "      <td>0.286805</td>\n",
       "      <td>0.377908</td>\n",
       "      <td>0.149718</td>\n",
       "      <td>0.116125</td>\n",
       "      <td>0.319130</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>54.506310</td>\n",
       "      <td>8.534871</td>\n",
       "      <td>6.765330</td>\n",
       "      <td>2.731705</td>\n",
       "      <td>3.034995</td>\n",
       "      <td>5.870074</td>\n",
       "      <td>6.415011</td>\n",
       "      <td>5.530279</td>\n",
       "      <td>3.704629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108132</td>\n",
       "      <td>0.068317</td>\n",
       "      <td>0.056253</td>\n",
       "      <td>0.195592</td>\n",
       "      <td>0.190453</td>\n",
       "      <td>0.134718</td>\n",
       "      <td>0.072268</td>\n",
       "      <td>0.050515</td>\n",
       "      <td>0.087241</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>35.139320</td>\n",
       "      <td>24.022022</td>\n",
       "      <td>13.486429</td>\n",
       "      <td>4.989185</td>\n",
       "      <td>3.973933</td>\n",
       "      <td>3.638945</td>\n",
       "      <td>3.372632</td>\n",
       "      <td>3.027413</td>\n",
       "      <td>7.088790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080603</td>\n",
       "      <td>0.081616</td>\n",
       "      <td>0.050210</td>\n",
       "      <td>0.103784</td>\n",
       "      <td>0.064158</td>\n",
       "      <td>0.065894</td>\n",
       "      <td>0.057960</td>\n",
       "      <td>0.051233</td>\n",
       "      <td>0.082560</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>9.798020</td>\n",
       "      <td>10.395767</td>\n",
       "      <td>30.070915</td>\n",
       "      <td>5.071613</td>\n",
       "      <td>3.062017</td>\n",
       "      <td>2.705522</td>\n",
       "      <td>2.427417</td>\n",
       "      <td>1.669092</td>\n",
       "      <td>1.556226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046448</td>\n",
       "      <td>0.035920</td>\n",
       "      <td>0.034157</td>\n",
       "      <td>0.075519</td>\n",
       "      <td>0.047879</td>\n",
       "      <td>0.043430</td>\n",
       "      <td>0.040736</td>\n",
       "      <td>0.032956</td>\n",
       "      <td>0.049081</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>21.128716</td>\n",
       "      <td>15.409268</td>\n",
       "      <td>4.162855</td>\n",
       "      <td>3.932925</td>\n",
       "      <td>3.936626</td>\n",
       "      <td>3.710372</td>\n",
       "      <td>3.884445</td>\n",
       "      <td>6.257363</td>\n",
       "      <td>9.081596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062187</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.057154</td>\n",
       "      <td>0.063969</td>\n",
       "      <td>0.070002</td>\n",
       "      <td>0.067328</td>\n",
       "      <td>0.064743</td>\n",
       "      <td>0.064991</td>\n",
       "      <td>0.222946</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>17.859299</td>\n",
       "      <td>3.056327</td>\n",
       "      <td>1.429758</td>\n",
       "      <td>1.264734</td>\n",
       "      <td>1.340887</td>\n",
       "      <td>0.758673</td>\n",
       "      <td>1.142326</td>\n",
       "      <td>1.081971</td>\n",
       "      <td>1.143361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068258</td>\n",
       "      <td>0.050393</td>\n",
       "      <td>0.021314</td>\n",
       "      <td>0.180978</td>\n",
       "      <td>0.148934</td>\n",
       "      <td>0.092323</td>\n",
       "      <td>0.070061</td>\n",
       "      <td>0.037965</td>\n",
       "      <td>0.175942</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 905 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0    delta_E1  delta_E10  delta_E100  delta_E101  delta_E102  \\\n",
       "0            0    8.186840  10.606450   44.688722   12.757568    4.279060   \n",
       "1            1   25.939918  19.104370   16.145237   13.580193    5.161970   \n",
       "2            2  153.159664  15.066106    6.662611    5.487291    4.630674   \n",
       "3            3    7.089047  10.054340   25.154236   23.959194   14.029636   \n",
       "4            4   33.236915  12.426685    6.418845    4.376580    4.513896   \n",
       "..         ...         ...        ...         ...         ...         ...   \n",
       "77          77   54.506310   8.534871    6.765330    2.731705    3.034995   \n",
       "78          78   35.139320  24.022022   13.486429    4.989185    3.973933   \n",
       "79          79    9.798020  10.395767   30.070915    5.071613    3.062017   \n",
       "80          80   21.128716  15.409268    4.162855    3.932925    3.936626   \n",
       "81          81   17.859299   3.056327    1.429758    1.264734    1.340887   \n",
       "\n",
       "    delta_E103  delta_E104  delta_E105  delta_E106  ...  gamma_E91  gamma_E92  \\\n",
       "0     6.252396    7.404375    8.745515    8.742263  ...  61.988657  29.291054   \n",
       "1    11.025384    7.649788    9.213589    6.471280  ...  23.900143  14.315390   \n",
       "2     4.649062    4.682885    4.240100    3.813276  ...  44.393759  59.016298   \n",
       "3    10.943584    8.306384    4.749801    1.830242  ...  20.186578  11.595557   \n",
       "4     4.290389    4.220892    4.660313    6.759117  ...   0.439690   0.102942   \n",
       "..         ...         ...         ...         ...  ...        ...        ...   \n",
       "77    5.870074    6.415011    5.530279    3.704629  ...   0.108132   0.068317   \n",
       "78    3.638945    3.372632    3.027413    7.088790  ...   0.080603   0.081616   \n",
       "79    2.705522    2.427417    1.669092    1.556226  ...   0.046448   0.035920   \n",
       "80    3.710372    3.884445    6.257363    9.081596  ...   0.062187   0.060051   \n",
       "81    0.758673    1.142326    1.081971    1.143361  ...   0.068258   0.050393   \n",
       "\n",
       "    gamma_E93   gamma_E94   gamma_E95  gamma_E96  gamma_E97  gamma_E98  \\\n",
       "0   29.744932  136.190847  212.924658  81.294987  48.675846  24.726053   \n",
       "1   15.506638   94.855550   59.283250  44.557439  16.220062  13.037046   \n",
       "2   33.794492   70.474366   78.776522  34.908377  61.082163  45.893154   \n",
       "3   11.295946   64.130118   22.663702  21.716074  12.985209  11.825560   \n",
       "4    0.091856    0.881043    0.286805   0.377908   0.149718   0.116125   \n",
       "..        ...         ...         ...        ...        ...        ...   \n",
       "77   0.056253    0.195592    0.190453   0.134718   0.072268   0.050515   \n",
       "78   0.050210    0.103784    0.064158   0.065894   0.057960   0.051233   \n",
       "79   0.034157    0.075519    0.047879   0.043430   0.040736   0.032956   \n",
       "80   0.057154    0.063969    0.070002   0.067328   0.064743   0.064991   \n",
       "81   0.021314    0.180978    0.148934   0.092323   0.070061   0.037965   \n",
       "\n",
       "     gamma_E99  target  \n",
       "0   142.232636     1.0  \n",
       "1    65.670841     1.0  \n",
       "2    93.348860     0.0  \n",
       "3    29.928711     1.0  \n",
       "4     0.319130     0.0  \n",
       "..         ...     ...  \n",
       "77    0.087241     1.0  \n",
       "78    0.082560     1.0  \n",
       "79    0.049081     1.0  \n",
       "80    0.222946     1.0  \n",
       "81    0.175942     1.0  \n",
       "\n",
       "[82 rows x 905 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('group1_5_risk_EC_abs_total (1) (1).csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809869d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delta_E1</th>\n",
       "      <th>delta_E10</th>\n",
       "      <th>delta_E100</th>\n",
       "      <th>delta_E101</th>\n",
       "      <th>delta_E102</th>\n",
       "      <th>delta_E103</th>\n",
       "      <th>delta_E104</th>\n",
       "      <th>delta_E105</th>\n",
       "      <th>delta_E106</th>\n",
       "      <th>delta_E107</th>\n",
       "      <th>...</th>\n",
       "      <th>gamma_E91</th>\n",
       "      <th>gamma_E92</th>\n",
       "      <th>gamma_E93</th>\n",
       "      <th>gamma_E94</th>\n",
       "      <th>gamma_E95</th>\n",
       "      <th>gamma_E96</th>\n",
       "      <th>gamma_E97</th>\n",
       "      <th>gamma_E98</th>\n",
       "      <th>gamma_E99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.186840</td>\n",
       "      <td>10.606450</td>\n",
       "      <td>44.688722</td>\n",
       "      <td>12.757568</td>\n",
       "      <td>4.279060</td>\n",
       "      <td>6.252396</td>\n",
       "      <td>7.404375</td>\n",
       "      <td>8.745515</td>\n",
       "      <td>8.742263</td>\n",
       "      <td>16.643758</td>\n",
       "      <td>...</td>\n",
       "      <td>61.988657</td>\n",
       "      <td>29.291054</td>\n",
       "      <td>29.744932</td>\n",
       "      <td>136.190847</td>\n",
       "      <td>212.924658</td>\n",
       "      <td>81.294987</td>\n",
       "      <td>48.675846</td>\n",
       "      <td>24.726053</td>\n",
       "      <td>142.232636</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.939918</td>\n",
       "      <td>19.104370</td>\n",
       "      <td>16.145237</td>\n",
       "      <td>13.580193</td>\n",
       "      <td>5.161970</td>\n",
       "      <td>11.025384</td>\n",
       "      <td>7.649788</td>\n",
       "      <td>9.213589</td>\n",
       "      <td>6.471280</td>\n",
       "      <td>24.463102</td>\n",
       "      <td>...</td>\n",
       "      <td>23.900143</td>\n",
       "      <td>14.315390</td>\n",
       "      <td>15.506638</td>\n",
       "      <td>94.855550</td>\n",
       "      <td>59.283250</td>\n",
       "      <td>44.557439</td>\n",
       "      <td>16.220062</td>\n",
       "      <td>13.037046</td>\n",
       "      <td>65.670841</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153.159664</td>\n",
       "      <td>15.066106</td>\n",
       "      <td>6.662611</td>\n",
       "      <td>5.487291</td>\n",
       "      <td>4.630674</td>\n",
       "      <td>4.649062</td>\n",
       "      <td>4.682885</td>\n",
       "      <td>4.240100</td>\n",
       "      <td>3.813276</td>\n",
       "      <td>21.183522</td>\n",
       "      <td>...</td>\n",
       "      <td>44.393759</td>\n",
       "      <td>59.016298</td>\n",
       "      <td>33.794492</td>\n",
       "      <td>70.474366</td>\n",
       "      <td>78.776522</td>\n",
       "      <td>34.908377</td>\n",
       "      <td>61.082163</td>\n",
       "      <td>45.893154</td>\n",
       "      <td>93.348860</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.089047</td>\n",
       "      <td>10.054340</td>\n",
       "      <td>25.154236</td>\n",
       "      <td>23.959194</td>\n",
       "      <td>14.029636</td>\n",
       "      <td>10.943584</td>\n",
       "      <td>8.306384</td>\n",
       "      <td>4.749801</td>\n",
       "      <td>1.830242</td>\n",
       "      <td>31.439636</td>\n",
       "      <td>...</td>\n",
       "      <td>20.186578</td>\n",
       "      <td>11.595557</td>\n",
       "      <td>11.295946</td>\n",
       "      <td>64.130118</td>\n",
       "      <td>22.663702</td>\n",
       "      <td>21.716074</td>\n",
       "      <td>12.985209</td>\n",
       "      <td>11.825560</td>\n",
       "      <td>29.928711</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.236915</td>\n",
       "      <td>12.426685</td>\n",
       "      <td>6.418845</td>\n",
       "      <td>4.376580</td>\n",
       "      <td>4.513896</td>\n",
       "      <td>4.290389</td>\n",
       "      <td>4.220892</td>\n",
       "      <td>4.660313</td>\n",
       "      <td>6.759117</td>\n",
       "      <td>8.330469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439690</td>\n",
       "      <td>0.102942</td>\n",
       "      <td>0.091856</td>\n",
       "      <td>0.881043</td>\n",
       "      <td>0.286805</td>\n",
       "      <td>0.377908</td>\n",
       "      <td>0.149718</td>\n",
       "      <td>0.116125</td>\n",
       "      <td>0.319130</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>54.506310</td>\n",
       "      <td>8.534871</td>\n",
       "      <td>6.765330</td>\n",
       "      <td>2.731705</td>\n",
       "      <td>3.034995</td>\n",
       "      <td>5.870074</td>\n",
       "      <td>6.415011</td>\n",
       "      <td>5.530279</td>\n",
       "      <td>3.704629</td>\n",
       "      <td>6.814907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108132</td>\n",
       "      <td>0.068317</td>\n",
       "      <td>0.056253</td>\n",
       "      <td>0.195592</td>\n",
       "      <td>0.190453</td>\n",
       "      <td>0.134718</td>\n",
       "      <td>0.072268</td>\n",
       "      <td>0.050515</td>\n",
       "      <td>0.087241</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>35.139320</td>\n",
       "      <td>24.022022</td>\n",
       "      <td>13.486429</td>\n",
       "      <td>4.989185</td>\n",
       "      <td>3.973933</td>\n",
       "      <td>3.638945</td>\n",
       "      <td>3.372632</td>\n",
       "      <td>3.027413</td>\n",
       "      <td>7.088790</td>\n",
       "      <td>6.686735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080603</td>\n",
       "      <td>0.081616</td>\n",
       "      <td>0.050210</td>\n",
       "      <td>0.103784</td>\n",
       "      <td>0.064158</td>\n",
       "      <td>0.065894</td>\n",
       "      <td>0.057960</td>\n",
       "      <td>0.051233</td>\n",
       "      <td>0.082560</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>9.798020</td>\n",
       "      <td>10.395767</td>\n",
       "      <td>30.070915</td>\n",
       "      <td>5.071613</td>\n",
       "      <td>3.062017</td>\n",
       "      <td>2.705522</td>\n",
       "      <td>2.427417</td>\n",
       "      <td>1.669092</td>\n",
       "      <td>1.556226</td>\n",
       "      <td>7.140624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046448</td>\n",
       "      <td>0.035920</td>\n",
       "      <td>0.034157</td>\n",
       "      <td>0.075519</td>\n",
       "      <td>0.047879</td>\n",
       "      <td>0.043430</td>\n",
       "      <td>0.040736</td>\n",
       "      <td>0.032956</td>\n",
       "      <td>0.049081</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>21.128716</td>\n",
       "      <td>15.409268</td>\n",
       "      <td>4.162855</td>\n",
       "      <td>3.932925</td>\n",
       "      <td>3.936626</td>\n",
       "      <td>3.710372</td>\n",
       "      <td>3.884445</td>\n",
       "      <td>6.257363</td>\n",
       "      <td>9.081596</td>\n",
       "      <td>13.574650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062187</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.057154</td>\n",
       "      <td>0.063969</td>\n",
       "      <td>0.070002</td>\n",
       "      <td>0.067328</td>\n",
       "      <td>0.064743</td>\n",
       "      <td>0.064991</td>\n",
       "      <td>0.222946</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>17.859299</td>\n",
       "      <td>3.056327</td>\n",
       "      <td>1.429758</td>\n",
       "      <td>1.264734</td>\n",
       "      <td>1.340887</td>\n",
       "      <td>0.758673</td>\n",
       "      <td>1.142326</td>\n",
       "      <td>1.081971</td>\n",
       "      <td>1.143361</td>\n",
       "      <td>1.769880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068258</td>\n",
       "      <td>0.050393</td>\n",
       "      <td>0.021314</td>\n",
       "      <td>0.180978</td>\n",
       "      <td>0.148934</td>\n",
       "      <td>0.092323</td>\n",
       "      <td>0.070061</td>\n",
       "      <td>0.037965</td>\n",
       "      <td>0.175942</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 904 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      delta_E1  delta_E10  delta_E100  delta_E101  delta_E102  delta_E103  \\\n",
       "0     8.186840  10.606450   44.688722   12.757568    4.279060    6.252396   \n",
       "1    25.939918  19.104370   16.145237   13.580193    5.161970   11.025384   \n",
       "2   153.159664  15.066106    6.662611    5.487291    4.630674    4.649062   \n",
       "3     7.089047  10.054340   25.154236   23.959194   14.029636   10.943584   \n",
       "4    33.236915  12.426685    6.418845    4.376580    4.513896    4.290389   \n",
       "..         ...        ...         ...         ...         ...         ...   \n",
       "77   54.506310   8.534871    6.765330    2.731705    3.034995    5.870074   \n",
       "78   35.139320  24.022022   13.486429    4.989185    3.973933    3.638945   \n",
       "79    9.798020  10.395767   30.070915    5.071613    3.062017    2.705522   \n",
       "80   21.128716  15.409268    4.162855    3.932925    3.936626    3.710372   \n",
       "81   17.859299   3.056327    1.429758    1.264734    1.340887    0.758673   \n",
       "\n",
       "    delta_E104  delta_E105  delta_E106  delta_E107  ...  gamma_E91  gamma_E92  \\\n",
       "0     7.404375    8.745515    8.742263   16.643758  ...  61.988657  29.291054   \n",
       "1     7.649788    9.213589    6.471280   24.463102  ...  23.900143  14.315390   \n",
       "2     4.682885    4.240100    3.813276   21.183522  ...  44.393759  59.016298   \n",
       "3     8.306384    4.749801    1.830242   31.439636  ...  20.186578  11.595557   \n",
       "4     4.220892    4.660313    6.759117    8.330469  ...   0.439690   0.102942   \n",
       "..         ...         ...         ...         ...  ...        ...        ...   \n",
       "77    6.415011    5.530279    3.704629    6.814907  ...   0.108132   0.068317   \n",
       "78    3.372632    3.027413    7.088790    6.686735  ...   0.080603   0.081616   \n",
       "79    2.427417    1.669092    1.556226    7.140624  ...   0.046448   0.035920   \n",
       "80    3.884445    6.257363    9.081596   13.574650  ...   0.062187   0.060051   \n",
       "81    1.142326    1.081971    1.143361    1.769880  ...   0.068258   0.050393   \n",
       "\n",
       "    gamma_E93   gamma_E94   gamma_E95  gamma_E96  gamma_E97  gamma_E98  \\\n",
       "0   29.744932  136.190847  212.924658  81.294987  48.675846  24.726053   \n",
       "1   15.506638   94.855550   59.283250  44.557439  16.220062  13.037046   \n",
       "2   33.794492   70.474366   78.776522  34.908377  61.082163  45.893154   \n",
       "3   11.295946   64.130118   22.663702  21.716074  12.985209  11.825560   \n",
       "4    0.091856    0.881043    0.286805   0.377908   0.149718   0.116125   \n",
       "..        ...         ...         ...        ...        ...        ...   \n",
       "77   0.056253    0.195592    0.190453   0.134718   0.072268   0.050515   \n",
       "78   0.050210    0.103784    0.064158   0.065894   0.057960   0.051233   \n",
       "79   0.034157    0.075519    0.047879   0.043430   0.040736   0.032956   \n",
       "80   0.057154    0.063969    0.070002   0.067328   0.064743   0.064991   \n",
       "81   0.021314    0.180978    0.148934   0.092323   0.070061   0.037965   \n",
       "\n",
       "     gamma_E99  target  \n",
       "0   142.232636     1.0  \n",
       "1    65.670841     1.0  \n",
       "2    93.348860     0.0  \n",
       "3    29.928711     1.0  \n",
       "4     0.319130     0.0  \n",
       "..         ...     ...  \n",
       "77    0.087241     1.0  \n",
       "78    0.082560     1.0  \n",
       "79    0.049081     1.0  \n",
       "80    0.222946     1.0  \n",
       "81    0.175942     1.0  \n",
       "\n",
       "[82 rows x 904 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[: , 1:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "067151d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = df\n",
    "target = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8418285",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [ col for col in train_data.columns if col != target] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4b39a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 904\n",
      "129\n",
      "258\n",
      "129\n",
      "129\n",
      "129\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "train_data = df\n",
    "#test_data = test\n",
    "\n",
    "NUM_FOLDS  = 10\n",
    "# the number of feature that you want to show \n",
    "Num_feat = 20\n",
    "\n",
    "target ='target'\n",
    "#unused_feat = ['abcd_site','kfold']\n",
    "\n",
    "# mri feature가 시작하는 column의 index 구하기\n",
    "# np.where의 결과값이 array에 들어가기 때문에 방금 계산해 넣어놓은 [0]번째 값을 가져온다.\n",
    "start_delta_index = np.where(train_data.columns.values == \"delta_E11\")[0][0]\n",
    "start_theta_index = np.where(train_data.columns.values == \"theta_E11\")[0][0]\n",
    "start_beta1_index = np.where(train_data.columns.values == \"beta1_E11\")[0][0]\n",
    "start_beta2_index = np.where(train_data.columns.values == \"beta2_E11\")[0][0]\n",
    "start_beta3_index = np.where(train_data.columns.values == \"beta3_E11\")[0][0]\n",
    "start_gamma_index = np.where(train_data.columns.values == \"gamma_E11\")[0][0]\n",
    "\n",
    "\n",
    "\n",
    "delta = list(train_data.columns[start_delta_index:start_theta_index])\n",
    "theta = list(train_data.columns[start_theta_index:start_beta1_index])\n",
    "beta1 = list(train_data.columns[start_beta1_index:start_beta2_index])\n",
    "beta2 = list(train_data.columns[start_beta2_index:start_beta3_index])\n",
    "beta3= list(train_data.columns[start_beta3_index:start_gamma_index])\n",
    "gamma = list(train_data.columns[start_gamma_index:])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(train_data), len(train_data.columns))\n",
    "print(len(delta))\n",
    "print(len(theta))\n",
    "print(len(beta1))\n",
    "print(len(beta2))\n",
    "print(len(beta3))\n",
    "print(len(gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99abb8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sparsemax(nn.Module):\n",
    "    def __init__(self, dim=None):\n",
    "        super(Sparsemax, self).__init__()\n",
    "        self.dim = -1 if dim is None else dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.transpose(0, self.dim)\n",
    "        original_size = input.size()\n",
    "        input = input.reshape(input.size(0), -1)\n",
    "        input = input.transpose(0, 1)\n",
    "        dim = 1\n",
    "\n",
    "        number_of_logits = input.size(dim)\n",
    "        \n",
    "        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n",
    "        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n",
    "        range = torch.arange(start=1, end=number_of_logits + 1, device=device,step=1, dtype=input.dtype).view(1, -1)\n",
    "        range = range.expand_as(zs)\n",
    "\n",
    "        bound = 1 + range * zs\n",
    "        cumulative_sum_zs = torch.cumsum(zs, dim)\n",
    "        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n",
    "        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n",
    "        zs_sparse = is_gt * zs\n",
    "        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n",
    "        taus = taus.expand_as(input)\n",
    "        self.output = torch.max(torch.zeros_like(input), input - taus)\n",
    "        output = self.output\n",
    "        output = output.transpose(0, 1)\n",
    "        output = output.reshape(original_size)\n",
    "        output = output.transpose(0, self.dim)\n",
    "        return output\n",
    "    def backward(self, grad_output):\n",
    "        dim = 1\n",
    "        nonzeros = torch.ne(self.output, 0)\n",
    "        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n",
    "        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n",
    "        return self.grad_input\n",
    "\n",
    "def initialize_non_glu(module,inp_dim,out_dim):\n",
    "    gain = np.sqrt((inp_dim+out_dim)/np.sqrt(4*inp_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "    \n",
    "class GBN(nn.Module):\n",
    "    def __init__(self,inp,vbs=128,momentum=0.01):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(inp,momentum=momentum)\n",
    "        self.vbs = vbs\n",
    "    def forward(self,x):\n",
    "        chunk = torch.chunk(x,max(1,x.size(0)//self.vbs),0)\n",
    "        res = [self.bn(y) for y in chunk ]\n",
    "        return torch.cat(res,0)\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self,inp_dim,out_dim,fc=None,vbs=128):\n",
    "        super().__init__()\n",
    "        if fc:\n",
    "            self.fc = fc\n",
    "        else:\n",
    "            self.fc = nn.Linear(inp_dim,out_dim*2)\n",
    "        self.bn = GBN(out_dim*2,vbs=vbs) \n",
    "        self.od = out_dim\n",
    "    def forward(self,x):\n",
    "        x = self.bn(self.fc(x))\n",
    "        return x[:,:self.od]*torch.sigmoid(x[:,self.od:])\n",
    "    \n",
    "\n",
    "class FeatureTransformer(nn.Module):\n",
    "    def __init__(self,inp_dim,out_dim,shared,n_ind,vbs=128):\n",
    "        super().__init__()\n",
    "        first = True\n",
    "        self.shared = nn.ModuleList()\n",
    "        if shared:\n",
    "            self.shared.append(GLU(inp_dim,out_dim,shared[0],vbs=vbs))\n",
    "            first= False    \n",
    "            for fc in shared[1:]:\n",
    "                self.shared.append(GLU(out_dim,out_dim,fc,vbs=vbs))\n",
    "        else:\n",
    "            self.shared = None\n",
    "        self.independ = nn.ModuleList()\n",
    "        if first:\n",
    "            self.independ.append(GLU(inp,out_dim,vbs=vbs))\n",
    "        for x in range(first, n_ind):\n",
    "            self.independ.append(GLU(out_dim,out_dim,vbs=vbs))\n",
    "        self.scale = torch.sqrt(torch.tensor([.5],device=device))\n",
    "    def forward(self,x):\n",
    "        if self.shared:\n",
    "            x = self.shared[0](x)\n",
    "            for glu in self.shared[1:]:\n",
    "                x = torch.add(x, glu(x))\n",
    "                x = x*self.scale\n",
    "        for glu in self.independ:\n",
    "            x = torch.add(x, glu(x))\n",
    "            x = x*self.scale\n",
    "        return x\n",
    "class AttentionTransformer(nn.Module):\n",
    "    def __init__(self,inp_dim,out_dim,relax,vbs=128):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(inp_dim,out_dim)\n",
    "        self.bn = GBN(out_dim,vbs=vbs)\n",
    "#         self.smax = Sparsemax()\n",
    "        self.r = torch.tensor([relax],device=device)\n",
    "    def forward(self,a,priors):\n",
    "        a = self.bn(self.fc(a))\n",
    "        mask = torch.sigmoid(a*priors)\n",
    "        priors =priors*(self.r-mask)\n",
    "        return mask\n",
    "\n",
    "class DecisionStep(nn.Module):\n",
    "    def __init__(self,inp_dim,n_d,n_a,shared,n_ind,relax,vbs=128):\n",
    "        super().__init__()\n",
    "        self.fea_tran = FeatureTransformer(inp_dim,n_d+n_a,shared,n_ind,vbs)\n",
    "        self.atten_tran = AttentionTransformer(n_a,inp_dim,relax,vbs)\n",
    "    def forward(self,x,a,priors):\n",
    "        mask = self.atten_tran(a,priors)\n",
    "        loss = ((-1)*mask*torch.log(mask+1e-10)).mean()\n",
    "        x = self.fea_tran(x*mask)\n",
    "        return x,loss\n",
    "\n",
    "\n",
    "class TabNet(nn.Module):\n",
    "    def __init__(self,inp_dim,final_out_dim,n_d=2,n_a=2,n_shared=2,n_ind=2,n_steps=3,relax=1.3,vbs=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        if n_shared>0:\n",
    "            self.shared = nn.ModuleList()\n",
    "            self.shared.append(nn.Linear(inp_dim,2*(n_d+n_a)))\n",
    "            for x in range(n_shared-1):\n",
    "                self.shared.append(nn.Linear(n_d+n_a,2*(n_d+n_a)))\n",
    "        else:\n",
    "            self.shared=None\n",
    "            \n",
    "        self.first_step = FeatureTransformer(inp_dim,n_d+n_a,self.shared,n_ind) \n",
    "        self.steps = nn.ModuleList()\n",
    "        for x in range(n_steps-1):\n",
    "            self.steps.append(DecisionStep(inp_dim,n_d,n_a,self.shared,n_ind,relax,vbs))\n",
    "        self.fc = nn.Linear(n_d,final_out_dim)\n",
    "        self.bn = nn.BatchNorm1d(inp_dim)\n",
    "        self.n_d = n_d\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        \n",
    "        #self.features = nn.Sequential()\n",
    "        #self.features.add_module(\"BN1\", nn.BatchNorm1d(inp_dim))\n",
    "        \n",
    "        \n",
    "            \n",
    "#         self.features.add_module(\"FirstStep\", FeatureTransformer(inp_dim,n_d+n_a,self.shared,n_ind))\n",
    "#         #self.steps = nn.ModuleList()\n",
    "#         for x in range(n_steps-1):\n",
    "#             self.features.add_module('Step%d' % x, DecisionStep(inp_dim,n_d,n_a,self.shared,n_ind,relax,vbs))\n",
    "#         self.features.add_module('FC', nn.Linear(n_d, final_out_dim))\n",
    "#         self.features.add_module('Sigmoid', nn.Sigmoid())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn(x)\n",
    "        x_a = self.first_step(x)[:,self.n_d:]\n",
    "        loss = torch.zeros(1).to(x.device)\n",
    "        out = torch.zeros(x.size(0),self.n_d).to(x.device)\n",
    "        priors = torch.ones(x.shape).to(x.device)\n",
    "        for step in self.steps:\n",
    "            x_te,l = step(x,x_a,priors)\n",
    "            out += F.relu(x_te[:,:self.n_d])\n",
    "            x_a = x_te[:,self.n_d:]\n",
    "            loss += l\n",
    "        out = self.fc(out)\n",
    "        #out = self.sigm(out)\n",
    "        #x = self.features(x)\n",
    "        \n",
    "        return out, loss\n",
    "    \n",
    "    def _count_parameters(self):\n",
    "        n_params = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            print(name)\n",
    "            print(param.size())\n",
    "            print(param.numel())\n",
    "            n_params += param.numel()\n",
    "            #print('num of parameters so far: {}'.format(n_params))\n",
    "\n",
    "    def reset_parameters(self, verbose=False):\n",
    "        for module in self.modules():\n",
    "            # pass self, otherwise infinite loop\n",
    "            if isinstance(module, self.__class__):\n",
    "                continue\n",
    "            if 'reset_parameters' in dir(module):\n",
    "                if callable(module.reset_parameters):\n",
    "                    module.reset_parameters()\n",
    "                    #if verbose:\n",
    "                        #print(\"Reset parameters in {}\".format(module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4d49900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesTabnet(nn.Module):\n",
    "    def __init__(self, n_samples, device, model):\n",
    "        super(BayesTabnet, self).__init__()\n",
    "        self.n_samples = n_samples\n",
    "        self.device = device\n",
    "        self.w_prior_shape = 1\n",
    "        self.w_prior_rate = 0.05\n",
    "        self.beta_prior_shape = 2\n",
    "        self.beta_prior_rate = 1.e-6\n",
    "        \n",
    "        instances = []\n",
    "        for i in range(self.n_samples):\n",
    "            new_instance = copy.deepcopy(model)\n",
    "            new_instance.reset_parameters()\n",
    "            #print('Reset parameters in model instance {}'.format(i+1))\n",
    "            instances.append(new_instance)\n",
    "        self.nets = nn.ModuleList(instances)\n",
    "        del instances\n",
    "        \n",
    "        log_beta = Gamma(self.beta_prior_shape, self.beta_prior_rate).sample((self.n_samples,)).log()\n",
    "        for i in range(self.n_samples):\n",
    "            self.nets[i].log_beta = Parameter(log_beta[i])\n",
    "\n",
    "        #print('Total number of parameters: {}'.format(self._num_parameters()))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _num_parameters(self):\n",
    "        count = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            count += param.numel()\n",
    "        return count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.nets[idx]\n",
    "\n",
    "    @property\n",
    "    def log_beta(self):\n",
    "        return torch.tensor([self.nets[i].log_beta.item() \n",
    "            for i in range(self.n_samples)], device=self.device)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = []\n",
    "        for i in range(self.n_samples):\n",
    "            fout, floss = self.nets[i].forward(input)\n",
    "            output.append(fout)\n",
    "        output = torch.stack(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _log_joint(self, index, output, target, ntrain): #depends on beta prior\n",
    "        #log_likelihood = ntrain / output.size(0) * (\n",
    "        #                    - 0.5 * self.nets[index].log_beta.exp()\n",
    "        #                    * (target - output).pow(2).sum()\n",
    "        #                    + 0.5 * target.numel() * self.nets[index].log_beta)\n",
    "       \n",
    "        log_likelihood = ntrain / output.size(0) * (\n",
    "                            - 0.5 * self.nets[index].log_beta.exp()\n",
    "                            * -(target * torch.log(torch.sigmoid(output)+0.00001) + (1-target) * torch.log(1-torch.sigmoid(output)+0.00001)).sum()\n",
    "                            + 0.5 * target.numel() * self.nets[index].log_beta)\n",
    "\n",
    "        log_prob_prior_w = torch.tensor(0.).to(self.device)\n",
    "        for param in self.nets[index].bn.parameters():\n",
    "            log_prob_prior_w += \\\n",
    "                torch.log1p(0.5 / self.w_prior_rate * param.pow(2)).sum()\n",
    "        for param in self.nets[index].first_step.parameters():\n",
    "            log_prob_prior_w += \\\n",
    "                torch.log1p(0.5 / self.w_prior_rate * param.pow(2)).sum()\n",
    "        for param in self.nets[index].steps.parameters():\n",
    "            log_prob_prior_w += \\\n",
    "                torch.log1p(0.5 / self.w_prior_rate * param.pow(2)).sum()\n",
    "        for param in self.nets[index].fc.parameters():\n",
    "            log_prob_prior_w += \\\n",
    "                torch.log1p(0.5 / self.w_prior_rate * param.pow(2)).sum()\n",
    "        log_prob_prior_w *= -(self.w_prior_shape + 0.5)\n",
    "        log_prob_prior_log_beta = (self.beta_prior_shape * self.nets[index].log_beta \\\n",
    "                    - self.nets[index].log_beta.exp() * self.beta_prior_rate)\n",
    "        return log_likelihood + log_prob_prior_w + log_prob_prior_log_beta\n",
    "\n",
    "    def _compute_bce(self, input, target, size_average=True, out=False):\n",
    "        output = self.forward(input)\n",
    "        #log_beta = self.log_beta.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        #log_2pi_S = torch.tensor(0.5 * target[0].numel() * math.log(2 * math.pi)\n",
    "        #               + math.log(self.n_samples), device=self.device)\n",
    "        #exponent = - 0.5 * (log_beta.exp() * ((target - output) ** 2)).view(\n",
    "        #    self.n_samples, target.size(0), -1).sum(-1) \\\n",
    "        #           + 0.5 * target[0].numel() * self.log_beta.unsqueeze(-1)\n",
    "\n",
    "        #nlp = - log_sum_exp(exponent, dim=0).mean() + log_2pi_S\n",
    "        bce = F.binary_cross_entropy_with_logits(output.mean(0), target)\n",
    "\n",
    "        if not size_average:\n",
    "            bce *= target.numel()\n",
    "            #nlp *= target.size(0)\n",
    "        if not out:\n",
    "            return bce\n",
    "        else:\n",
    "            return bce, output\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        y = self.forward(x_test)\n",
    "        y_pred_mean = y.mean(0)\n",
    "        EyyT = (y ** 2).mean(0)\n",
    "        EyEyT = y_pred_mean ** 2\n",
    "        beta_inv = (- self.log_beta).exp()\n",
    "        y_pred_var = beta_inv.mean() + EyyT - EyEyT\n",
    "\n",
    "        return y_pred_mean, y_pred_var\n",
    "\n",
    "    def propagate(self, mc_loader):\n",
    "        output_size = mc_loader.dataset[0][1].size()\n",
    "        cond_Ey = torch.zeros(self.n_samples, *output_size, device=self.device)\n",
    "        cond_Eyy = torch.zeros_like(cond_Ey)\n",
    "\n",
    "        for _, (x_mc, _) in enumerate(mc_loader):\n",
    "            x_mc = x_mc.to(self.device)          \n",
    "            y = self.forward(x_mc)\n",
    "            cond_Ey += y.mean(1)\n",
    "            cond_Eyy += y.pow(2).mean(1)\n",
    "        cond_Ey /= len(mc_loader)\n",
    "        cond_Eyy /= len(mc_loader)\n",
    "        beta_inv = (- self.log_beta).exp()\n",
    "        print('Noise variances: {}'.format(beta_inv))\n",
    "        \n",
    "        y_cond_pred_var = cond_Eyy - cond_Ey ** 2 \\\n",
    "                     + beta_inv.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        return cond_Ey.mean(0), cond_Ey.var(0), \\\n",
    "               y_cond_pred_var.mean(0), y_cond_pred_var.var(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3283d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVGD(object):\n",
    "    def __init__(self, train_loader, n_samples, lr, lr_noise, ntrain, btrain, out, device, bayes_net):\n",
    "        self.bayes_net = bayes_net\n",
    "        self.train_loader = train_loader\n",
    "        self.n_samples = n_samples\n",
    "        self.lr = lr\n",
    "        self.lr_noise = lr_noise\n",
    "        self.ntrain = ntrain\n",
    "        self.batch_train_size = btrain\n",
    "        self.out_channels = out\n",
    "        self.device = device\n",
    "        self.optimizers, self.schedulers = self._optimizers_schedulers(self.lr, self.lr_noise)\n",
    "\n",
    "    def _squared_dist(self, X):\n",
    "        XXT = torch.mm(X, X.t())\n",
    "        XTX = XXT.diag()\n",
    "        return -2.0 * XXT + XTX + XTX.unsqueeze(1)\n",
    "\n",
    "    def _Kxx_dxKxx(self, X):\n",
    "        squared_dist = self._squared_dist(X)\n",
    "        l_square = 0.5 * squared_dist.median() / math.log(self.n_samples)\n",
    "        Kxx = torch.exp(-0.5 / l_square * squared_dist)\n",
    "        dxKxx = (Kxx.sum(1).diag() - Kxx).matmul(X) / l_square\n",
    "        return Kxx, dxKxx\n",
    "    \n",
    "    def _optimizers_schedulers(self, lr, lr_noise):\n",
    "        optimizers = []\n",
    "        schedulers = []\n",
    "        for i in range(self.n_samples):\n",
    "            parameters = [{'params': [self.bayes_net[i].log_beta], 'lr': lr_noise}, {'params': self.bayes_net[i].bn.parameters()}, \n",
    "                          {'params': self.bayes_net[i].steps.parameters()}, {'params': self.bayes_net[i].fc.parameters()}]\n",
    "            #torch.optim.Adam(model.parameters(),lr=2e-2,weight_decay=0.01)\n",
    "            #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 10, gamma = 0.99)  \n",
    "            optimizer_i = torch.optim.Adam(parameters,lr=lr,weight_decay=0.01)\n",
    "            optimizers.append(optimizer_i)\n",
    "            schedulers.append(ReduceLROnPlateau(optimizer_i, mode='min', factor=0.1, patience=10, verbose=True))\n",
    "            #schedulers.append(StepLR(optimizer_i, step_size = 10, gamma = 0.99))\n",
    "        return optimizers, schedulers\n",
    "\n",
    "    def train(self, epoch, logger, l1_pen, train_loader):\n",
    "        self.bayes_net.train()\n",
    "        bce_train = 0\n",
    "        acc_train = 0\n",
    "        auc_train = 0\n",
    "        for batch_idx, (input_x, true_y) in enumerate(self.train_loader):\n",
    "            input_x, true_y = input_x.to(self.device), true_y.to(self.device)\n",
    "            true_y = torch.unsqueeze(true_y, 1)\n",
    "\n",
    "            self.bayes_net.zero_grad()\n",
    "            pred_y = torch.zeros_like(true_y)\n",
    "            grad_log_joint = []\n",
    "            theta = []\n",
    "            log_joint = 0.\n",
    "\n",
    "            for idx in range(self.n_samples):\n",
    "                pred_y_i, pred_y_loss = self.bayes_net[idx].forward(input_x)\n",
    "                pred_y += pred_y_i.detach()\n",
    "                log_joint_i = self.bayes_net._log_joint(idx, pred_y_i, true_y, self.ntrain)\n",
    "                log_joint_i += l1_pen*pred_y_loss.cpu().detach().numpy()[0]\n",
    "                log_joint_i.backward()\n",
    "                log_joint += log_joint_i.item()\n",
    "\n",
    "                vec_param, vec_grad_log_joint = parameters_to_vector(self.bayes_net[idx].parameters(), both=True)\n",
    "                grad_log_joint.append(vec_grad_log_joint.unsqueeze(0))\n",
    "                theta.append(vec_param.unsqueeze(0))\n",
    "\n",
    "            theta = torch.cat(theta)\n",
    "            Kxx, dxKxx = self._Kxx_dxKxx(theta)\n",
    "            grad_log_joint = torch.cat(grad_log_joint)\n",
    "            grad_logp = torch.mm(Kxx, grad_log_joint)\n",
    "            grad_theta = - (grad_logp + dxKxx) / self.n_samples\n",
    "\n",
    "            for idx in range(self.n_samples):\n",
    "                vector_to_parameters(grad_theta[idx], self.bayes_net[idx].parameters(), grad=True)\n",
    "                self.optimizers[idx].step()\n",
    "                #self.schedulers[idx].step()\n",
    "            bce_train += F.binary_cross_entropy_with_logits(pred_y / self.n_samples, true_y).item()\n",
    "            acc_train += accuracy_score(true_y.cpu(), np.where((pred_y.cpu()/self.n_samples).data < 0.5, 0, 1))\n",
    "            #auc_train += roc_auc_score(y_true = true_y.cpu(), y_score = (pred_y.cpu()/self.n_samples).data)\n",
    "            #r2_score += R2_value(pred_y / self.n_samples, true_y)\n",
    "\n",
    "        avg_bce_train = bce_train / len(self.train_loader)\n",
    "        avg_acc_train = acc_train / len(self.train_loader)\n",
    "        #avg_auc_train = auc_train / len(self.train_loader)\n",
    "        #r2_train = r2_score / len(self.train_loader)\n",
    "        logger['bce_train'].append(bce_train)\n",
    "        #logger['r2_train'].append(r2_train)\n",
    "        logger['log_beta'].append(self.bayes_net.log_beta.mean().item())\n",
    "        #print(\"epoch {}, training bce: {:.6f}, training acc: {:.6f}, training auc: {:.6f}\".format(epoch, avg_bce_train, avg_acc_train, avg_auc_train))\n",
    "        print(\"epoch {}, training bce: {:.6f}, training acc: {:.6f}\".format(epoch, avg_bce_train, avg_acc_train))\n",
    "\n",
    "\n",
    "        for idx in range(self.n_samples):\n",
    "            self.schedulers[idx].step(avg_bce_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c819bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch, logger, test_loader, model_load, model_state):\n",
    "    if model_load == True:\n",
    "      model.load_state_dict(torch.load(model_state))\n",
    "    model.eval()\n",
    "    start = time()\n",
    "    \n",
    "    bce_test = 0.\n",
    "    #count = 0\n",
    "    acc_test = 0.\n",
    "    auc_test = 0.\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        bce, outputs = model._compute_bce(inputs, targets.unsqueeze(1), size_average=True, out=True)\n",
    "        outputs_mean = outputs.mean(0)\n",
    "        \n",
    "        EyyT = (outputs ** 2).mean(0)\n",
    "        EyEyT = outputs_mean ** 2\n",
    "        outputs_noise_var = (- model.log_beta).exp().mean()\n",
    "        outputs_var =  EyyT - EyEyT + outputs_noise_var\n",
    "        \n",
    "        acc_test += accuracy_score(targets.cpu(),np.where(outputs_mean.cpu().data < 0.5, 0, 1))\n",
    "        auc_test += roc_auc_score(y_true = targets.cpu(), y_score = outputs_mean.cpu().data)\n",
    "        \n",
    "        bce_test += bce.item()\n",
    "\n",
    "    bce_test = bce_test/len(test_loader)\n",
    "    acc_test = acc_test/len(test_loader)\n",
    "    auc_test = auc_test/len(test_loader)\n",
    "\n",
    "    #if epoch == args.epochs:\n",
    "    #    saveData(tar_save, out_save, err_save, uncer_save, args.save_dir)\n",
    "    #    plotTestSample(out_save, tar_save, err_save, uncer_save, args.save_dir, args.num_Plot)\n",
    "\n",
    "    stop = time()\n",
    "    \n",
    "    print('Test epoch: {}, bce: {:.6f}, acc: {:.6f}, auc: {:.6f}, time: {:.3f}'.format(epoch, bce_test, acc_test, auc_test, stop-start))\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        #logger['r2_test'].append(r2_score)\n",
    "        logger['bce_test'].append(bce_test)\n",
    "        #logger['mnlp_test'].append(mnlp_test)\n",
    "    \n",
    "    return(model, acc_test, auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c69762c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(NUM_FOLDS, Num_feat, clf_res, test_data_processed, features):\n",
    "    importance_res = []\n",
    "    for i in clf_res:\n",
    "        importance_clf =i.feature_importances_\n",
    "        importance_res.append(importance_clf)\n",
    "    \n",
    "    importance=[importance_res[0][i]/NUM_FOLDS+importance_res[1][i]/NUM_FOLDS+ importance_res[2][i]/NUM_FOLDS+ importance_res[3][i]/NUM_FOLDS+ importance_res[4][i]/NUM_FOLDS for i in range(len(importance_res[1]))]\n",
    "    \n",
    "    #feat_name_sort=test_data_processed[features].columns[labels_importance]\n",
    "    feat_name_sort = test_data_processed[features].columns\n",
    "    important_features = pd.DataFrame([importance],columns = feat_name_sort, index=['Importance']) \n",
    "    important_features = important_features.transpose().sort_values(by=['Importance'], ascending=False)\n",
    "    important_features = important_features.head(Num_feat)\n",
    "\n",
    "    return important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c3a5aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done preprocessing\n"
     ]
    }
   ],
   "source": [
    "def preprocessing (train_data, NUM_FOLDS, target):\n",
    "    #imputation\n",
    "    train_data_processed = train_data.fillna(0).reset_index()\n",
    "\n",
    "    train_data_processed=train_data_processed.rename(columns = {'index' : 'subjectkey'})\n",
    "\n",
    "\n",
    "    train_data[\"kfold\"] = -1\n",
    "\n",
    "    # frac: 전체 row 중 몇 %를 반환할 지 결정 -> frac=1을 설정해서 모든 데이터를 반환\n",
    "    # random_state: 추후 이것과 동일한 샘플링을 재현하기 위함\n",
    "    # sample: 데이터에서 임의의 샘플 선정 -> frac=1이면 전체 data의 순서만 임의로 바뀜\n",
    "    train_data_processed = train_data_processed.sample(frac=1,random_state=2020).reset_index(drop=True)\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state =0)\n",
    "    \n",
    "    # enumerate: 각 split된 data set 순서대로 index를 함께 반환\n",
    "        # ex. fold 0 일 때, 80%는 trn_, 20%는 val_\n",
    "    for fold, (trn_, val_) in enumerate(kf.split(X=train_data_processed, y=train_data_processed[target])):\n",
    "\n",
    "        # 'kfold' 칼럼은 cross validation할 때 fold 순서를 지정해놓은 것. \n",
    "        train_data_processed.loc[val_, 'kfold'] = fold\n",
    "    \n",
    "    print(\"done preprocessing\")\n",
    "    return train_data_processed\n",
    "\n",
    "train_data_processed = preprocessing(train_data,10,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73997491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bestpar(fold, train_data_processed, features, target):\n",
    "    \n",
    "    # Store maximum auc\n",
    "    max_auc= 0\n",
    "    # Store maximum hypterparameter set\n",
    "    max_hy = []\n",
    "    \"\"\"\n",
    "    # define hyperparameter space : learning rate, \n",
    "    n_ = [4,8,16]                              # \n",
    "    lr_ = [2e-2, 1e-2, 5e-3, 2e-3, 1e-3, 1e-4] # learning rate\n",
    "    w_ = [0.01, 0.001, 0.0001]                 # weight decay\n",
    "    g_ = [0.95, 0.99, 0.9]                     # scheduler params - gamma\n",
    "    ss_ = [10, 20, 30]                         # scheduler params - step_size\n",
    "    \n",
    "    # Orginal hyperparameter space \n",
    "    \"\"\"\n",
    "    # define hyperparameter space (quick version)\n",
    "    n_ = [4,16]\n",
    "    lr_ = [2e-2,1e-3]\n",
    "    w_ = [0.01,0.001]\n",
    "    \n",
    "    \n",
    "    all_ = [n_, lr_, w_]\n",
    "    h_space = [s for s in itertools.product(*all_)]\n",
    "    \n",
    "    for hy in tqdm(h_space):\n",
    "        \"\"\"===================Cross Validation===================\"\"\"\n",
    "        \n",
    "        \"\"\"validation & test 결과\"\"\"\n",
    "        valid_res = []\n",
    "\n",
    "        for i in range(fold):\n",
    "            #print(\"fold \", i)\n",
    "            # 5개의 fold 사용했으므로 변수 fold 값은 차례대로 0,1,2,3,4 중 하나\n",
    "\n",
    "            df_train = train_data_processed[train_data_processed.kfold != i]  # 5개 중 4개 train에 할당\n",
    "            df_valid = train_data_processed[train_data_processed.kfold == i]  # 5개 중 1개 validation에 할당\n",
    "\n",
    "            X_train = df_train[features].values\n",
    "            Y_train = df_train[target].values\n",
    "            \n",
    "            X_valid = df_valid[features].values\n",
    "            Y_valid = df_valid[target].values\n",
    "\n",
    "            train_x = torch.Tensor(X_train)\n",
    "            train_y = torch.Tensor(Y_train.flatten())\n",
    "            train_ds = utils.TensorDataset(train_x,train_y)\n",
    "            #class_sample_count = np.array(\n",
    "            #                           [len(np.where(np.transpose(y_train.flatten()) == t)[0]) for t in np.unique(np.transpose(y_train.flatten()))])\n",
    "            #class_weight = 1/class_sample_count\n",
    "            #samples_weight = np.array([class_weight[t] for t in np.transpose(y_train.flatten())])\n",
    "            #samples_weight = torch.from_numpy(samples_weight)\n",
    "            #sampler = td.WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "            train_loader = td.DataLoader(train_ds, batch_size=64,\n",
    "                shuffle=False, num_workers=1)\n",
    "\n",
    "            val_x = torch.Tensor(X_valid)\n",
    "            val_y = torch.Tensor(Y_valid.flatten())\n",
    "            val_ds = utils.TensorDataset(val_x,val_y)\n",
    "            val_loader = td.DataLoader(val_ds, batch_size=len(Y_valid.flatten()),\n",
    "                shuffle=False, num_workers=1)\n",
    "\n",
    "                        \n",
    "            model_TabNet = TabNet(inp_dim = X_train.shape[1], final_out_dim = 1, n_d=hy[0],n_a=hy[0],n_shared=2,n_ind=2,n_steps=3,relax=1.3,vbs=128).to(device)\n",
    "            model = BayesTabnet(n_samples = 10, device = device, model = model_TabNet).to(device)\n",
    "            criterion, optimizer, scheduler, logger = getOpt(lrs = 'ReduceLROnPlateau', lr = hy[1], weight_decay = hy[2], model = model)\n",
    "            model_SVGD = SVGD(train_loader = train_loader, n_samples = 10, lr = hy[1], lr_noise = hy[2], ntrain = X_train.shape[0], btrain = 64, out = 1, device = device, bayes_net = model)\n",
    "            best_val_auc = 0.\n",
    "            best_val_epoch = 1\n",
    "            for epoch in range(1,  11):\n",
    "              model_SVGD.train(epoch, logger, 0.001, train_loader)\n",
    "              with torch.no_grad():\n",
    "                val_model, val_acc, val_auc = test(model, epoch, logger, val_loader, False, None)\n",
    "                if val_auc > best_val_auc:\n",
    "                  torch.save(val_model.state_dict(), 'BayesTabNetMRI_Fold'+str(epoch)+'_.pt')\n",
    "                  best_val_auc = val_auc\n",
    "                  best_val_epoch = epoch\n",
    "                \n",
    "            valid_res.append(best_val_auc)\n",
    "            print('[%3d/%4d] '%(i+1, fold),'Valid score: %2f'% best_val_auc)\n",
    "    \n",
    "        \"\"\"valid의 평균, 표준편차 출력\"\"\"\n",
    "        print(\"=====parameter별 valid, test score=====\")\n",
    "        print(\"Validation 평균: %3f\"%np.mean(valid_res))\n",
    "\n",
    "        if np.mean(valid_res)>max_auc:\n",
    "            print(\"Find new maximum AUC!!\")\n",
    "            max_hy = hy\n",
    "            max_auc = np.mean(valid_res)\n",
    "    \n",
    "    return max_hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26291251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestpar_tuning(fold, train_data_processed, max_hy, features, target):\n",
    "    hy = max_hy\n",
    "    print(\"Max hy:\" ,hy)\n",
    "    \n",
    "\n",
    "    \"\"\"validation\"\"\"\n",
    "    valid_res = []\n",
    "\n",
    "    clf_res = []\n",
    "    preds_prob_res = []\n",
    "    \n",
    "    \"\"\"해당 버전에 추가된 코드\"\"\"    \n",
    "    y_valid_true = []\n",
    "    y_valid_pred = []\n",
    "    \n",
    "    y_valid_subject = []\n",
    "\n",
    "    \n",
    "    for i in range(fold):\n",
    "\n",
    "        df_train = train_data_processed[train_data_processed.kfold != i]  # 5개 중 4개 train에 할당\n",
    "        df_valid = train_data_processed[train_data_processed.kfold == i]  # 5개 중 1개 validation에 할당\n",
    "\n",
    "        X_train = df_train[features].values\n",
    "        Y_train = df_train[target].values\n",
    "        \n",
    "        X_valid = df_valid[features].values\n",
    "        Y_valid = df_valid[target].values\n",
    "\n",
    "        y_valid_subject.append(df_valid['subjectkey'].values)\n",
    "\n",
    "        \n",
    "        \"\"\"해당 버전 추가 코드\"\"\"\n",
    "        y_valid_true.append(Y_valid)  \n",
    "\n",
    "        train_x = torch.Tensor(X_train)\n",
    "        train_y = torch.Tensor(Y_train.flatten())\n",
    "        train_ds = utils.TensorDataset(train_x,train_y)\n",
    "        \n",
    "        #class_sample_count = np.array(\n",
    "        #                           [len(np.where(np.transpose(y_train.flatten()) == t)[0]) for t in np.unique(np.transpose(y_train.flatten()))])\n",
    "        #class_weight = 1/class_sample_count\n",
    "        #samples_weight = np.array([class_weight[t] for t in np.transpose(y_train.flatten())])\n",
    "        #samples_weight = torch.from_numpy(samples_weight)\n",
    "        #sampler = td.WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        train_loader = td.DataLoader(train_ds, batch_size=64,\n",
    "            shuffle=False, num_workers=1)\n",
    "\n",
    "        val_x = torch.Tensor(X_valid)\n",
    "        val_y = torch.Tensor(Y_valid.flatten())\n",
    "        val_ds = utils.TensorDataset(val_x,val_y)\n",
    "        val_loader = td.DataLoader(val_ds, batch_size=len(Y_valid.flatten()),\n",
    "            shuffle=False, num_workers=1)\n",
    "        \n",
    "\n",
    "                    \n",
    "\n",
    "        model_TabNet = TabNet(inp_dim = X_train.shape[1], final_out_dim = 1, n_d=hy[0],n_a=hy[0],n_shared=2,n_ind=2,n_steps=3,relax=1.3,vbs=128).to(device)\n",
    "        model = BayesTabnet(n_samples = 10, device = device, model = model_TabNet).to(device)\n",
    "        criterion, optimizer, scheduler, logger = getOpt(lrs = 'ReduceLROnPlateau', lr = hy[1], weight_decay = hy[2], model = model)\n",
    "        model_SVGD = SVGD(train_loader = train_loader, n_samples = 10, lr = hy[1], lr_noise = hy[2], ntrain = X_train.shape[0], btrain = 64, out = 1, device = device, bayes_net = model)\n",
    "        best_val_auc = 0.\n",
    "        best_val_epoch = 1\n",
    "        for epoch in range(1,  11):\n",
    "            model_SVGD.train(epoch, logger, 0.001, train_loader)\n",
    "            with torch.no_grad():\n",
    "              val_model, val_acc, val_auc = test(model, epoch, logger, val_loader, False, None)\n",
    "              if val_auc > best_val_auc:\n",
    "                torch.save(val_model.state_dict(), 'BayesTabNetMRI_Fold'+str(epoch)+'_.pt')\n",
    "                best_val_auc = val_auc\n",
    "                best_val_epoch = epoch\n",
    "        \n",
    "        \n",
    "        print('Loading Weights from Epoch: %d'%best_val_epoch)\n",
    "        model_weight_path = 'BayesTabNetMRI_Fold'+str(best_val_epoch)+'_.pt'\n",
    "        model.load_state_dict(torch.load(model_weight_path))\n",
    "        \n",
    "        valid_res.append(best_val_auc)\n",
    "        print('[%3d/%4d] '%(i+1, fold),'Valid score: %2f'% best_val_auc)\n",
    "        clf_res.append(model) #해당 fold에서 최고로 성능이 좋은 모델 append \n",
    "\n",
    "    \"\"\"valid의 평균 출력\"\"\"\n",
    "    print(\"Validation 평균: %3f\"%np.mean(valid_res))\n",
    "    \n",
    "    #return test_auc,test_acc ,valid_result, clf_res, preds_prob, X_test, Y_test, y_valid_pred, y_test_pred, y_valid_subject, y_test_subject\n",
    "\n",
    "    return valid_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9ead921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_data_processed, fold, Num_feat, features, target):\n",
    "    name_test = train_data_processed['subjectkey'].values\n",
    "    print(\"-------------------------------Training Begining-------------------------------\")\n",
    "    n_ = [4,8,16]\n",
    "    lr_ = [2e-2, 1e-2, 1e-3, 1e-4]\n",
    "    w_ = [0.01, 0.001, 0.0001]\n",
    "    all_ = [n_, lr_, w_]\n",
    "    h_space = [s for s in itertools.product(*all_)]\n",
    "    \n",
    "    # Start training\n",
    "    max_hy = find_bestpar(fold, train_data_processed, features, target)\n",
    "    # if you want to just test the code, you should use this\n",
    "    #max_hy = h_space[0]\n",
    "    #print(\"Found maximum hyperparmeter, now work with that\")\n",
    "\n",
    "    print(\"-------------------------------Testing Begining-------------------------------\")\n",
    "    valid_auc = bestpar_tuning(fold, train_data_processed, max_hy, features, target)\n",
    "    \n",
    "    #print(\"-------------------------------Important Feature-------------------------------\")\n",
    "    #import_feat=feature(Num_feat, clf_res, test_data_processed, features)\n",
    "    #import_feat=0\n",
    "    #preds_val_prob = clf.predict_proba(X_valid)\n",
    "\n",
    "    return valid_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a7fe7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "    def __init__(self, train_data_processed, Num_FOLDS, Num_feat, features,target):\n",
    "        valid_auc = run(train_data_processed,Num_FOLDS, Num_feat,features, target)\n",
    "    \n",
    "    \n",
    "        self.train_data_processed = train_data_processed\n",
    "        self.valid_auc = valid_auc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16009df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_all= [col for col in train_data_processed.columns if col in delta + theta+ beta1 +beta2 + beta3 + gamma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0817c0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------Training Begining-------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f913976a3447c38ad17061cae48292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training bce: 0.677347, training acc: 0.638021\n",
      "Test epoch: 1, bce: 0.724514, acc: 0.555556, auc: 0.350000, time: 1.094\n",
      "epoch 2, training bce: 0.612295, training acc: 0.638021\n",
      "Test epoch: 2, bce: 0.730756, acc: 0.555556, auc: 0.450000, time: 1.094\n",
      "epoch 3, training bce: 0.567509, training acc: 0.645833\n",
      "Test epoch: 3, bce: 0.756986, acc: 0.555556, auc: 0.400000, time: 1.091\n",
      "epoch 4, training bce: 0.532405, training acc: 0.661458\n",
      "Test epoch: 4, bce: 0.778002, acc: 0.555556, auc: 0.450000, time: 1.141\n",
      "epoch 5, training bce: 0.497442, training acc: 0.717014\n",
      "Test epoch: 5, bce: 0.799884, acc: 0.555556, auc: 0.450000, time: 1.100\n",
      "epoch 6, training bce: 0.463057, training acc: 0.780382\n",
      "Test epoch: 6, bce: 0.806699, acc: 0.555556, auc: 0.500000, time: 1.118\n",
      "epoch 7, training bce: 0.427117, training acc: 0.788194\n",
      "Test epoch: 7, bce: 0.811020, acc: 0.555556, auc: 0.550000, time: 1.136\n",
      "epoch 8, training bce: 0.390806, training acc: 0.796007\n",
      "Test epoch: 8, bce: 0.822956, acc: 0.555556, auc: 0.550000, time: 1.110\n",
      "epoch 9, training bce: 0.353418, training acc: 0.796007\n",
      "Test epoch: 9, bce: 0.839812, acc: 0.555556, auc: 0.550000, time: 1.120\n",
      "epoch 10, training bce: 0.313984, training acc: 0.851562\n",
      "Test epoch: 10, bce: 0.857891, acc: 0.555556, auc: 0.550000, time: 1.110\n",
      "[  1/  10]  Valid score: 0.550000\n",
      "epoch 1, training bce: 0.685643, training acc: 0.685764\n",
      "Test epoch: 1, bce: 0.794077, acc: 0.555556, auc: 0.500000, time: 1.114\n",
      "epoch 2, training bce: 0.602156, training acc: 0.693576\n",
      "Test epoch: 2, bce: 0.814394, acc: 0.333333, auc: 0.350000, time: 1.093\n",
      "epoch 3, training bce: 0.540620, training acc: 0.709201\n",
      "Test epoch: 3, bce: 0.794419, acc: 0.222222, auc: 0.300000, time: 1.097\n",
      "epoch 4, training bce: 0.492637, training acc: 0.820312\n",
      "Test epoch: 4, bce: 0.779304, acc: 0.444444, auc: 0.300000, time: 1.092\n",
      "epoch 5, training bce: 0.454904, training acc: 0.820312\n",
      "Test epoch: 5, bce: 0.773341, acc: 0.555556, auc: 0.300000, time: 1.103\n",
      "epoch 6, training bce: 0.422946, training acc: 0.820312\n",
      "Test epoch: 6, bce: 0.805258, acc: 0.555556, auc: 0.300000, time: 1.119\n",
      "epoch 7, training bce: 0.394780, training acc: 0.828125\n",
      "Test epoch: 7, bce: 0.862296, acc: 0.555556, auc: 0.300000, time: 1.133\n",
      "epoch 8, training bce: 0.369960, training acc: 0.828125\n",
      "Test epoch: 8, bce: 0.924880, acc: 0.555556, auc: 0.300000, time: 1.103\n",
      "epoch 9, training bce: 0.347426, training acc: 0.828125\n",
      "Test epoch: 9, bce: 0.985353, acc: 0.555556, auc: 0.300000, time: 1.089\n",
      "epoch 10, training bce: 0.324679, training acc: 0.835938\n",
      "Test epoch: 10, bce: 1.040534, acc: 0.555556, auc: 0.300000, time: 1.093\n",
      "[  2/  10]  Valid score: 0.500000\n",
      "epoch 1, training bce: 0.685376, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.799995, acc: 0.625000, auc: 0.266667, time: 1.116\n",
      "epoch 2, training bce: 0.600592, training acc: 0.689063\n",
      "Test epoch: 2, bce: 0.695263, acc: 0.500000, auc: 0.466667, time: 1.102\n",
      "epoch 3, training bce: 0.542741, training acc: 0.704688\n",
      "Test epoch: 3, bce: 0.674295, acc: 0.625000, auc: 0.400000, time: 1.121\n",
      "epoch 4, training bce: 0.499295, training acc: 0.704688\n",
      "Test epoch: 4, bce: 0.679451, acc: 0.625000, auc: 0.400000, time: 1.109\n",
      "epoch 5, training bce: 0.465583, training acc: 0.812500\n",
      "Test epoch: 5, bce: 0.689771, acc: 0.625000, auc: 0.466667, time: 1.093\n",
      "epoch 6, training bce: 0.437386, training acc: 0.812500\n",
      "Test epoch: 6, bce: 0.697575, acc: 0.625000, auc: 0.533333, time: 1.094\n",
      "epoch 7, training bce: 0.410751, training acc: 0.812500\n",
      "Test epoch: 7, bce: 0.701275, acc: 0.625000, auc: 0.600000, time: 1.087\n",
      "epoch 8, training bce: 0.385734, training acc: 0.820312\n",
      "Test epoch: 8, bce: 0.708715, acc: 0.625000, auc: 0.600000, time: 1.094\n",
      "epoch 9, training bce: 0.361970, training acc: 0.820312\n",
      "Test epoch: 9, bce: 0.714290, acc: 0.625000, auc: 0.600000, time: 1.103\n",
      "epoch 10, training bce: 0.338530, training acc: 0.828125\n",
      "Test epoch: 10, bce: 0.715168, acc: 0.625000, auc: 0.666667, time: 1.094\n",
      "[  3/  10]  Valid score: 0.666667\n",
      "epoch 1, training bce: 0.699817, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.749650, acc: 0.625000, auc: 0.133333, time: 1.100\n",
      "epoch 2, training bce: 0.626353, training acc: 0.646875\n",
      "Test epoch: 2, bce: 0.745948, acc: 0.625000, auc: 0.333333, time: 1.083\n",
      "epoch 3, training bce: 0.571560, training acc: 0.720313\n",
      "Test epoch: 3, bce: 0.732579, acc: 0.625000, auc: 0.400000, time: 1.091\n",
      "epoch 4, training bce: 0.531261, training acc: 0.720313\n",
      "Test epoch: 4, bce: 0.741995, acc: 0.625000, auc: 0.400000, time: 1.089\n",
      "epoch 5, training bce: 0.500578, training acc: 0.720313\n",
      "Test epoch: 5, bce: 0.756958, acc: 0.625000, auc: 0.333333, time: 1.105\n",
      "epoch 6, training bce: 0.471845, training acc: 0.720313\n",
      "Test epoch: 6, bce: 0.774109, acc: 0.625000, auc: 0.333333, time: 1.096\n",
      "epoch 7, training bce: 0.441895, training acc: 0.770312\n",
      "Test epoch: 7, bce: 0.795452, acc: 0.625000, auc: 0.266667, time: 1.078\n",
      "epoch 8, training bce: 0.410655, training acc: 0.770312\n",
      "Test epoch: 8, bce: 0.818073, acc: 0.625000, auc: 0.266667, time: 1.078\n",
      "epoch 9, training bce: 0.377184, training acc: 0.778125\n",
      "Test epoch: 9, bce: 0.838778, acc: 0.625000, auc: 0.266667, time: 1.092\n",
      "epoch 10, training bce: 0.343753, training acc: 0.778125\n",
      "Test epoch: 10, bce: 0.865500, acc: 0.625000, auc: 0.266667, time: 1.083\n",
      "[  4/  10]  Valid score: 0.400000\n",
      "epoch 1, training bce: 0.639960, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.663870, acc: 0.625000, auc: 0.733333, time: 1.084\n",
      "epoch 2, training bce: 0.572805, training acc: 0.696875\n",
      "Test epoch: 2, bce: 0.678829, acc: 0.625000, auc: 0.600000, time: 1.102\n",
      "epoch 3, training bce: 0.527007, training acc: 0.712500\n",
      "Test epoch: 3, bce: 0.692553, acc: 0.625000, auc: 0.533333, time: 1.085\n",
      "epoch 4, training bce: 0.497835, training acc: 0.762500\n",
      "Test epoch: 4, bce: 0.700755, acc: 0.625000, auc: 0.533333, time: 1.086\n",
      "epoch 5, training bce: 0.472099, training acc: 0.770312\n",
      "Test epoch: 5, bce: 0.717825, acc: 0.625000, auc: 0.533333, time: 1.079\n",
      "epoch 6, training bce: 0.448694, training acc: 0.820312\n",
      "Test epoch: 6, bce: 0.744303, acc: 0.625000, auc: 0.466667, time: 1.083\n",
      "epoch 7, training bce: 0.425246, training acc: 0.820312\n",
      "Test epoch: 7, bce: 0.773720, acc: 0.625000, auc: 0.466667, time: 1.106\n",
      "epoch 8, training bce: 0.401156, training acc: 0.820312\n",
      "Test epoch: 8, bce: 0.800417, acc: 0.625000, auc: 0.466667, time: 1.101\n",
      "epoch 9, training bce: 0.376851, training acc: 0.828125\n",
      "Test epoch: 9, bce: 0.826841, acc: 0.625000, auc: 0.466667, time: 1.102\n",
      "epoch 10, training bce: 0.352147, training acc: 0.828125\n",
      "Test epoch: 10, bce: 0.852681, acc: 0.625000, auc: 0.466667, time: 1.089\n",
      "[  5/  10]  Valid score: 0.733333\n",
      "epoch 1, training bce: 0.667905, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.652921, acc: 0.750000, auc: 0.466667, time: 1.087\n",
      "epoch 2, training bce: 0.613383, training acc: 0.689063\n",
      "Test epoch: 2, bce: 0.616051, acc: 0.750000, auc: 0.733333, time: 1.108\n",
      "epoch 3, training bce: 0.565296, training acc: 0.696875\n",
      "Test epoch: 3, bce: 0.554917, acc: 0.875000, auc: 0.733333, time: 1.116\n",
      "epoch 4, training bce: 0.530970, training acc: 0.754687\n",
      "Test epoch: 4, bce: 0.488096, acc: 0.875000, auc: 0.800000, time: 1.081\n",
      "epoch 5, training bce: 0.497784, training acc: 0.754687\n",
      "Test epoch: 5, bce: 0.444284, acc: 0.875000, auc: 0.866667, time: 1.113\n",
      "epoch 6, training bce: 0.464395, training acc: 0.812500\n",
      "Test epoch: 6, bce: 0.412011, acc: 0.875000, auc: 0.866667, time: 1.088\n",
      "epoch 7, training bce: 0.433368, training acc: 0.812500\n",
      "Test epoch: 7, bce: 0.387097, acc: 0.875000, auc: 0.866667, time: 1.091\n",
      "epoch 8, training bce: 0.402861, training acc: 0.820312\n",
      "Test epoch: 8, bce: 0.370112, acc: 0.875000, auc: 0.866667, time: 1.096\n",
      "epoch 9, training bce: 0.372696, training acc: 0.828125\n",
      "Test epoch: 9, bce: 0.361325, acc: 0.875000, auc: 0.866667, time: 1.103\n",
      "epoch 10, training bce: 0.342776, training acc: 0.828125\n",
      "Test epoch: 10, bce: 0.358812, acc: 0.875000, auc: 0.866667, time: 1.096\n",
      "[  6/  10]  Valid score: 0.866667\n",
      "epoch 1, training bce: 0.666585, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.707380, acc: 0.625000, auc: 0.266667, time: 1.091\n",
      "epoch 2, training bce: 0.590256, training acc: 0.696875\n",
      "Test epoch: 2, bce: 0.800940, acc: 0.500000, auc: 0.400000, time: 1.083\n",
      "epoch 3, training bce: 0.538817, training acc: 0.762500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test epoch: 3, bce: 0.849593, acc: 0.500000, auc: 0.400000, time: 1.086\n",
      "epoch 4, training bce: 0.504546, training acc: 0.762500\n",
      "Test epoch: 4, bce: 0.875438, acc: 0.500000, auc: 0.466667, time: 1.085\n",
      "epoch 5, training bce: 0.475578, training acc: 0.812500\n",
      "Test epoch: 5, bce: 0.881816, acc: 0.500000, auc: 0.600000, time: 1.084\n",
      "epoch 6, training bce: 0.449793, training acc: 0.812500\n",
      "Test epoch: 6, bce: 0.886258, acc: 0.500000, auc: 0.600000, time: 1.085\n",
      "epoch 7, training bce: 0.425245, training acc: 0.820312\n",
      "Test epoch: 7, bce: 0.892823, acc: 0.500000, auc: 0.600000, time: 1.115\n",
      "epoch 8, training bce: 0.401829, training acc: 0.820312\n",
      "Test epoch: 8, bce: 0.907779, acc: 0.500000, auc: 0.733333, time: 1.100\n",
      "epoch 9, training bce: 0.379797, training acc: 0.828125\n",
      "Test epoch: 9, bce: 0.930112, acc: 0.500000, auc: 0.733333, time: 1.075\n",
      "epoch 10, training bce: 0.358829, training acc: 0.828125\n",
      "Test epoch: 10, bce: 0.958248, acc: 0.500000, auc: 0.733333, time: 1.079\n",
      "[  7/  10]  Valid score: 0.733333\n",
      "epoch 1, training bce: 0.689378, training acc: 0.639062\n",
      "Test epoch: 1, bce: 0.650676, acc: 0.750000, auc: 0.866667, time: 1.086\n",
      "epoch 2, training bce: 0.633564, training acc: 0.662500\n",
      "Test epoch: 2, bce: 0.617400, acc: 0.875000, auc: 0.933333, time: 1.097\n",
      "epoch 3, training bce: 0.591554, training acc: 0.720313\n",
      "Test epoch: 3, bce: 0.582686, acc: 0.625000, auc: 1.000000, time: 1.098\n",
      "epoch 4, training bce: 0.557661, training acc: 0.720313\n",
      "Test epoch: 4, bce: 0.558131, acc: 0.625000, auc: 1.000000, time: 1.081\n",
      "epoch 5, training bce: 0.527204, training acc: 0.720313\n",
      "Test epoch: 5, bce: 0.537529, acc: 0.625000, auc: 1.000000, time: 1.070\n",
      "epoch 6, training bce: 0.495770, training acc: 0.720313\n",
      "Test epoch: 6, bce: 0.513046, acc: 0.750000, auc: 1.000000, time: 1.082\n",
      "epoch 7, training bce: 0.464324, training acc: 0.770312\n",
      "Test epoch: 7, bce: 0.481628, acc: 0.750000, auc: 1.000000, time: 1.085\n",
      "epoch 8, training bce: 0.431761, training acc: 0.770312\n",
      "Test epoch: 8, bce: 0.444958, acc: 0.750000, auc: 1.000000, time: 1.087\n",
      "epoch 9, training bce: 0.397234, training acc: 0.785937\n",
      "Test epoch: 9, bce: 0.409047, acc: 0.750000, auc: 1.000000, time: 1.077\n",
      "epoch 10, training bce: 0.359470, training acc: 0.785937\n",
      "Test epoch: 10, bce: 0.379389, acc: 0.750000, auc: 1.000000, time: 1.089\n",
      "[  8/  10]  Valid score: 1.000000\n",
      "epoch 1, training bce: 0.659910, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.756136, acc: 0.625000, auc: 0.200000, time: 1.089\n",
      "epoch 2, training bce: 0.608757, training acc: 0.646875\n",
      "Test epoch: 2, bce: 0.699781, acc: 0.625000, auc: 0.533333, time: 1.085\n",
      "epoch 3, training bce: 0.564086, training acc: 0.646875\n",
      "Test epoch: 3, bce: 0.692375, acc: 0.625000, auc: 0.533333, time: 1.075\n",
      "epoch 4, training bce: 0.529093, training acc: 0.646875\n",
      "Test epoch: 4, bce: 0.662722, acc: 0.625000, auc: 0.600000, time: 1.087\n",
      "epoch 5, training bce: 0.490714, training acc: 0.662500\n",
      "Test epoch: 5, bce: 0.635842, acc: 0.625000, auc: 0.600000, time: 1.094\n",
      "epoch 6, training bce: 0.451386, training acc: 0.712500\n",
      "Test epoch: 6, bce: 0.627056, acc: 0.625000, auc: 0.600000, time: 1.088\n",
      "epoch 7, training bce: 0.410547, training acc: 0.720313\n",
      "Test epoch: 7, bce: 0.642561, acc: 0.625000, auc: 0.600000, time: 1.094\n",
      "epoch 8, training bce: 0.365264, training acc: 0.770312\n",
      "Test epoch: 8, bce: 0.681380, acc: 0.625000, auc: 0.666667, time: 1.081\n",
      "epoch 9, training bce: 0.318558, training acc: 0.828125\n",
      "Test epoch: 9, bce: 0.744113, acc: 0.625000, auc: 0.666667, time: 1.093\n",
      "epoch 10, training bce: 0.268938, training acc: 0.851562\n",
      "Test epoch: 10, bce: 0.826331, acc: 0.625000, auc: 0.666667, time: 1.094\n",
      "[  9/  10]  Valid score: 0.666667\n",
      "epoch 1, training bce: 0.680268, training acc: 0.604688\n",
      "Test epoch: 1, bce: 0.584120, acc: 0.750000, auc: 0.866667, time: 1.095\n",
      "epoch 2, training bce: 0.617020, training acc: 0.612500\n",
      "Test epoch: 2, bce: 0.569517, acc: 0.750000, auc: 0.933333, time: 1.099\n",
      "epoch 3, training bce: 0.576852, training acc: 0.612500\n",
      "Test epoch: 3, bce: 0.570520, acc: 0.750000, auc: 0.866667, time: 1.088\n",
      "epoch 4, training bce: 0.540742, training acc: 0.620313\n",
      "Test epoch: 4, bce: 0.566080, acc: 0.750000, auc: 0.866667, time: 1.094\n",
      "epoch 5, training bce: 0.501921, training acc: 0.670312\n",
      "Test epoch: 5, bce: 0.552812, acc: 0.750000, auc: 0.866667, time: 1.083\n",
      "epoch 6, training bce: 0.461151, training acc: 0.670312\n",
      "Test epoch: 6, bce: 0.534173, acc: 0.750000, auc: 0.866667, time: 1.084\n",
      "epoch 7, training bce: 0.419033, training acc: 0.735938\n",
      "Test epoch: 7, bce: 0.508598, acc: 0.750000, auc: 0.866667, time: 1.097\n",
      "epoch 8, training bce: 0.372981, training acc: 0.785937\n",
      "Test epoch: 8, bce: 0.477371, acc: 0.750000, auc: 0.866667, time: 1.092\n",
      "epoch 9, training bce: 0.325263, training acc: 0.801562\n",
      "Test epoch: 9, bce: 0.448487, acc: 0.750000, auc: 0.866667, time: 1.102\n",
      "epoch 10, training bce: 0.276788, training acc: 0.867188\n",
      "Test epoch: 10, bce: 0.426553, acc: 0.750000, auc: 0.866667, time: 1.107\n",
      "[ 10/  10]  Valid score: 0.933333\n",
      "=====parameter별 valid, test score=====\n",
      "Validation 평균: 0.705000\n",
      "Find new maximum AUC!!\n",
      "epoch 1, training bce: 0.679825, training acc: 0.638021\n",
      "Test epoch: 1, bce: 0.733108, acc: 0.555556, auc: 0.350000, time: 1.074\n",
      "epoch 2, training bce: 0.612797, training acc: 0.645833\n",
      "Test epoch: 2, bce: 0.749109, acc: 0.555556, auc: 0.400000, time: 1.103\n",
      "epoch 3, training bce: 0.567834, training acc: 0.709201\n",
      "Test epoch: 3, bce: 0.736263, acc: 0.555556, auc: 0.450000, time: 1.086\n",
      "epoch 4, training bce: 0.534809, training acc: 0.709201\n",
      "Test epoch: 4, bce: 0.723810, acc: 0.555556, auc: 0.450000, time: 1.095\n",
      "epoch 5, training bce: 0.504383, training acc: 0.717014\n",
      "Test epoch: 5, bce: 0.724415, acc: 0.555556, auc: 0.500000, time: 1.099\n",
      "epoch 6, training bce: 0.473174, training acc: 0.724826\n",
      "Test epoch: 6, bce: 0.738382, acc: 0.555556, auc: 0.500000, time: 1.100\n",
      "epoch 7, training bce: 0.441758, training acc: 0.788194\n",
      "Test epoch: 7, bce: 0.763026, acc: 0.555556, auc: 0.550000, time: 1.109\n",
      "epoch 8, training bce: 0.409583, training acc: 0.788194\n",
      "Test epoch: 8, bce: 0.795895, acc: 0.555556, auc: 0.450000, time: 1.089\n",
      "epoch 9, training bce: 0.375829, training acc: 0.796007\n",
      "Test epoch: 9, bce: 0.834491, acc: 0.555556, auc: 0.450000, time: 1.086\n",
      "epoch 10, training bce: 0.339609, training acc: 0.851562\n",
      "Test epoch: 10, bce: 0.876564, acc: 0.555556, auc: 0.400000, time: 1.090\n",
      "[  1/  10]  Valid score: 0.550000\n",
      "epoch 1, training bce: 0.658918, training acc: 0.685764\n",
      "Test epoch: 1, bce: 0.719590, acc: 0.555556, auc: 0.300000, time: 1.101\n",
      "epoch 2, training bce: 0.602410, training acc: 0.693576\n",
      "Test epoch: 2, bce: 0.709839, acc: 0.555556, auc: 0.300000, time: 1.102\n",
      "epoch 3, training bce: 0.560596, training acc: 0.701389\n",
      "Test epoch: 3, bce: 0.706909, acc: 0.555556, auc: 0.300000, time: 1.083\n",
      "epoch 4, training bce: 0.528506, training acc: 0.709201\n",
      "Test epoch: 4, bce: 0.712461, acc: 0.555556, auc: 0.300000, time: 1.088\n",
      "epoch 5, training bce: 0.495527, training acc: 0.820312\n",
      "Test epoch: 5, bce: 0.723554, acc: 0.555556, auc: 0.300000, time: 1.080\n",
      "epoch 6, training bce: 0.460099, training acc: 0.835938\n",
      "Test epoch: 6, bce: 0.748106, acc: 0.555556, auc: 0.300000, time: 1.074\n",
      "epoch 7, training bce: 0.424228, training acc: 0.835938\n",
      "Test epoch: 7, bce: 0.781997, acc: 0.555556, auc: 0.350000, time: 1.085\n",
      "epoch 8, training bce: 0.390026, training acc: 0.835938\n",
      "Test epoch: 8, bce: 0.822487, acc: 0.555556, auc: 0.350000, time: 1.081\n",
      "epoch 9, training bce: 0.355838, training acc: 0.843750\n",
      "Test epoch: 9, bce: 0.862695, acc: 0.555556, auc: 0.350000, time: 1.099\n",
      "epoch 10, training bce: 0.322941, training acc: 0.851562\n",
      "Test epoch: 10, bce: 0.896685, acc: 0.555556, auc: 0.350000, time: 1.083\n",
      "[  2/  10]  Valid score: 0.350000\n",
      "epoch 1, training bce: 0.685134, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.871543, acc: 0.625000, auc: 0.400000, time: 1.114\n",
      "epoch 2, training bce: 0.622886, training acc: 0.689063\n",
      "Test epoch: 2, bce: 0.784549, acc: 0.625000, auc: 0.533333, time: 1.087\n",
      "epoch 3, training bce: 0.563895, training acc: 0.704688\n",
      "Test epoch: 3, bce: 0.697930, acc: 0.625000, auc: 0.466667, time: 1.088\n",
      "epoch 4, training bce: 0.518313, training acc: 0.762500\n",
      "Test epoch: 4, bce: 0.650253, acc: 0.625000, auc: 0.466667, time: 1.083\n",
      "epoch 5, training bce: 0.481661, training acc: 0.812500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test epoch: 5, bce: 0.635283, acc: 0.625000, auc: 0.600000, time: 1.081\n",
      "epoch 6, training bce: 0.448413, training acc: 0.812500\n",
      "Test epoch: 6, bce: 0.637354, acc: 0.625000, auc: 0.666667, time: 1.100\n",
      "epoch 7, training bce: 0.418092, training acc: 0.812500\n",
      "Test epoch: 7, bce: 0.651989, acc: 0.625000, auc: 0.666667, time: 1.105\n",
      "epoch 8, training bce: 0.389732, training acc: 0.812500\n",
      "Test epoch: 8, bce: 0.690021, acc: 0.625000, auc: 0.666667, time: 1.108\n",
      "epoch 9, training bce: 0.361793, training acc: 0.828125\n",
      "Test epoch: 9, bce: 0.728697, acc: 0.625000, auc: 0.666667, time: 1.090\n",
      "epoch 10, training bce: 0.333493, training acc: 0.828125\n",
      "Test epoch: 10, bce: 0.766564, acc: 0.625000, auc: 0.666667, time: 1.087\n",
      "[  3/  10]  Valid score: 0.666667\n",
      "epoch 1, training bce: 0.652691, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.792704, acc: 0.500000, auc: 0.066667, time: 1.080\n",
      "epoch 2, training bce: 0.584906, training acc: 0.654687\n",
      "Test epoch: 2, bce: 0.814013, acc: 0.500000, auc: 0.066667, time: 1.090\n",
      "epoch 3, training bce: 0.539979, training acc: 0.704688\n",
      "Test epoch: 3, bce: 0.822023, acc: 0.500000, auc: 0.066667, time: 1.086\n",
      "epoch 4, training bce: 0.498522, training acc: 0.728125\n",
      "Test epoch: 4, bce: 0.842540, acc: 0.625000, auc: 0.000000, time: 1.114\n",
      "epoch 5, training bce: 0.458095, training acc: 0.778125\n",
      "Test epoch: 5, bce: 0.883473, acc: 0.625000, auc: 0.000000, time: 1.098\n",
      "epoch 6, training bce: 0.416839, training acc: 0.778125\n",
      "Test epoch: 6, bce: 0.947958, acc: 0.625000, auc: 0.000000, time: 1.092\n",
      "epoch 7, training bce: 0.375455, training acc: 0.778125\n",
      "Test epoch: 7, bce: 1.031329, acc: 0.625000, auc: 0.000000, time: 1.098\n",
      "epoch 8, training bce: 0.331984, training acc: 0.778125\n",
      "Test epoch: 8, bce: 1.115353, acc: 0.625000, auc: 0.000000, time: 1.086\n",
      "epoch 9, training bce: 0.286905, training acc: 0.851562\n",
      "Test epoch: 9, bce: 1.179453, acc: 0.625000, auc: 0.000000, time: 1.079\n",
      "epoch 10, training bce: 0.240845, training acc: 0.859375\n",
      "Test epoch: 10, bce: 1.229417, acc: 0.625000, auc: 0.000000, time: 1.080\n",
      "[  4/  10]  Valid score: 0.066667\n",
      "epoch 1, training bce: 0.646055, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.943745, acc: 0.375000, auc: 0.400000, time: 1.113\n",
      "epoch 2, training bce: 0.578240, training acc: 0.762500\n",
      "Test epoch: 2, bce: 1.000215, acc: 0.500000, auc: 0.400000, time: 1.158\n",
      "epoch 3, training bce: 0.529790, training acc: 0.812500\n",
      "Test epoch: 3, bce: 0.972861, acc: 0.500000, auc: 0.533333, time: 1.096\n",
      "epoch 4, training bce: 0.494416, training acc: 0.812500\n",
      "Test epoch: 4, bce: 0.956985, acc: 0.500000, auc: 0.533333, time: 1.077\n",
      "epoch 5, training bce: 0.462255, training acc: 0.812500\n",
      "Test epoch: 5, bce: 0.955029, acc: 0.500000, auc: 0.533333, time: 1.081\n",
      "epoch 6, training bce: 0.432213, training acc: 0.812500\n",
      "Test epoch: 6, bce: 0.966411, acc: 0.500000, auc: 0.466667, time: 1.082\n",
      "epoch 7, training bce: 0.404614, training acc: 0.812500\n",
      "Test epoch: 7, bce: 0.989583, acc: 0.500000, auc: 0.466667, time: 1.084\n",
      "epoch 8, training bce: 0.378064, training acc: 0.828125\n",
      "Test epoch: 8, bce: 1.017071, acc: 0.500000, auc: 0.466667, time: 1.110\n",
      "epoch 9, training bce: 0.351491, training acc: 0.828125\n",
      "Test epoch: 9, bce: 1.037178, acc: 0.500000, auc: 0.466667, time: 1.114\n",
      "epoch 10, training bce: 0.324735, training acc: 0.835938\n",
      "Test epoch: 10, bce: 1.048781, acc: 0.500000, auc: 0.466667, time: 1.092\n",
      "[  5/  10]  Valid score: 0.533333\n",
      "epoch 1, training bce: 0.703548, training acc: 0.696875\n",
      "Test epoch: 1, bce: 0.610498, acc: 0.750000, auc: 0.733333, time: 1.094\n",
      "epoch 2, training bce: 0.637030, training acc: 0.696875\n",
      "Test epoch: 2, bce: 0.577067, acc: 0.750000, auc: 0.733333, time: 1.087\n",
      "epoch 3, training bce: 0.593940, training acc: 0.696875\n",
      "Test epoch: 3, bce: 0.560950, acc: 0.750000, auc: 0.733333, time: 1.086\n",
      "epoch 4, training bce: 0.563353, training acc: 0.696875\n",
      "Test epoch: 4, bce: 0.550365, acc: 0.750000, auc: 0.733333, time: 1.103\n",
      "epoch 5, training bce: 0.536475, training acc: 0.696875\n",
      "Test epoch: 5, bce: 0.538253, acc: 0.750000, auc: 0.800000, time: 1.095\n",
      "epoch 6, training bce: 0.512317, training acc: 0.696875\n",
      "Test epoch: 6, bce: 0.529283, acc: 0.750000, auc: 0.800000, time: 1.104\n",
      "epoch 7, training bce: 0.489730, training acc: 0.696875\n",
      "Test epoch: 7, bce: 0.521608, acc: 0.750000, auc: 0.866667, time: 1.102\n",
      "epoch 8, training bce: 0.467008, training acc: 0.696875\n",
      "Test epoch: 8, bce: 0.512898, acc: 0.750000, auc: 0.866667, time: 1.092\n",
      "epoch 9, training bce: 0.443048, training acc: 0.754687\n",
      "Test epoch: 9, bce: 0.504207, acc: 0.750000, auc: 0.866667, time: 1.081\n",
      "epoch 10, training bce: 0.417461, training acc: 0.754687\n",
      "Test epoch: 10, bce: 0.500582, acc: 0.750000, auc: 0.800000, time: 1.076\n",
      "[  6/  10]  Valid score: 0.866667\n",
      "epoch 1, training bce: 0.663914, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.822372, acc: 0.625000, auc: 0.666667, time: 1.080\n",
      "epoch 2, training bce: 0.594566, training acc: 0.754687\n",
      "Test epoch: 2, bce: 0.894669, acc: 0.625000, auc: 0.800000, time: 1.102\n",
      "epoch 3, training bce: 0.546124, training acc: 0.762500\n",
      "Test epoch: 3, bce: 0.897937, acc: 0.500000, auc: 0.800000, time: 1.082\n",
      "epoch 4, training bce: 0.508040, training acc: 0.812500\n",
      "Test epoch: 4, bce: 0.924825, acc: 0.500000, auc: 0.800000, time: 1.092\n",
      "epoch 5, training bce: 0.471388, training acc: 0.812500\n",
      "Test epoch: 5, bce: 0.946197, acc: 0.500000, auc: 0.733333, time: 1.092\n",
      "epoch 6, training bce: 0.437185, training acc: 0.820312\n",
      "Test epoch: 6, bce: 0.972249, acc: 0.500000, auc: 0.666667, time: 1.083\n",
      "epoch 7, training bce: 0.405748, training acc: 0.820312\n",
      "Test epoch: 7, bce: 0.999531, acc: 0.500000, auc: 0.666667, time: 1.080\n",
      "epoch 8, training bce: 0.377875, training acc: 0.820312\n",
      "Test epoch: 8, bce: 1.021556, acc: 0.500000, auc: 0.666667, time: 1.088\n",
      "epoch 9, training bce: 0.351438, training acc: 0.820312\n",
      "Test epoch: 9, bce: 1.031066, acc: 0.500000, auc: 0.666667, time: 1.100\n",
      "epoch 10, training bce: 0.323198, training acc: 0.835938\n",
      "Test epoch: 10, bce: 1.033156, acc: 0.500000, auc: 0.666667, time: 1.112\n",
      "[  7/  10]  Valid score: 0.800000\n",
      "epoch 1, training bce: 0.688576, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.568798, acc: 0.750000, auc: 0.866667, time: 1.090\n",
      "epoch 2, training bce: 0.631170, training acc: 0.646875\n",
      "Test epoch: 2, bce: 0.557772, acc: 0.750000, auc: 1.000000, time: 1.073\n",
      "epoch 3, training bce: 0.584310, training acc: 0.720313\n",
      "Test epoch: 3, bce: 0.559055, acc: 0.750000, auc: 0.933333, time: 1.083\n",
      "epoch 4, training bce: 0.547926, training acc: 0.720313\n",
      "Test epoch: 4, bce: 0.538294, acc: 0.750000, auc: 0.933333, time: 1.079\n",
      "epoch 5, training bce: 0.513130, training acc: 0.778125\n",
      "Test epoch: 5, bce: 0.514546, acc: 0.750000, auc: 0.933333, time: 1.087\n",
      "epoch 6, training bce: 0.478081, training acc: 0.778125\n",
      "Test epoch: 6, bce: 0.493194, acc: 0.750000, auc: 1.000000, time: 1.081\n",
      "epoch 7, training bce: 0.443290, training acc: 0.778125\n",
      "Test epoch: 7, bce: 0.474354, acc: 0.750000, auc: 0.933333, time: 1.115\n",
      "epoch 8, training bce: 0.407115, training acc: 0.778125\n",
      "Test epoch: 8, bce: 0.452148, acc: 0.750000, auc: 0.933333, time: 1.099\n",
      "epoch 9, training bce: 0.368674, training acc: 0.778125\n",
      "Test epoch: 9, bce: 0.428031, acc: 0.750000, auc: 0.933333, time: 1.091\n",
      "epoch 10, training bce: 0.327261, training acc: 0.843750\n",
      "Test epoch: 10, bce: 0.403371, acc: 0.750000, auc: 1.000000, time: 1.100\n",
      "[  8/  10]  Valid score: 1.000000\n",
      "epoch 1, training bce: 0.684569, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.711262, acc: 0.625000, auc: 0.533333, time: 1.088\n",
      "epoch 2, training bce: 0.616632, training acc: 0.646875\n",
      "Test epoch: 2, bce: 0.729776, acc: 0.625000, auc: 0.533333, time: 1.087\n",
      "epoch 3, training bce: 0.564519, training acc: 0.646875\n",
      "Test epoch: 3, bce: 0.696177, acc: 0.625000, auc: 0.600000, time: 1.107\n",
      "epoch 4, training bce: 0.528112, training acc: 0.662500\n",
      "Test epoch: 4, bce: 0.663352, acc: 0.625000, auc: 0.600000, time: 1.121\n",
      "epoch 5, training bce: 0.496968, training acc: 0.720313\n",
      "Test epoch: 5, bce: 0.640260, acc: 0.625000, auc: 0.666667, time: 1.093\n",
      "epoch 6, training bce: 0.468472, training acc: 0.720313\n",
      "Test epoch: 6, bce: 0.641603, acc: 0.625000, auc: 0.666667, time: 1.084\n",
      "epoch 7, training bce: 0.440140, training acc: 0.720313\n",
      "Test epoch: 7, bce: 0.673297, acc: 0.625000, auc: 0.600000, time: 1.088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, training bce: 0.409122, training acc: 0.770312\n",
      "Test epoch: 8, bce: 0.731268, acc: 0.625000, auc: 0.600000, time: 1.085\n",
      "epoch 9, training bce: 0.375597, training acc: 0.778125\n",
      "Test epoch: 9, bce: 0.802536, acc: 0.625000, auc: 0.533333, time: 1.090\n",
      "epoch 10, training bce: 0.339970, training acc: 0.828125\n",
      "Test epoch: 10, bce: 0.881515, acc: 0.625000, auc: 0.533333, time: 1.086\n",
      "[  9/  10]  Valid score: 0.666667\n",
      "epoch 1, training bce: 0.717999, training acc: 0.581250\n",
      "Test epoch: 1, bce: 0.690093, acc: 0.750000, auc: 0.866667, time: 1.111\n",
      "epoch 2, training bce: 0.646660, training acc: 0.662500\n",
      "Test epoch: 2, bce: 0.651922, acc: 0.750000, auc: 0.866667, time: 1.154\n",
      "epoch 3, training bce: 0.607515, training acc: 0.662500\n",
      "Test epoch: 3, bce: 0.632150, acc: 0.625000, auc: 0.866667, time: 1.116\n",
      "epoch 4, training bce: 0.574124, training acc: 0.662500\n",
      "Test epoch: 4, bce: 0.617652, acc: 0.750000, auc: 0.866667, time: 1.094\n",
      "epoch 5, training bce: 0.538215, training acc: 0.662500\n",
      "Test epoch: 5, bce: 0.598719, acc: 0.750000, auc: 0.866667, time: 1.091\n",
      "epoch 6, training bce: 0.499432, training acc: 0.678125\n",
      "Test epoch: 6, bce: 0.573246, acc: 0.750000, auc: 0.866667, time: 1.099\n",
      "epoch 7, training bce: 0.459483, training acc: 0.743750\n",
      "Test epoch: 7, bce: 0.546887, acc: 0.625000, auc: 0.866667, time: 1.102\n",
      "epoch 8, training bce: 0.417544, training acc: 0.793750\n",
      "Test epoch: 8, bce: 0.520103, acc: 0.750000, auc: 0.866667, time: 1.094\n",
      "epoch 9, training bce: 0.373967, training acc: 0.801562\n",
      "Test epoch: 9, bce: 0.494706, acc: 0.750000, auc: 0.866667, time: 1.105\n",
      "epoch 10, training bce: 0.327328, training acc: 0.859375\n",
      "Test epoch: 10, bce: 0.469369, acc: 0.625000, auc: 0.866667, time: 1.095\n",
      "[ 10/  10]  Valid score: 0.866667\n",
      "=====parameter별 valid, test score=====\n",
      "Validation 평균: 0.636667\n",
      "epoch 1, training bce: 0.683174, training acc: 0.638021\n",
      "Test epoch: 1, bce: 0.698085, acc: 0.555556, auc: 0.350000, time: 1.093\n",
      "epoch 2, training bce: 0.643542, training acc: 0.645833\n",
      "Test epoch: 2, bce: 0.696684, acc: 0.555556, auc: 0.450000, time: 1.082\n",
      "epoch 3, training bce: 0.619547, training acc: 0.653646\n",
      "Test epoch: 3, bce: 0.696350, acc: 0.555556, auc: 0.400000, time: 1.081\n",
      "epoch 4, training bce: 0.604750, training acc: 0.653646\n",
      "Test epoch: 4, bce: 0.695817, acc: 0.555556, auc: 0.400000, time: 1.085\n",
      "epoch 5, training bce: 0.592976, training acc: 0.669271\n",
      "Test epoch: 5, bce: 0.695332, acc: 0.555556, auc: 0.450000, time: 1.100\n",
      "epoch 6, training bce: 0.581305, training acc: 0.677083\n",
      "Test epoch: 6, bce: 0.695001, acc: 0.555556, auc: 0.450000, time: 1.092\n",
      "epoch 7, training bce: 0.571160, training acc: 0.732639\n",
      "Test epoch: 7, bce: 0.694668, acc: 0.555556, auc: 0.450000, time: 1.122\n",
      "epoch 8, training bce: 0.562195, training acc: 0.740451\n",
      "Test epoch: 8, bce: 0.694223, acc: 0.555556, auc: 0.450000, time: 1.102\n",
      "epoch 9, training bce: 0.553518, training acc: 0.796007\n",
      "Test epoch: 9, bce: 0.693790, acc: 0.555556, auc: 0.450000, time: 1.089\n",
      "epoch 10, training bce: 0.545847, training acc: 0.796007\n",
      "Test epoch: 10, bce: 0.693452, acc: 0.555556, auc: 0.450000, time: 1.098\n",
      "[  1/  10]  Valid score: 0.450000\n",
      "epoch 1, training bce: 0.705250, training acc: 0.677951\n",
      "Test epoch: 1, bce: 0.694692, acc: 0.555556, auc: 0.350000, time: 1.082\n",
      "epoch 2, training bce: 0.644372, training acc: 0.709201\n",
      "Test epoch: 2, bce: 0.695373, acc: 0.555556, auc: 0.350000, time: 1.086\n",
      "epoch 3, training bce: 0.614992, training acc: 0.828125\n",
      "Test epoch: 3, bce: 0.695849, acc: 0.555556, auc: 0.450000, time: 1.091\n",
      "epoch 4, training bce: 0.602023, training acc: 0.835938\n",
      "Test epoch: 4, bce: 0.695910, acc: 0.555556, auc: 0.400000, time: 1.107\n",
      "epoch 5, training bce: 0.591269, training acc: 0.843750\n",
      "Test epoch: 5, bce: 0.695719, acc: 0.555556, auc: 0.400000, time: 1.096\n",
      "epoch 6, training bce: 0.582003, training acc: 0.843750\n",
      "Test epoch: 6, bce: 0.695543, acc: 0.555556, auc: 0.400000, time: 1.090\n",
      "epoch 7, training bce: 0.574495, training acc: 0.843750\n",
      "Test epoch: 7, bce: 0.695393, acc: 0.555556, auc: 0.400000, time: 1.081\n",
      "epoch 8, training bce: 0.567424, training acc: 0.843750\n",
      "Test epoch: 8, bce: 0.695356, acc: 0.555556, auc: 0.400000, time: 1.080\n",
      "epoch 9, training bce: 0.560986, training acc: 0.843750\n",
      "Test epoch: 9, bce: 0.695388, acc: 0.555556, auc: 0.400000, time: 1.078\n",
      "epoch 10, training bce: 0.555103, training acc: 0.843750\n",
      "Test epoch: 10, bce: 0.695463, acc: 0.555556, auc: 0.400000, time: 1.100\n",
      "[  2/  10]  Valid score: 0.450000\n",
      "epoch 1, training bce: 0.703301, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.713928, acc: 0.625000, auc: 0.866667, time: 1.086\n",
      "epoch 2, training bce: 0.664420, training acc: 0.704688\n",
      "Test epoch: 2, bce: 0.716102, acc: 0.625000, auc: 0.733333, time: 1.122\n",
      "epoch 3, training bce: 0.639707, training acc: 0.812500\n",
      "Test epoch: 3, bce: 0.716342, acc: 0.625000, auc: 0.800000, time: 1.084\n",
      "epoch 4, training bce: 0.621891, training acc: 0.812500\n",
      "Test epoch: 4, bce: 0.715824, acc: 0.625000, auc: 0.666667, time: 1.087\n",
      "epoch 5, training bce: 0.610032, training acc: 0.812500\n",
      "Test epoch: 5, bce: 0.714880, acc: 0.625000, auc: 0.600000, time: 1.081\n",
      "epoch 6, training bce: 0.599502, training acc: 0.812500\n",
      "Test epoch: 6, bce: 0.713507, acc: 0.625000, auc: 0.533333, time: 1.084\n",
      "epoch 7, training bce: 0.589534, training acc: 0.812500\n",
      "Test epoch: 7, bce: 0.712623, acc: 0.625000, auc: 0.533333, time: 1.096\n",
      "epoch 8, training bce: 0.579713, training acc: 0.820312\n",
      "Test epoch: 8, bce: 0.711989, acc: 0.625000, auc: 0.466667, time: 1.096\n",
      "epoch 9, training bce: 0.570803, training acc: 0.828125\n",
      "Test epoch: 9, bce: 0.711433, acc: 0.625000, auc: 0.466667, time: 1.094\n",
      "epoch 10, training bce: 0.562237, training acc: 0.835938\n",
      "Test epoch: 10, bce: 0.710718, acc: 0.625000, auc: 0.533333, time: 1.080\n",
      "[  3/  10]  Valid score: 0.866667\n",
      "epoch 1, training bce: 0.688222, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.689803, acc: 0.625000, auc: 0.400000, time: 1.074\n",
      "epoch 2, training bce: 0.631754, training acc: 0.654687\n",
      "Test epoch: 2, bce: 0.692251, acc: 0.625000, auc: 0.266667, time: 1.076\n",
      "epoch 3, training bce: 0.608067, training acc: 0.654687\n",
      "Test epoch: 3, bce: 0.692306, acc: 0.625000, auc: 0.266667, time: 1.078\n",
      "epoch 4, training bce: 0.591462, training acc: 0.654687\n",
      "Test epoch: 4, bce: 0.691919, acc: 0.625000, auc: 0.266667, time: 1.098\n",
      "epoch 5, training bce: 0.578496, training acc: 0.704688\n",
      "Test epoch: 5, bce: 0.691326, acc: 0.625000, auc: 0.266667, time: 1.111\n",
      "epoch 6, training bce: 0.566842, training acc: 0.704688\n",
      "Test epoch: 6, bce: 0.690789, acc: 0.625000, auc: 0.266667, time: 1.094\n",
      "epoch 7, training bce: 0.555144, training acc: 0.704688\n",
      "Test epoch: 7, bce: 0.690643, acc: 0.625000, auc: 0.266667, time: 1.086\n",
      "epoch 8, training bce: 0.545208, training acc: 0.712500\n",
      "Test epoch: 8, bce: 0.690704, acc: 0.625000, auc: 0.266667, time: 1.087\n",
      "epoch 9, training bce: 0.535398, training acc: 0.720313\n",
      "Test epoch: 9, bce: 0.690734, acc: 0.625000, auc: 0.200000, time: 1.085\n",
      "epoch 10, training bce: 0.525732, training acc: 0.720313\n",
      "Test epoch: 10, bce: 0.690743, acc: 0.625000, auc: 0.200000, time: 1.081\n",
      "[  4/  10]  Valid score: 0.400000\n",
      "epoch 1, training bce: 0.720797, training acc: 0.681250\n",
      "Test epoch: 1, bce: 0.708356, acc: 0.625000, auc: 0.333333, time: 1.091\n",
      "epoch 2, training bce: 0.668346, training acc: 0.696875\n",
      "Test epoch: 2, bce: 0.707228, acc: 0.625000, auc: 0.466667, time: 1.089\n",
      "epoch 3, training bce: 0.639609, training acc: 0.762500\n",
      "Test epoch: 3, bce: 0.706078, acc: 0.625000, auc: 0.400000, time: 1.086\n",
      "epoch 4, training bce: 0.619321, training acc: 0.820312\n",
      "Test epoch: 4, bce: 0.704978, acc: 0.625000, auc: 0.466667, time: 1.090\n",
      "epoch 5, training bce: 0.604715, training acc: 0.820312\n",
      "Test epoch: 5, bce: 0.703974, acc: 0.625000, auc: 0.466667, time: 1.086\n",
      "epoch 6, training bce: 0.593822, training acc: 0.820312\n",
      "Test epoch: 6, bce: 0.702947, acc: 0.625000, auc: 0.466667, time: 1.077\n",
      "epoch 7, training bce: 0.584377, training acc: 0.828125\n",
      "Test epoch: 7, bce: 0.701691, acc: 0.625000, auc: 0.466667, time: 1.084\n",
      "epoch 8, training bce: 0.574587, training acc: 0.828125\n",
      "Test epoch: 8, bce: 0.699825, acc: 0.625000, auc: 0.466667, time: 1.094\n",
      "epoch 9, training bce: 0.565467, training acc: 0.828125\n",
      "Test epoch: 9, bce: 0.697882, acc: 0.625000, auc: 0.466667, time: 1.090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, training bce: 0.555801, training acc: 0.835938\n",
      "Test epoch: 10, bce: 0.696099, acc: 0.625000, auc: 0.466667, time: 1.080\n",
      "[  5/  10]  Valid score: 0.466667\n",
      "epoch 1, training bce: 0.685136, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.675230, acc: 0.625000, auc: 0.866667, time: 1.099\n",
      "epoch 2, training bce: 0.630430, training acc: 0.689063\n",
      "Test epoch: 2, bce: 0.670519, acc: 0.625000, auc: 0.866667, time: 1.090\n",
      "epoch 3, training bce: 0.602411, training acc: 0.696875\n",
      "Test epoch: 3, bce: 0.667482, acc: 0.625000, auc: 0.866667, time: 1.081\n",
      "epoch 4, training bce: 0.583295, training acc: 0.704688\n",
      "Test epoch: 4, bce: 0.665572, acc: 0.625000, auc: 0.800000, time: 1.083\n",
      "epoch 5, training bce: 0.570788, training acc: 0.754687\n",
      "Test epoch: 5, bce: 0.663880, acc: 0.625000, auc: 0.800000, time: 1.093\n",
      "epoch 6, training bce: 0.558429, training acc: 0.754687\n",
      "Test epoch: 6, bce: 0.662675, acc: 0.625000, auc: 0.800000, time: 1.094\n",
      "epoch 7, training bce: 0.548159, training acc: 0.754687\n",
      "Test epoch: 7, bce: 0.661500, acc: 0.625000, auc: 0.800000, time: 1.093\n",
      "epoch 8, training bce: 0.539234, training acc: 0.754687\n",
      "Test epoch: 8, bce: 0.660166, acc: 0.625000, auc: 0.800000, time: 1.085\n",
      "epoch 9, training bce: 0.530679, training acc: 0.754687\n",
      "Test epoch: 9, bce: 0.658722, acc: 0.625000, auc: 0.800000, time: 1.082\n",
      "epoch 10, training bce: 0.522787, training acc: 0.804688\n",
      "Test epoch: 10, bce: 0.657098, acc: 0.625000, auc: 0.800000, time: 1.083\n",
      "[  6/  10]  Valid score: 0.866667\n",
      "epoch 1, training bce: 0.711891, training acc: 0.696875\n",
      "Test epoch: 1, bce: 0.721167, acc: 0.625000, auc: 0.400000, time: 1.082\n",
      "epoch 2, training bce: 0.657851, training acc: 0.754687\n",
      "Test epoch: 2, bce: 0.721462, acc: 0.625000, auc: 0.666667, time: 1.083\n",
      "epoch 3, training bce: 0.630896, training acc: 0.828125\n",
      "Test epoch: 3, bce: 0.721100, acc: 0.625000, auc: 0.733333, time: 1.549\n",
      "epoch 4, training bce: 0.616409, training acc: 0.828125\n",
      "Test epoch: 4, bce: 0.719886, acc: 0.625000, auc: 0.733333, time: 1.091\n",
      "epoch 5, training bce: 0.604405, training acc: 0.828125\n",
      "Test epoch: 5, bce: 0.718918, acc: 0.625000, auc: 0.733333, time: 1.105\n",
      "epoch 6, training bce: 0.594572, training acc: 0.828125\n",
      "Test epoch: 6, bce: 0.717848, acc: 0.625000, auc: 0.733333, time: 1.087\n",
      "epoch 7, training bce: 0.585874, training acc: 0.828125\n",
      "Test epoch: 7, bce: 0.716787, acc: 0.625000, auc: 0.800000, time: 1.087\n",
      "epoch 8, training bce: 0.578229, training acc: 0.828125\n",
      "Test epoch: 8, bce: 0.715720, acc: 0.625000, auc: 0.800000, time: 1.087\n",
      "epoch 9, training bce: 0.570523, training acc: 0.835938\n",
      "Test epoch: 9, bce: 0.714580, acc: 0.625000, auc: 0.800000, time: 1.092\n",
      "epoch 10, training bce: 0.563413, training acc: 0.835938\n",
      "Test epoch: 10, bce: 0.713628, acc: 0.625000, auc: 0.800000, time: 1.093\n",
      "[  7/  10]  Valid score: 0.800000\n",
      "epoch 1, training bce: 0.684986, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.681157, acc: 0.625000, auc: 0.933333, time: 1.097\n",
      "epoch 2, training bce: 0.640489, training acc: 0.662500\n",
      "Test epoch: 2, bce: 0.682882, acc: 0.625000, auc: 0.933333, time: 1.091\n",
      "epoch 3, training bce: 0.611701, training acc: 0.670312\n",
      "Test epoch: 3, bce: 0.683436, acc: 0.625000, auc: 0.800000, time: 1.080\n",
      "epoch 4, training bce: 0.596293, training acc: 0.778125\n",
      "Test epoch: 4, bce: 0.682821, acc: 0.625000, auc: 0.733333, time: 1.080\n",
      "epoch 5, training bce: 0.583731, training acc: 0.778125\n",
      "Test epoch: 5, bce: 0.681239, acc: 0.625000, auc: 0.733333, time: 1.089\n",
      "epoch 6, training bce: 0.572733, training acc: 0.778125\n",
      "Test epoch: 6, bce: 0.679171, acc: 0.625000, auc: 0.733333, time: 1.089\n",
      "epoch 7, training bce: 0.562769, training acc: 0.778125\n",
      "Test epoch: 7, bce: 0.676829, acc: 0.625000, auc: 0.866667, time: 1.106\n",
      "epoch 8, training bce: 0.553118, training acc: 0.778125\n",
      "Test epoch: 8, bce: 0.674424, acc: 0.625000, auc: 0.933333, time: 1.099\n",
      "epoch 9, training bce: 0.543576, training acc: 0.778125\n",
      "Test epoch: 9, bce: 0.672067, acc: 0.625000, auc: 0.933333, time: 1.094\n",
      "epoch 10, training bce: 0.535200, training acc: 0.778125\n",
      "Test epoch: 10, bce: 0.669455, acc: 0.625000, auc: 1.000000, time: 1.089\n",
      "[  8/  10]  Valid score: 1.000000\n",
      "epoch 1, training bce: 0.683376, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.690940, acc: 0.625000, auc: 0.466667, time: 1.076\n",
      "epoch 2, training bce: 0.637546, training acc: 0.662500\n",
      "Test epoch: 2, bce: 0.681889, acc: 0.625000, auc: 0.333333, time: 1.080\n",
      "epoch 3, training bce: 0.612490, training acc: 0.678125\n",
      "Test epoch: 3, bce: 0.676683, acc: 0.625000, auc: 0.333333, time: 1.078\n",
      "epoch 4, training bce: 0.593325, training acc: 0.778125\n",
      "Test epoch: 4, bce: 0.675702, acc: 0.625000, auc: 0.333333, time: 1.087\n",
      "epoch 5, training bce: 0.581806, training acc: 0.778125\n",
      "Test epoch: 5, bce: 0.674299, acc: 0.625000, auc: 0.466667, time: 1.103\n",
      "epoch 6, training bce: 0.570262, training acc: 0.785937\n",
      "Test epoch: 6, bce: 0.672779, acc: 0.625000, auc: 0.466667, time: 1.089\n",
      "epoch 7, training bce: 0.560264, training acc: 0.785937\n",
      "Test epoch: 7, bce: 0.671674, acc: 0.625000, auc: 0.600000, time: 1.094\n",
      "epoch 8, training bce: 0.550869, training acc: 0.785937\n",
      "Test epoch: 8, bce: 0.669759, acc: 0.625000, auc: 0.600000, time: 1.090\n",
      "epoch 9, training bce: 0.541996, training acc: 0.785937\n",
      "Test epoch: 9, bce: 0.668204, acc: 0.625000, auc: 0.600000, time: 1.094\n",
      "epoch 10, training bce: 0.533215, training acc: 0.785937\n",
      "Test epoch: 10, bce: 0.667035, acc: 0.625000, auc: 0.666667, time: 1.089\n",
      "[  9/  10]  Valid score: 0.666667\n",
      "epoch 1, training bce: 0.691927, training acc: 0.604688\n",
      "Test epoch: 1, bce: 0.697168, acc: 0.625000, auc: 0.733333, time: 1.084\n",
      "epoch 2, training bce: 0.641614, training acc: 0.635938\n",
      "Test epoch: 2, bce: 0.694005, acc: 0.625000, auc: 0.866667, time: 1.101\n",
      "epoch 3, training bce: 0.614393, training acc: 0.685937\n",
      "Test epoch: 3, bce: 0.691464, acc: 0.625000, auc: 0.866667, time: 1.092\n",
      "epoch 4, training bce: 0.598570, training acc: 0.693750\n",
      "Test epoch: 4, bce: 0.690249, acc: 0.625000, auc: 0.866667, time: 1.092\n",
      "epoch 5, training bce: 0.583994, training acc: 0.693750\n",
      "Test epoch: 5, bce: 0.689227, acc: 0.625000, auc: 0.733333, time: 1.131\n",
      "epoch 6, training bce: 0.573192, training acc: 0.693750\n",
      "Test epoch: 6, bce: 0.687989, acc: 0.625000, auc: 0.733333, time: 1.094\n",
      "epoch 7, training bce: 0.563509, training acc: 0.701562\n",
      "Test epoch: 7, bce: 0.686537, acc: 0.625000, auc: 0.733333, time: 1.093\n",
      "epoch 8, training bce: 0.554507, training acc: 0.751563\n",
      "Test epoch: 8, bce: 0.684761, acc: 0.625000, auc: 0.800000, time: 1.101\n",
      "epoch 9, training bce: 0.545750, training acc: 0.751563\n",
      "Test epoch: 9, bce: 0.682890, acc: 0.625000, auc: 0.866667, time: 1.115\n",
      "epoch 10, training bce: 0.536881, training acc: 0.801562\n",
      "Test epoch: 10, bce: 0.680990, acc: 0.625000, auc: 0.866667, time: 1.087\n",
      "[ 10/  10]  Valid score: 0.866667\n",
      "=====parameter별 valid, test score=====\n",
      "Validation 평균: 0.683333\n",
      "epoch 1, training bce: 0.692650, training acc: 0.645833\n",
      "Test epoch: 1, bce: 0.702660, acc: 0.555556, auc: 0.150000, time: 1.084\n",
      "epoch 2, training bce: 0.636954, training acc: 0.709201\n",
      "Test epoch: 2, bce: 0.700269, acc: 0.555556, auc: 0.200000, time: 1.079\n",
      "epoch 3, training bce: 0.616460, training acc: 0.717014\n",
      "Test epoch: 3, bce: 0.699567, acc: 0.555556, auc: 0.300000, time: 1.078\n",
      "epoch 4, training bce: 0.600304, training acc: 0.732639\n",
      "Test epoch: 4, bce: 0.699024, acc: 0.555556, auc: 0.300000, time: 1.081\n",
      "epoch 5, training bce: 0.587372, training acc: 0.732639\n",
      "Test epoch: 5, bce: 0.698439, acc: 0.555556, auc: 0.300000, time: 1.094\n",
      "epoch 6, training bce: 0.575639, training acc: 0.788194\n",
      "Test epoch: 6, bce: 0.698215, acc: 0.555556, auc: 0.300000, time: 1.091\n",
      "epoch 7, training bce: 0.564095, training acc: 0.796007\n",
      "Test epoch: 7, bce: 0.698131, acc: 0.555556, auc: 0.350000, time: 1.100\n",
      "epoch 8, training bce: 0.554964, training acc: 0.796007\n",
      "Test epoch: 8, bce: 0.698099, acc: 0.555556, auc: 0.300000, time: 1.086\n",
      "epoch 9, training bce: 0.547338, training acc: 0.796007\n",
      "Test epoch: 9, bce: 0.698095, acc: 0.555556, auc: 0.300000, time: 1.087\n",
      "epoch 10, training bce: 0.540553, training acc: 0.796007\n",
      "Test epoch: 10, bce: 0.698124, acc: 0.555556, auc: 0.300000, time: 1.085\n",
      "[  1/  10]  Valid score: 0.350000\n",
      "epoch 1, training bce: 0.688994, training acc: 0.685764\n",
      "Test epoch: 1, bce: 0.693498, acc: 0.555556, auc: 0.400000, time: 1.086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, training bce: 0.626333, training acc: 0.693576\n",
      "Test epoch: 2, bce: 0.694100, acc: 0.555556, auc: 0.400000, time: 1.103\n",
      "epoch 3, training bce: 0.585402, training acc: 0.764757\n",
      "Test epoch: 3, bce: 0.694789, acc: 0.555556, auc: 0.400000, time: 1.087\n",
      "epoch 4, training bce: 0.563534, training acc: 0.772569\n",
      "Test epoch: 4, bce: 0.695154, acc: 0.555556, auc: 0.450000, time: 1.085\n",
      "epoch 5, training bce: 0.547639, training acc: 0.772569\n",
      "Test epoch: 5, bce: 0.695393, acc: 0.555556, auc: 0.400000, time: 1.075\n",
      "epoch 6, training bce: 0.534321, training acc: 0.780382\n",
      "Test epoch: 6, bce: 0.695520, acc: 0.555556, auc: 0.400000, time: 1.075\n",
      "epoch 7, training bce: 0.522824, training acc: 0.835938\n",
      "Test epoch: 7, bce: 0.695809, acc: 0.555556, auc: 0.400000, time: 1.074\n",
      "epoch 8, training bce: 0.511949, training acc: 0.835938\n",
      "Test epoch: 8, bce: 0.696165, acc: 0.555556, auc: 0.400000, time: 1.086\n",
      "epoch 9, training bce: 0.502149, training acc: 0.843750\n",
      "Test epoch: 9, bce: 0.696552, acc: 0.555556, auc: 0.400000, time: 1.083\n",
      "epoch 10, training bce: 0.492857, training acc: 0.843750\n",
      "Test epoch: 10, bce: 0.696853, acc: 0.555556, auc: 0.400000, time: 1.097\n",
      "[  2/  10]  Valid score: 0.450000\n",
      "epoch 1, training bce: 0.635543, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.668205, acc: 0.625000, auc: 0.533333, time: 1.090\n",
      "epoch 2, training bce: 0.578307, training acc: 0.689063\n",
      "Test epoch: 2, bce: 0.672882, acc: 0.625000, auc: 0.533333, time: 1.096\n",
      "epoch 3, training bce: 0.558963, training acc: 0.689063\n",
      "Test epoch: 3, bce: 0.674460, acc: 0.625000, auc: 0.533333, time: 1.082\n",
      "epoch 4, training bce: 0.542142, training acc: 0.689063\n",
      "Test epoch: 4, bce: 0.675463, acc: 0.625000, auc: 0.533333, time: 1.084\n",
      "epoch 5, training bce: 0.528885, training acc: 0.689063\n",
      "Test epoch: 5, bce: 0.676505, acc: 0.625000, auc: 0.533333, time: 1.116\n",
      "epoch 6, training bce: 0.516964, training acc: 0.696875\n",
      "Test epoch: 6, bce: 0.677435, acc: 0.625000, auc: 0.533333, time: 1.109\n",
      "epoch 7, training bce: 0.506372, training acc: 0.696875\n",
      "Test epoch: 7, bce: 0.677746, acc: 0.625000, auc: 0.533333, time: 1.121\n",
      "epoch 8, training bce: 0.496594, training acc: 0.746875\n",
      "Test epoch: 8, bce: 0.677883, acc: 0.625000, auc: 0.533333, time: 1.084\n",
      "epoch 9, training bce: 0.487011, training acc: 0.746875\n",
      "Test epoch: 9, bce: 0.677656, acc: 0.625000, auc: 0.533333, time: 1.078\n",
      "epoch 10, training bce: 0.478223, training acc: 0.746875\n",
      "Test epoch: 10, bce: 0.676881, acc: 0.625000, auc: 0.533333, time: 1.080\n",
      "[  3/  10]  Valid score: 0.533333\n",
      "epoch 1, training bce: 0.686485, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.698978, acc: 0.625000, auc: 0.400000, time: 1.087\n",
      "epoch 2, training bce: 0.646744, training acc: 0.646875\n",
      "Test epoch: 2, bce: 0.699572, acc: 0.625000, auc: 0.400000, time: 1.093\n",
      "epoch 3, training bce: 0.613626, training acc: 0.662500\n",
      "Test epoch: 3, bce: 0.699175, acc: 0.625000, auc: 0.333333, time: 1.092\n",
      "epoch 4, training bce: 0.594738, training acc: 0.720313\n",
      "Test epoch: 4, bce: 0.698767, acc: 0.625000, auc: 0.266667, time: 1.088\n",
      "epoch 5, training bce: 0.578881, training acc: 0.728125\n",
      "Test epoch: 5, bce: 0.698827, acc: 0.625000, auc: 0.266667, time: 1.086\n",
      "epoch 6, training bce: 0.565154, training acc: 0.728125\n",
      "Test epoch: 6, bce: 0.698714, acc: 0.625000, auc: 0.200000, time: 1.088\n",
      "epoch 7, training bce: 0.554038, training acc: 0.728125\n",
      "Test epoch: 7, bce: 0.698322, acc: 0.625000, auc: 0.200000, time: 1.089\n",
      "epoch 8, training bce: 0.543390, training acc: 0.778125\n",
      "Test epoch: 8, bce: 0.697995, acc: 0.625000, auc: 0.066667, time: 1.085\n",
      "epoch 9, training bce: 0.533865, training acc: 0.778125\n",
      "Test epoch: 9, bce: 0.697464, acc: 0.625000, auc: 0.066667, time: 1.088\n",
      "epoch 10, training bce: 0.524244, training acc: 0.778125\n",
      "Test epoch: 10, bce: 0.696982, acc: 0.625000, auc: 0.066667, time: 1.088\n",
      "[  4/  10]  Valid score: 0.400000\n",
      "epoch 1, training bce: 0.706514, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.707349, acc: 0.625000, auc: 0.533333, time: 1.099\n",
      "epoch 2, training bce: 0.657279, training acc: 0.689063\n",
      "Test epoch: 2, bce: 0.708627, acc: 0.625000, auc: 0.533333, time: 1.095\n",
      "epoch 3, training bce: 0.633097, training acc: 0.812500\n",
      "Test epoch: 3, bce: 0.709175, acc: 0.625000, auc: 0.466667, time: 1.092\n",
      "epoch 4, training bce: 0.616984, training acc: 0.828125\n",
      "Test epoch: 4, bce: 0.709181, acc: 0.625000, auc: 0.466667, time: 1.078\n",
      "epoch 5, training bce: 0.604865, training acc: 0.828125\n",
      "Test epoch: 5, bce: 0.708758, acc: 0.625000, auc: 0.466667, time: 1.088\n",
      "epoch 6, training bce: 0.593516, training acc: 0.828125\n",
      "Test epoch: 6, bce: 0.707884, acc: 0.625000, auc: 0.466667, time: 1.097\n",
      "epoch 7, training bce: 0.583978, training acc: 0.835938\n",
      "Test epoch: 7, bce: 0.706508, acc: 0.625000, auc: 0.466667, time: 1.125\n",
      "epoch 8, training bce: 0.575485, training acc: 0.835938\n",
      "Test epoch: 8, bce: 0.705464, acc: 0.625000, auc: 0.466667, time: 1.095\n",
      "epoch 9, training bce: 0.567586, training acc: 0.835938\n",
      "Test epoch: 9, bce: 0.704527, acc: 0.625000, auc: 0.466667, time: 1.097\n",
      "epoch 10, training bce: 0.559840, training acc: 0.835938\n",
      "Test epoch: 10, bce: 0.703682, acc: 0.625000, auc: 0.466667, time: 1.089\n",
      "[  5/  10]  Valid score: 0.533333\n",
      "epoch 1, training bce: 0.672552, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.688154, acc: 0.625000, auc: 0.333333, time: 1.087\n",
      "epoch 2, training bce: 0.624509, training acc: 0.689063\n",
      "Test epoch: 2, bce: 0.680282, acc: 0.625000, auc: 0.800000, time: 1.095\n",
      "epoch 3, training bce: 0.595084, training acc: 0.696875\n",
      "Test epoch: 3, bce: 0.672649, acc: 0.625000, auc: 1.000000, time: 1.097\n",
      "epoch 4, training bce: 0.575391, training acc: 0.754687\n",
      "Test epoch: 4, bce: 0.666470, acc: 0.625000, auc: 0.933333, time: 1.104\n",
      "epoch 5, training bce: 0.561927, training acc: 0.804688\n",
      "Test epoch: 5, bce: 0.661993, acc: 0.625000, auc: 0.933333, time: 1.094\n",
      "epoch 6, training bce: 0.552144, training acc: 0.812500\n",
      "Test epoch: 6, bce: 0.659136, acc: 0.625000, auc: 0.933333, time: 1.092\n",
      "epoch 7, training bce: 0.542735, training acc: 0.812500\n",
      "Test epoch: 7, bce: 0.657426, acc: 0.625000, auc: 0.933333, time: 1.080\n",
      "epoch 8, training bce: 0.533976, training acc: 0.812500\n",
      "Test epoch: 8, bce: 0.656617, acc: 0.625000, auc: 0.933333, time: 1.075\n",
      "epoch 9, training bce: 0.525751, training acc: 0.812500\n",
      "Test epoch: 9, bce: 0.656057, acc: 0.625000, auc: 0.933333, time: 1.082\n",
      "epoch 10, training bce: 0.517567, training acc: 0.812500\n",
      "Test epoch: 10, bce: 0.655334, acc: 0.625000, auc: 0.933333, time: 1.084\n",
      "[  6/  10]  Valid score: 1.000000\n",
      "epoch 1, training bce: 0.711545, training acc: 0.681250\n",
      "Test epoch: 1, bce: 0.700104, acc: 0.625000, auc: 0.466667, time: 1.104\n",
      "epoch 2, training bce: 0.655935, training acc: 0.754687\n",
      "Test epoch: 2, bce: 0.702035, acc: 0.625000, auc: 0.400000, time: 1.101\n",
      "epoch 3, training bce: 0.629613, training acc: 0.762500\n",
      "Test epoch: 3, bce: 0.703488, acc: 0.625000, auc: 0.466667, time: 1.095\n",
      "epoch 4, training bce: 0.613388, training acc: 0.770312\n",
      "Test epoch: 4, bce: 0.703673, acc: 0.625000, auc: 0.666667, time: 1.086\n",
      "epoch 5, training bce: 0.600372, training acc: 0.820312\n",
      "Test epoch: 5, bce: 0.703085, acc: 0.625000, auc: 0.800000, time: 1.080\n",
      "epoch 6, training bce: 0.589471, training acc: 0.820312\n",
      "Test epoch: 6, bce: 0.702088, acc: 0.625000, auc: 0.800000, time: 1.094\n",
      "epoch 7, training bce: 0.579299, training acc: 0.820312\n",
      "Test epoch: 7, bce: 0.700751, acc: 0.625000, auc: 0.800000, time: 1.101\n",
      "epoch 8, training bce: 0.570564, training acc: 0.828125\n",
      "Test epoch: 8, bce: 0.699206, acc: 0.625000, auc: 0.800000, time: 1.106\n",
      "epoch 9, training bce: 0.561806, training acc: 0.835938\n",
      "Test epoch: 9, bce: 0.697614, acc: 0.625000, auc: 0.800000, time: 1.095\n",
      "epoch 10, training bce: 0.553471, training acc: 0.835938\n",
      "Test epoch: 10, bce: 0.696048, acc: 0.625000, auc: 0.800000, time: 1.096\n",
      "[  7/  10]  Valid score: 0.800000\n",
      "epoch 1, training bce: 0.704081, training acc: 0.654687\n",
      "Test epoch: 1, bce: 0.708438, acc: 0.625000, auc: 0.533333, time: 1.089\n",
      "epoch 2, training bce: 0.641765, training acc: 0.670312\n",
      "Test epoch: 2, bce: 0.708548, acc: 0.625000, auc: 0.733333, time: 1.077\n",
      "epoch 3, training bce: 0.615592, training acc: 0.720313\n",
      "Test epoch: 3, bce: 0.708297, acc: 0.625000, auc: 0.733333, time: 1.084\n",
      "epoch 4, training bce: 0.599288, training acc: 0.770312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test epoch: 4, bce: 0.706287, acc: 0.625000, auc: 0.733333, time: 1.090\n",
      "epoch 5, training bce: 0.584208, training acc: 0.770312\n",
      "Test epoch: 5, bce: 0.703719, acc: 0.625000, auc: 0.800000, time: 1.099\n",
      "epoch 6, training bce: 0.571613, training acc: 0.785937\n",
      "Test epoch: 6, bce: 0.700558, acc: 0.625000, auc: 0.800000, time: 1.110\n",
      "epoch 7, training bce: 0.559187, training acc: 0.785937\n",
      "Test epoch: 7, bce: 0.697424, acc: 0.625000, auc: 0.800000, time: 1.091\n",
      "epoch 8, training bce: 0.547468, training acc: 0.785937\n",
      "Test epoch: 8, bce: 0.694218, acc: 0.625000, auc: 0.800000, time: 1.091\n",
      "epoch 9, training bce: 0.537251, training acc: 0.785937\n",
      "Test epoch: 9, bce: 0.691042, acc: 0.625000, auc: 0.800000, time: 1.086\n",
      "epoch 10, training bce: 0.527159, training acc: 0.785937\n",
      "Test epoch: 10, bce: 0.687754, acc: 0.625000, auc: 0.800000, time: 1.090\n",
      "[  8/  10]  Valid score: 0.800000\n",
      "epoch 1, training bce: 0.708623, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.709656, acc: 0.625000, auc: 0.200000, time: 1.095\n",
      "epoch 2, training bce: 0.665383, training acc: 0.646875\n",
      "Test epoch: 2, bce: 0.706792, acc: 0.625000, auc: 0.266667, time: 1.087\n",
      "epoch 3, training bce: 0.638333, training acc: 0.654687\n",
      "Test epoch: 3, bce: 0.701565, acc: 0.625000, auc: 0.333333, time: 1.092\n",
      "epoch 4, training bce: 0.622944, training acc: 0.662500\n",
      "Test epoch: 4, bce: 0.697461, acc: 0.625000, auc: 0.400000, time: 1.079\n",
      "epoch 5, training bce: 0.608876, training acc: 0.720313\n",
      "Test epoch: 5, bce: 0.694852, acc: 0.625000, auc: 0.466667, time: 1.123\n",
      "epoch 6, training bce: 0.597249, training acc: 0.720313\n",
      "Test epoch: 6, bce: 0.691969, acc: 0.625000, auc: 0.600000, time: 1.092\n",
      "epoch 7, training bce: 0.587619, training acc: 0.778125\n",
      "Test epoch: 7, bce: 0.689578, acc: 0.625000, auc: 0.600000, time: 1.093\n",
      "epoch 8, training bce: 0.578159, training acc: 0.778125\n",
      "Test epoch: 8, bce: 0.687706, acc: 0.625000, auc: 0.600000, time: 1.092\n",
      "epoch 9, training bce: 0.569181, training acc: 0.778125\n",
      "Test epoch: 9, bce: 0.686600, acc: 0.625000, auc: 0.600000, time: 1.097\n",
      "epoch 10, training bce: 0.560588, training acc: 0.785937\n",
      "Test epoch: 10, bce: 0.685725, acc: 0.625000, auc: 0.600000, time: 1.102\n",
      "[  9/  10]  Valid score: 0.600000\n",
      "epoch 1, training bce: 0.708851, training acc: 0.604688\n",
      "Test epoch: 1, bce: 0.727249, acc: 0.625000, auc: 0.600000, time: 1.087\n",
      "epoch 2, training bce: 0.646543, training acc: 0.662500\n",
      "Test epoch: 2, bce: 0.723165, acc: 0.625000, auc: 0.733333, time: 1.079\n",
      "epoch 3, training bce: 0.616675, training acc: 0.678125\n",
      "Test epoch: 3, bce: 0.720074, acc: 0.625000, auc: 0.800000, time: 1.088\n",
      "epoch 4, training bce: 0.592927, training acc: 0.735938\n",
      "Test epoch: 4, bce: 0.716492, acc: 0.625000, auc: 0.933333, time: 1.113\n",
      "epoch 5, training bce: 0.575958, training acc: 0.793750\n",
      "Test epoch: 5, bce: 0.712825, acc: 0.625000, auc: 0.933333, time: 1.095\n",
      "epoch 6, training bce: 0.560271, training acc: 0.801562\n",
      "Test epoch: 6, bce: 0.709283, acc: 0.625000, auc: 0.866667, time: 1.095\n",
      "epoch 7, training bce: 0.546108, training acc: 0.809375\n",
      "Test epoch: 7, bce: 0.705705, acc: 0.625000, auc: 0.866667, time: 1.085\n",
      "epoch 8, training bce: 0.533659, training acc: 0.809375\n",
      "Test epoch: 8, bce: 0.702548, acc: 0.625000, auc: 0.866667, time: 1.081\n",
      "epoch 9, training bce: 0.521492, training acc: 0.809375\n",
      "Test epoch: 9, bce: 0.699604, acc: 0.625000, auc: 0.800000, time: 1.081\n",
      "epoch 10, training bce: 0.509538, training acc: 0.817187\n",
      "Test epoch: 10, bce: 0.696703, acc: 0.625000, auc: 0.800000, time: 1.089\n",
      "[ 10/  10]  Valid score: 0.933333\n",
      "=====parameter별 valid, test score=====\n",
      "Validation 평균: 0.640000\n",
      "epoch 1, training bce: 0.671671, training acc: 0.638021\n",
      "Test epoch: 1, bce: 0.725334, acc: 0.555556, auc: 0.650000, time: 1.108\n",
      "epoch 2, training bce: 0.529702, training acc: 0.693576\n",
      "Test epoch: 2, bce: 1.188516, acc: 0.555556, auc: 0.500000, time: 1.099\n",
      "epoch 3, training bce: 0.429146, training acc: 0.788194\n",
      "Test epoch: 3, bce: 1.957305, acc: 0.555556, auc: 0.450000, time: 1.089\n",
      "epoch 4, training bce: 0.376302, training acc: 0.796007\n",
      "Test epoch: 4, bce: 2.668385, acc: 0.555556, auc: 0.450000, time: 1.102\n",
      "epoch 5, training bce: 0.331030, training acc: 0.796007\n",
      "Test epoch: 5, bce: 3.243205, acc: 0.555556, auc: 0.450000, time: 1.160\n",
      "epoch 6, training bce: 0.283514, training acc: 0.851562\n",
      "Test epoch: 6, bce: 3.591918, acc: 0.555556, auc: 0.450000, time: 1.133\n",
      "epoch 7, training bce: 0.235051, training acc: 0.867188\n",
      "Test epoch: 7, bce: 3.658683, acc: 0.444444, auc: 0.450000, time: 1.097\n",
      "epoch 8, training bce: 0.186281, training acc: 0.906250\n",
      "Test epoch: 8, bce: 3.510535, acc: 0.444444, auc: 0.450000, time: 1.090\n",
      "epoch 9, training bce: 0.137215, training acc: 0.921875\n",
      "Test epoch: 9, bce: 3.269211, acc: 0.444444, auc: 0.450000, time: 1.082\n",
      "epoch 10, training bce: 0.094319, training acc: 0.968750\n",
      "Test epoch: 10, bce: 3.112405, acc: 0.444444, auc: 0.450000, time: 1.145\n",
      "[  1/  10]  Valid score: 0.650000\n",
      "epoch 1, training bce: 0.677052, training acc: 0.685764\n",
      "Test epoch: 1, bce: 0.778048, acc: 0.222222, auc: 0.350000, time: 1.082\n",
      "epoch 2, training bce: 0.505901, training acc: 0.756944\n",
      "Test epoch: 2, bce: 0.833129, acc: 0.444444, auc: 0.400000, time: 1.099\n",
      "epoch 3, training bce: 0.412664, training acc: 0.812500\n",
      "Test epoch: 3, bce: 0.873450, acc: 0.222222, auc: 0.350000, time: 1.088\n",
      "epoch 4, training bce: 0.368309, training acc: 0.835938\n",
      "Test epoch: 4, bce: 0.869915, acc: 0.222222, auc: 0.350000, time: 1.091\n",
      "epoch 5, training bce: 0.330986, training acc: 0.835938\n",
      "Test epoch: 5, bce: 0.876686, acc: 0.333333, auc: 0.350000, time: 1.103\n",
      "epoch 6, training bce: 0.293927, training acc: 0.843750\n",
      "Test epoch: 6, bce: 0.939571, acc: 0.555556, auc: 0.400000, time: 1.108\n",
      "epoch 7, training bce: 0.256852, training acc: 0.851562\n",
      "Test epoch: 7, bce: 1.032558, acc: 0.555556, auc: 0.400000, time: 1.107\n",
      "epoch 8, training bce: 0.219006, training acc: 0.851562\n",
      "Test epoch: 8, bce: 1.164793, acc: 0.555556, auc: 0.400000, time: 1.086\n",
      "epoch 9, training bce: 0.181097, training acc: 0.882812\n",
      "Test epoch: 9, bce: 1.332036, acc: 0.555556, auc: 0.450000, time: 1.087\n",
      "epoch 10, training bce: 0.140713, training acc: 0.914062\n",
      "Test epoch: 10, bce: 1.539424, acc: 0.555556, auc: 0.550000, time: 1.091\n",
      "[  2/  10]  Valid score: 0.550000\n",
      "epoch 1, training bce: 0.641997, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.829497, acc: 0.500000, auc: 0.533333, time: 1.101\n",
      "epoch 2, training bce: 0.481163, training acc: 0.812500\n",
      "Test epoch: 2, bce: 0.794906, acc: 0.500000, auc: 0.466667, time: 1.113\n",
      "epoch 3, training bce: 0.399147, training acc: 0.820312\n",
      "Test epoch: 3, bce: 0.857682, acc: 0.500000, auc: 0.466667, time: 1.092\n",
      "epoch 4, training bce: 0.351629, training acc: 0.820312\n",
      "Test epoch: 4, bce: 0.930941, acc: 0.625000, auc: 0.466667, time: 1.088\n",
      "epoch 5, training bce: 0.309590, training acc: 0.828125\n",
      "Test epoch: 5, bce: 0.976658, acc: 0.750000, auc: 0.466667, time: 1.102\n",
      "epoch 6, training bce: 0.266774, training acc: 0.835938\n",
      "Test epoch: 6, bce: 1.081466, acc: 0.750000, auc: 0.466667, time: 1.099\n",
      "epoch 7, training bce: 0.222220, training acc: 0.851562\n",
      "Test epoch: 7, bce: 1.204626, acc: 0.750000, auc: 0.533333, time: 1.092\n",
      "epoch 8, training bce: 0.175484, training acc: 0.875000\n",
      "Test epoch: 8, bce: 1.321655, acc: 0.750000, auc: 0.600000, time: 1.086\n",
      "epoch 9, training bce: 0.130357, training acc: 0.921875\n",
      "Test epoch: 9, bce: 1.490337, acc: 0.750000, auc: 0.666667, time: 1.094\n",
      "epoch 10, training bce: 0.091494, training acc: 0.960938\n",
      "Test epoch: 10, bce: 1.735992, acc: 0.750000, auc: 0.733333, time: 1.101\n",
      "[  3/  10]  Valid score: 0.733333\n",
      "epoch 1, training bce: 0.671025, training acc: 0.646875\n",
      "Test epoch: 1, bce: 1.057960, acc: 0.375000, auc: 0.333333, time: 1.095\n",
      "epoch 2, training bce: 0.526067, training acc: 0.770312\n",
      "Test epoch: 2, bce: 1.262898, acc: 0.250000, auc: 0.333333, time: 1.095\n",
      "epoch 3, training bce: 0.442996, training acc: 0.778125\n",
      "Test epoch: 3, bce: 1.574569, acc: 0.375000, auc: 0.333333, time: 1.091\n",
      "epoch 4, training bce: 0.390577, training acc: 0.778125\n",
      "Test epoch: 4, bce: 1.745251, acc: 0.375000, auc: 0.133333, time: 1.095\n",
      "epoch 5, training bce: 0.328387, training acc: 0.778125\n",
      "Test epoch: 5, bce: 1.879764, acc: 0.500000, auc: 0.000000, time: 1.101\n",
      "epoch 6, training bce: 0.264098, training acc: 0.843750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test epoch: 6, bce: 2.049184, acc: 0.625000, auc: 0.000000, time: 1.101\n",
      "epoch 7, training bce: 0.203240, training acc: 0.867188\n",
      "Test epoch: 7, bce: 2.328025, acc: 0.625000, auc: 0.000000, time: 1.085\n",
      "epoch 8, training bce: 0.148820, training acc: 0.921875\n",
      "Test epoch: 8, bce: 2.625710, acc: 0.625000, auc: 0.000000, time: 1.082\n",
      "epoch 9, training bce: 0.103769, training acc: 0.968750\n",
      "Test epoch: 9, bce: 2.888180, acc: 0.625000, auc: 0.000000, time: 1.088\n",
      "epoch 10, training bce: 0.067658, training acc: 0.984375\n",
      "Test epoch: 10, bce: 3.107615, acc: 0.625000, auc: 0.000000, time: 1.100\n",
      "[  4/  10]  Valid score: 0.333333\n",
      "epoch 1, training bce: 0.650452, training acc: 0.689063\n",
      "Test epoch: 1, bce: 1.037205, acc: 0.625000, auc: 0.533333, time: 1.099\n",
      "epoch 2, training bce: 0.489529, training acc: 0.796875\n",
      "Test epoch: 2, bce: 1.217939, acc: 0.625000, auc: 0.600000, time: 1.100\n",
      "epoch 3, training bce: 0.403303, training acc: 0.820312\n",
      "Test epoch: 3, bce: 1.326959, acc: 0.750000, auc: 0.600000, time: 1.083\n",
      "epoch 4, training bce: 0.363931, training acc: 0.828125\n",
      "Test epoch: 4, bce: 1.450404, acc: 0.625000, auc: 0.533333, time: 1.085\n",
      "epoch 5, training bce: 0.333741, training acc: 0.828125\n",
      "Test epoch: 5, bce: 1.572663, acc: 0.625000, auc: 0.533333, time: 1.092\n",
      "epoch 6, training bce: 0.301517, training acc: 0.828125\n",
      "Test epoch: 6, bce: 1.638196, acc: 0.625000, auc: 0.533333, time: 1.091\n",
      "epoch 7, training bce: 0.267946, training acc: 0.843750\n",
      "Test epoch: 7, bce: 1.655667, acc: 0.500000, auc: 0.533333, time: 1.087\n",
      "epoch 8, training bce: 0.234167, training acc: 0.851562\n",
      "Test epoch: 8, bce: 1.643766, acc: 0.500000, auc: 0.466667, time: 1.079\n",
      "epoch 9, training bce: 0.198412, training acc: 0.859375\n",
      "Test epoch: 9, bce: 1.577349, acc: 0.500000, auc: 0.466667, time: 1.087\n",
      "epoch 10, training bce: 0.160737, training acc: 0.898438\n",
      "Test epoch: 10, bce: 1.433885, acc: 0.500000, auc: 0.533333, time: 1.083\n",
      "[  5/  10]  Valid score: 0.600000\n",
      "epoch 1, training bce: 0.664160, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.650392, acc: 0.750000, auc: 0.733333, time: 1.093\n",
      "epoch 2, training bce: 0.532773, training acc: 0.746875\n",
      "Test epoch: 2, bce: 0.595913, acc: 0.750000, auc: 0.733333, time: 1.084\n",
      "epoch 3, training bce: 0.443362, training acc: 0.762500\n",
      "Test epoch: 3, bce: 0.560834, acc: 0.750000, auc: 0.733333, time: 1.088\n",
      "epoch 4, training bce: 0.397280, training acc: 0.820312\n",
      "Test epoch: 4, bce: 0.518930, acc: 0.750000, auc: 0.733333, time: 1.087\n",
      "epoch 5, training bce: 0.351708, training acc: 0.820312\n",
      "Test epoch: 5, bce: 0.480752, acc: 0.750000, auc: 0.800000, time: 1.106\n",
      "epoch 6, training bce: 0.302595, training acc: 0.828125\n",
      "Test epoch: 6, bce: 0.460786, acc: 0.750000, auc: 0.800000, time: 1.081\n",
      "epoch 7, training bce: 0.253524, training acc: 0.835938\n",
      "Test epoch: 7, bce: 0.462076, acc: 0.875000, auc: 0.800000, time: 1.083\n",
      "epoch 8, training bce: 0.200469, training acc: 0.859375\n",
      "Test epoch: 8, bce: 0.497308, acc: 0.875000, auc: 0.866667, time: 1.091\n",
      "epoch 9, training bce: 0.149793, training acc: 0.906250\n",
      "Test epoch: 9, bce: 0.576486, acc: 0.875000, auc: 0.866667, time: 1.097\n",
      "epoch 10, training bce: 0.103666, training acc: 0.945312\n",
      "Test epoch: 10, bce: 0.691616, acc: 0.875000, auc: 0.866667, time: 1.121\n",
      "[  6/  10]  Valid score: 0.866667\n",
      "epoch 1, training bce: 0.676342, training acc: 0.689063\n",
      "Test epoch: 1, bce: 1.032842, acc: 0.625000, auc: 0.800000, time: 1.090\n",
      "epoch 2, training bce: 0.518515, training acc: 0.762500\n",
      "Test epoch: 2, bce: 1.706443, acc: 0.625000, auc: 0.533333, time: 1.093\n",
      "epoch 3, training bce: 0.436200, training acc: 0.812500\n",
      "Test epoch: 3, bce: 2.018791, acc: 0.625000, auc: 0.600000, time: 1.088\n",
      "epoch 4, training bce: 0.394684, training acc: 0.820312\n",
      "Test epoch: 4, bce: 2.183286, acc: 0.625000, auc: 0.600000, time: 1.125\n",
      "epoch 5, training bce: 0.358408, training acc: 0.820312\n",
      "Test epoch: 5, bce: 2.306309, acc: 0.500000, auc: 0.600000, time: 1.094\n",
      "epoch 6, training bce: 0.322723, training acc: 0.828125\n",
      "Test epoch: 6, bce: 2.361103, acc: 0.500000, auc: 0.600000, time: 1.086\n",
      "epoch 7, training bce: 0.287413, training acc: 0.843750\n",
      "Test epoch: 7, bce: 2.348933, acc: 0.500000, auc: 0.600000, time: 1.087\n",
      "epoch 8, training bce: 0.248299, training acc: 0.843750\n",
      "Test epoch: 8, bce: 2.275230, acc: 0.500000, auc: 0.666667, time: 1.091\n",
      "epoch 9, training bce: 0.204032, training acc: 0.859375\n",
      "Test epoch: 9, bce: 2.162984, acc: 0.500000, auc: 0.666667, time: 1.123\n",
      "epoch 10, training bce: 0.156427, training acc: 0.882812\n",
      "Test epoch: 10, bce: 2.045195, acc: 0.500000, auc: 0.733333, time: 1.056\n",
      "[  7/  10]  Valid score: 0.800000\n",
      "epoch 1, training bce: 0.663146, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.948465, acc: 0.500000, auc: 0.666667, time: 1.048\n",
      "epoch 2, training bce: 0.512243, training acc: 0.712500\n",
      "Test epoch: 2, bce: 0.784222, acc: 0.875000, auc: 0.800000, time: 1.043\n",
      "epoch 3, training bce: 0.417749, training acc: 0.778125\n",
      "Test epoch: 3, bce: 0.672522, acc: 0.875000, auc: 0.866667, time: 1.044\n",
      "epoch 4, training bce: 0.358000, training acc: 0.785937\n",
      "Test epoch: 4, bce: 0.430320, acc: 0.875000, auc: 0.933333, time: 1.073\n",
      "epoch 5, training bce: 0.301051, training acc: 0.843750\n",
      "Test epoch: 5, bce: 0.238118, acc: 0.875000, auc: 1.000000, time: 1.059\n",
      "epoch 6, training bce: 0.244446, training acc: 0.851562\n",
      "Test epoch: 6, bce: 0.191369, acc: 0.875000, auc: 1.000000, time: 1.035\n",
      "epoch 7, training bce: 0.189345, training acc: 0.867188\n",
      "Test epoch: 7, bce: 0.186674, acc: 0.875000, auc: 1.000000, time: 1.028\n",
      "epoch 8, training bce: 0.137268, training acc: 0.929688\n",
      "Test epoch: 8, bce: 0.154382, acc: 0.875000, auc: 1.000000, time: 1.028\n",
      "epoch 9, training bce: 0.092769, training acc: 0.953125\n",
      "Test epoch: 9, bce: 0.110369, acc: 0.875000, auc: 1.000000, time: 1.103\n",
      "epoch 10, training bce: 0.060576, training acc: 0.968750\n",
      "Test epoch: 10, bce: 0.072738, acc: 1.000000, auc: 1.000000, time: 1.043\n",
      "[  8/  10]  Valid score: 1.000000\n",
      "epoch 1, training bce: 0.684318, training acc: 0.646875\n",
      "Test epoch: 1, bce: 1.246065, acc: 0.375000, auc: 0.533333, time: 1.044\n",
      "epoch 2, training bce: 0.556866, training acc: 0.746875\n",
      "Test epoch: 2, bce: 1.656592, acc: 0.500000, auc: 0.533333, time: 1.023\n",
      "epoch 3, training bce: 0.464522, training acc: 0.770312\n",
      "Test epoch: 3, bce: 1.863239, acc: 0.500000, auc: 0.533333, time: 1.037\n",
      "epoch 4, training bce: 0.403791, training acc: 0.770312\n",
      "Test epoch: 4, bce: 1.992661, acc: 0.500000, auc: 0.533333, time: 1.022\n",
      "epoch 5, training bce: 0.352088, training acc: 0.828125\n",
      "Test epoch: 5, bce: 2.166043, acc: 0.500000, auc: 0.533333, time: 1.038\n",
      "epoch 6, training bce: 0.305461, training acc: 0.851562\n",
      "Test epoch: 6, bce: 2.348212, acc: 0.500000, auc: 0.533333, time: 1.050\n",
      "epoch 7, training bce: 0.258963, training acc: 0.859375\n",
      "Test epoch: 7, bce: 2.461214, acc: 0.500000, auc: 0.466667, time: 1.044\n",
      "epoch 8, training bce: 0.210254, training acc: 0.875000\n",
      "Test epoch: 8, bce: 2.471629, acc: 0.500000, auc: 0.466667, time: 1.071\n",
      "epoch 9, training bce: 0.157187, training acc: 0.921875\n",
      "Test epoch: 9, bce: 2.370744, acc: 0.500000, auc: 0.466667, time: 1.056\n",
      "epoch 10, training bce: 0.105267, training acc: 0.937500\n",
      "Test epoch: 10, bce: 2.274214, acc: 0.500000, auc: 0.466667, time: 1.069\n",
      "[  9/  10]  Valid score: 0.533333\n",
      "epoch 1, training bce: 0.687944, training acc: 0.612500\n",
      "Test epoch: 1, bce: 0.690245, acc: 0.750000, auc: 0.800000, time: 1.052\n",
      "epoch 2, training bce: 0.580917, training acc: 0.620313\n",
      "Test epoch: 2, bce: 0.673561, acc: 0.875000, auc: 0.866667, time: 1.034\n",
      "epoch 3, training bce: 0.474570, training acc: 0.728125\n",
      "Test epoch: 3, bce: 0.675136, acc: 0.875000, auc: 0.866667, time: 1.029\n",
      "epoch 4, training bce: 0.400819, training acc: 0.793750\n",
      "Test epoch: 4, bce: 0.651922, acc: 0.875000, auc: 0.866667, time: 1.027\n",
      "epoch 5, training bce: 0.331654, training acc: 0.851562\n",
      "Test epoch: 5, bce: 0.629281, acc: 0.875000, auc: 0.866667, time: 1.024\n",
      "epoch 6, training bce: 0.270125, training acc: 0.867188\n",
      "Test epoch: 6, bce: 0.634959, acc: 0.875000, auc: 0.866667, time: 1.030\n",
      "epoch 7, training bce: 0.212965, training acc: 0.875000\n",
      "Test epoch: 7, bce: 0.693204, acc: 0.875000, auc: 0.866667, time: 1.024\n",
      "epoch 8, training bce: 0.157261, training acc: 0.937500\n",
      "Test epoch: 8, bce: 0.715605, acc: 0.875000, auc: 0.866667, time: 1.081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, training bce: 0.108316, training acc: 0.953125\n",
      "Test epoch: 9, bce: 0.779672, acc: 0.875000, auc: 0.866667, time: 1.032\n",
      "epoch 10, training bce: 0.071899, training acc: 0.960938\n",
      "Test epoch: 10, bce: 0.954684, acc: 0.750000, auc: 0.933333, time: 1.039\n",
      "[ 10/  10]  Valid score: 0.933333\n",
      "=====parameter별 valid, test score=====\n",
      "Validation 평균: 0.700000\n",
      "epoch 1, training bce: 0.689418, training acc: 0.638021\n",
      "Test epoch: 1, bce: 0.732042, acc: 0.555556, auc: 0.350000, time: 1.035\n",
      "epoch 2, training bce: 0.535185, training acc: 0.709201\n",
      "Test epoch: 2, bce: 1.023084, acc: 0.666667, auc: 0.500000, time: 1.054\n",
      "epoch 3, training bce: 0.431626, training acc: 0.788194\n",
      "Test epoch: 3, bce: 1.511143, acc: 0.555556, auc: 0.450000, time: 1.029\n",
      "epoch 4, training bce: 0.380869, training acc: 0.788194\n",
      "Test epoch: 4, bce: 2.243015, acc: 0.555556, auc: 0.450000, time: 1.068\n",
      "epoch 5, training bce: 0.332249, training acc: 0.796007\n",
      "Test epoch: 5, bce: 2.931798, acc: 0.555556, auc: 0.450000, time: 1.117\n",
      "epoch 6, training bce: 0.283537, training acc: 0.851562\n",
      "Test epoch: 6, bce: 3.356165, acc: 0.555556, auc: 0.450000, time: 1.043\n",
      "epoch 7, training bce: 0.232482, training acc: 0.851562\n",
      "Test epoch: 7, bce: 3.522638, acc: 0.555556, auc: 0.450000, time: 1.052\n",
      "epoch 8, training bce: 0.180100, training acc: 0.875000\n",
      "Test epoch: 8, bce: 3.548765, acc: 0.444444, auc: 0.450000, time: 1.051\n",
      "epoch 9, training bce: 0.130994, training acc: 0.921875\n",
      "Test epoch: 9, bce: 3.494438, acc: 0.444444, auc: 0.450000, time: 1.071\n",
      "epoch 10, training bce: 0.090308, training acc: 0.945312\n",
      "Test epoch: 10, bce: 3.362562, acc: 0.444444, auc: 0.450000, time: 1.090\n",
      "[  1/  10]  Valid score: 0.500000\n",
      "epoch 1, training bce: 0.639250, training acc: 0.685764\n",
      "Test epoch: 1, bce: 1.041298, acc: 0.444444, auc: 0.300000, time: 1.037\n",
      "epoch 2, training bce: 0.505376, training acc: 0.804688\n",
      "Test epoch: 2, bce: 1.120429, acc: 0.555556, auc: 0.350000, time: 1.042\n",
      "epoch 3, training bce: 0.411193, training acc: 0.828125\n",
      "Test epoch: 3, bce: 1.127875, acc: 0.444444, auc: 0.350000, time: 1.035\n",
      "epoch 4, training bce: 0.372303, training acc: 0.828125\n",
      "Test epoch: 4, bce: 1.101499, acc: 0.222222, auc: 0.350000, time: 1.046\n",
      "epoch 5, training bce: 0.340374, training acc: 0.835938\n",
      "Test epoch: 5, bce: 1.076917, acc: 0.333333, auc: 0.350000, time: 1.029\n",
      "epoch 6, training bce: 0.308645, training acc: 0.851562\n",
      "Test epoch: 6, bce: 1.111378, acc: 0.333333, auc: 0.300000, time: 1.043\n",
      "epoch 7, training bce: 0.277613, training acc: 0.851562\n",
      "Test epoch: 7, bce: 1.178381, acc: 0.444444, auc: 0.350000, time: 1.063\n",
      "epoch 8, training bce: 0.245880, training acc: 0.859375\n",
      "Test epoch: 8, bce: 1.263023, acc: 0.444444, auc: 0.400000, time: 1.042\n",
      "epoch 9, training bce: 0.210789, training acc: 0.867188\n",
      "Test epoch: 9, bce: 1.354834, acc: 0.444444, auc: 0.400000, time: 1.028\n",
      "epoch 10, training bce: 0.171773, training acc: 0.906250\n",
      "Test epoch: 10, bce: 1.451696, acc: 0.444444, auc: 0.400000, time: 1.046\n",
      "[  2/  10]  Valid score: 0.400000\n",
      "epoch 1, training bce: 0.650412, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.781396, acc: 0.500000, auc: 0.533333, time: 1.078\n",
      "epoch 2, training bce: 0.497130, training acc: 0.762500\n",
      "Test epoch: 2, bce: 0.881169, acc: 0.375000, auc: 0.333333, time: 1.049\n",
      "epoch 3, training bce: 0.411472, training acc: 0.820312\n",
      "Test epoch: 3, bce: 0.976622, acc: 0.375000, auc: 0.400000, time: 1.055\n",
      "epoch 4, training bce: 0.369508, training acc: 0.820312\n",
      "Test epoch: 4, bce: 1.059878, acc: 0.500000, auc: 0.400000, time: 1.032\n",
      "epoch 5, training bce: 0.335972, training acc: 0.828125\n",
      "Test epoch: 5, bce: 1.098524, acc: 0.625000, auc: 0.466667, time: 1.042\n",
      "epoch 6, training bce: 0.301490, training acc: 0.828125\n",
      "Test epoch: 6, bce: 1.069435, acc: 0.750000, auc: 0.466667, time: 1.030\n",
      "epoch 7, training bce: 0.264216, training acc: 0.835938\n",
      "Test epoch: 7, bce: 1.091987, acc: 0.750000, auc: 0.466667, time: 1.034\n",
      "epoch 8, training bce: 0.222119, training acc: 0.851562\n",
      "Test epoch: 8, bce: 1.162294, acc: 0.750000, auc: 0.533333, time: 1.047\n",
      "epoch 9, training bce: 0.177727, training acc: 0.882812\n",
      "Test epoch: 9, bce: 1.249114, acc: 0.750000, auc: 0.600000, time: 1.075\n",
      "epoch 10, training bce: 0.135024, training acc: 0.914062\n",
      "Test epoch: 10, bce: 1.360242, acc: 0.750000, auc: 0.600000, time: 1.063\n",
      "[  3/  10]  Valid score: 0.600000\n",
      "epoch 1, training bce: 0.673597, training acc: 0.639062\n",
      "Test epoch: 1, bce: 0.758804, acc: 0.625000, auc: 0.266667, time: 1.041\n",
      "epoch 2, training bce: 0.544063, training acc: 0.720313\n",
      "Test epoch: 2, bce: 0.878362, acc: 0.625000, auc: 0.133333, time: 1.059\n",
      "epoch 3, training bce: 0.451779, training acc: 0.770312\n",
      "Test epoch: 3, bce: 1.037776, acc: 0.500000, auc: 0.066667, time: 1.098\n",
      "epoch 4, training bce: 0.390321, training acc: 0.778125\n",
      "Test epoch: 4, bce: 1.263239, acc: 0.500000, auc: 0.066667, time: 1.054\n",
      "epoch 5, training bce: 0.331760, training acc: 0.785937\n",
      "Test epoch: 5, bce: 1.504785, acc: 0.625000, auc: 0.000000, time: 1.063\n",
      "epoch 6, training bce: 0.271182, training acc: 0.851562\n",
      "Test epoch: 6, bce: 1.793879, acc: 0.625000, auc: 0.000000, time: 1.061\n",
      "epoch 7, training bce: 0.208585, training acc: 0.867188\n",
      "Test epoch: 7, bce: 2.106831, acc: 0.625000, auc: 0.000000, time: 1.081\n",
      "epoch 8, training bce: 0.148933, training acc: 0.937500\n",
      "Test epoch: 8, bce: 2.371707, acc: 0.625000, auc: 0.000000, time: 1.037\n",
      "epoch 9, training bce: 0.099137, training acc: 0.968750\n",
      "Test epoch: 9, bce: 2.595338, acc: 0.625000, auc: 0.000000, time: 1.061\n",
      "epoch 10, training bce: 0.062966, training acc: 0.976562\n",
      "Test epoch: 10, bce: 2.910397, acc: 0.625000, auc: 0.000000, time: 1.035\n",
      "[  4/  10]  Valid score: 0.266667\n",
      "epoch 1, training bce: 0.663368, training acc: 0.689063\n",
      "Test epoch: 1, bce: 1.097117, acc: 0.375000, auc: 0.466667, time: 1.035\n",
      "epoch 2, training bce: 0.512483, training acc: 0.796875\n",
      "Test epoch: 2, bce: 1.090157, acc: 0.750000, auc: 0.533333, time: 1.032\n",
      "epoch 3, training bce: 0.417600, training acc: 0.812500\n",
      "Test epoch: 3, bce: 1.169805, acc: 0.625000, auc: 0.533333, time: 1.029\n",
      "epoch 4, training bce: 0.377854, training acc: 0.820312\n",
      "Test epoch: 4, bce: 1.293773, acc: 0.625000, auc: 0.533333, time: 1.037\n",
      "epoch 5, training bce: 0.342950, training acc: 0.828125\n",
      "Test epoch: 5, bce: 1.416203, acc: 0.625000, auc: 0.533333, time: 1.035\n",
      "epoch 6, training bce: 0.308058, training acc: 0.835938\n",
      "Test epoch: 6, bce: 1.487728, acc: 0.500000, auc: 0.466667, time: 1.047\n",
      "epoch 7, training bce: 0.272610, training acc: 0.843750\n",
      "Test epoch: 7, bce: 1.558694, acc: 0.500000, auc: 0.466667, time: 1.037\n",
      "epoch 8, training bce: 0.235523, training acc: 0.859375\n",
      "Test epoch: 8, bce: 1.657336, acc: 0.500000, auc: 0.533333, time: 1.027\n",
      "epoch 9, training bce: 0.197594, training acc: 0.867188\n",
      "Test epoch: 9, bce: 1.697102, acc: 0.500000, auc: 0.533333, time: 1.039\n",
      "epoch 10, training bce: 0.158146, training acc: 0.898438\n",
      "Test epoch: 10, bce: 1.564605, acc: 0.500000, auc: 0.533333, time: 1.028\n",
      "[  5/  10]  Valid score: 0.533333\n",
      "epoch 1, training bce: 0.662616, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.552768, acc: 0.750000, auc: 0.733333, time: 1.019\n",
      "epoch 2, training bce: 0.521765, training acc: 0.746875\n",
      "Test epoch: 2, bce: 0.562326, acc: 0.750000, auc: 0.733333, time: 1.033\n",
      "epoch 3, training bce: 0.436292, training acc: 0.746875\n",
      "Test epoch: 3, bce: 0.495583, acc: 0.875000, auc: 0.733333, time: 1.038\n",
      "epoch 4, training bce: 0.396009, training acc: 0.804688\n",
      "Test epoch: 4, bce: 0.445964, acc: 0.875000, auc: 0.733333, time: 1.033\n",
      "epoch 5, training bce: 0.356679, training acc: 0.812500\n",
      "Test epoch: 5, bce: 0.434631, acc: 0.875000, auc: 0.733333, time: 1.039\n",
      "epoch 6, training bce: 0.311807, training acc: 0.820312\n",
      "Test epoch: 6, bce: 0.448309, acc: 0.875000, auc: 0.733333, time: 1.030\n",
      "epoch 7, training bce: 0.261520, training acc: 0.828125\n",
      "Test epoch: 7, bce: 0.490257, acc: 0.875000, auc: 0.800000, time: 1.058\n",
      "epoch 8, training bce: 0.211070, training acc: 0.843750\n",
      "Test epoch: 8, bce: 0.607527, acc: 0.750000, auc: 0.800000, time: 1.064\n",
      "epoch 9, training bce: 0.162919, training acc: 0.875000\n",
      "Test epoch: 9, bce: 0.837759, acc: 0.750000, auc: 0.800000, time: 1.035\n",
      "epoch 10, training bce: 0.118063, training acc: 0.929688\n",
      "Test epoch: 10, bce: 1.196501, acc: 0.750000, auc: 0.800000, time: 1.074\n",
      "[  6/  10]  Valid score: 0.800000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training bce: 0.662477, training acc: 0.689063\n",
      "Test epoch: 1, bce: 1.282169, acc: 0.625000, auc: 0.800000, time: 1.101\n",
      "epoch 2, training bce: 0.495251, training acc: 0.804688\n",
      "Test epoch: 2, bce: 1.792770, acc: 0.625000, auc: 0.733333, time: 1.041\n",
      "epoch 3, training bce: 0.418784, training acc: 0.812500\n",
      "Test epoch: 3, bce: 2.038686, acc: 0.750000, auc: 0.600000, time: 1.086\n",
      "epoch 4, training bce: 0.381123, training acc: 0.820312\n",
      "Test epoch: 4, bce: 2.253639, acc: 0.625000, auc: 0.600000, time: 1.032\n",
      "epoch 5, training bce: 0.350473, training acc: 0.820312\n",
      "Test epoch: 5, bce: 2.456282, acc: 0.625000, auc: 0.600000, time: 1.046\n",
      "epoch 6, training bce: 0.320628, training acc: 0.828125\n",
      "Test epoch: 6, bce: 2.614030, acc: 0.500000, auc: 0.600000, time: 1.029\n",
      "epoch 7, training bce: 0.289925, training acc: 0.835938\n",
      "Test epoch: 7, bce: 2.699245, acc: 0.500000, auc: 0.600000, time: 1.035\n",
      "epoch 8, training bce: 0.255847, training acc: 0.851562\n",
      "Test epoch: 8, bce: 2.692694, acc: 0.500000, auc: 0.600000, time: 1.047\n",
      "epoch 9, training bce: 0.217110, training acc: 0.859375\n",
      "Test epoch: 9, bce: 2.596469, acc: 0.500000, auc: 0.600000, time: 1.035\n",
      "epoch 10, training bce: 0.170505, training acc: 0.890625\n",
      "Test epoch: 10, bce: 2.437886, acc: 0.500000, auc: 0.666667, time: 1.030\n",
      "[  7/  10]  Valid score: 0.800000\n",
      "epoch 1, training bce: 0.668204, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.808594, acc: 0.500000, auc: 0.466667, time: 1.038\n",
      "epoch 2, training bce: 0.543498, training acc: 0.704688\n",
      "Test epoch: 2, bce: 0.688048, acc: 0.875000, auc: 0.800000, time: 1.043\n",
      "epoch 3, training bce: 0.453517, training acc: 0.770312\n",
      "Test epoch: 3, bce: 0.605224, acc: 0.875000, auc: 0.800000, time: 1.036\n",
      "epoch 4, training bce: 0.397775, training acc: 0.778125\n",
      "Test epoch: 4, bce: 0.441713, acc: 0.875000, auc: 0.933333, time: 1.025\n",
      "epoch 5, training bce: 0.341546, training acc: 0.778125\n",
      "Test epoch: 5, bce: 0.305226, acc: 0.875000, auc: 1.000000, time: 1.036\n",
      "epoch 6, training bce: 0.284216, training acc: 0.843750\n",
      "Test epoch: 6, bce: 0.242759, acc: 0.875000, auc: 1.000000, time: 1.029\n",
      "epoch 7, training bce: 0.224436, training acc: 0.851562\n",
      "Test epoch: 7, bce: 0.210198, acc: 0.875000, auc: 1.000000, time: 1.036\n",
      "epoch 8, training bce: 0.165147, training acc: 0.898438\n",
      "Test epoch: 8, bce: 0.162127, acc: 0.875000, auc: 1.000000, time: 1.051\n",
      "epoch 9, training bce: 0.114290, training acc: 0.929688\n",
      "Test epoch: 9, bce: 0.106367, acc: 0.875000, auc: 1.000000, time: 1.043\n",
      "epoch 10, training bce: 0.075820, training acc: 0.960938\n",
      "Test epoch: 10, bce: 0.069258, acc: 1.000000, auc: 1.000000, time: 1.028\n",
      "[  8/  10]  Valid score: 1.000000\n",
      "epoch 1, training bce: 0.710307, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.853096, acc: 0.625000, auc: 0.533333, time: 1.091\n",
      "epoch 2, training bce: 0.563335, training acc: 0.704688\n",
      "Test epoch: 2, bce: 0.986295, acc: 0.500000, auc: 0.533333, time: 1.059\n",
      "epoch 3, training bce: 0.471225, training acc: 0.762500\n",
      "Test epoch: 3, bce: 1.263316, acc: 0.500000, auc: 0.533333, time: 1.070\n",
      "epoch 4, training bce: 0.414525, training acc: 0.770312\n",
      "Test epoch: 4, bce: 1.564144, acc: 0.500000, auc: 0.533333, time: 1.128\n",
      "epoch 5, training bce: 0.365565, training acc: 0.778125\n",
      "Test epoch: 5, bce: 1.857147, acc: 0.500000, auc: 0.533333, time: 1.036\n",
      "epoch 6, training bce: 0.316957, training acc: 0.843750\n",
      "Test epoch: 6, bce: 2.151936, acc: 0.500000, auc: 0.533333, time: 1.041\n",
      "epoch 7, training bce: 0.269600, training acc: 0.859375\n",
      "Test epoch: 7, bce: 2.327911, acc: 0.500000, auc: 0.533333, time: 1.051\n",
      "epoch 8, training bce: 0.219168, training acc: 0.875000\n",
      "Test epoch: 8, bce: 2.360806, acc: 0.500000, auc: 0.533333, time: 1.035\n",
      "epoch 9, training bce: 0.163374, training acc: 0.898438\n",
      "Test epoch: 9, bce: 2.362009, acc: 0.500000, auc: 0.600000, time: 1.039\n",
      "epoch 10, training bce: 0.110236, training acc: 0.937500\n",
      "Test epoch: 10, bce: 2.361392, acc: 0.500000, auc: 0.600000, time: 1.037\n",
      "[  9/  10]  Valid score: 0.600000\n",
      "epoch 1, training bce: 0.685847, training acc: 0.604688\n",
      "Test epoch: 1, bce: 0.806022, acc: 0.750000, auc: 0.533333, time: 1.038\n",
      "epoch 2, training bce: 0.571248, training acc: 0.654687\n",
      "Test epoch: 2, bce: 0.660675, acc: 0.875000, auc: 0.866667, time: 1.030\n",
      "epoch 3, training bce: 0.454925, training acc: 0.770312\n",
      "Test epoch: 3, bce: 0.661392, acc: 0.875000, auc: 0.866667, time: 1.032\n",
      "epoch 4, training bce: 0.375369, training acc: 0.801562\n",
      "Test epoch: 4, bce: 0.678943, acc: 0.875000, auc: 0.866667, time: 1.038\n",
      "epoch 5, training bce: 0.303597, training acc: 0.859375\n",
      "Test epoch: 5, bce: 0.728196, acc: 0.750000, auc: 0.866667, time: 1.034\n",
      "epoch 6, training bce: 0.234415, training acc: 0.882812\n",
      "Test epoch: 6, bce: 0.813133, acc: 0.750000, auc: 0.866667, time: 1.033\n",
      "epoch 7, training bce: 0.171271, training acc: 0.929688\n",
      "Test epoch: 7, bce: 0.905089, acc: 0.625000, auc: 0.866667, time: 1.025\n",
      "epoch 8, training bce: 0.119701, training acc: 0.945312\n",
      "Test epoch: 8, bce: 0.961263, acc: 0.750000, auc: 0.933333, time: 1.028\n",
      "epoch 9, training bce: 0.082127, training acc: 0.960938\n",
      "Test epoch: 9, bce: 1.092612, acc: 0.750000, auc: 0.933333, time: 1.031\n",
      "epoch 10, training bce: 0.055325, training acc: 0.976562\n",
      "Test epoch: 10, bce: 1.240898, acc: 0.625000, auc: 0.933333, time: 1.031\n",
      "[ 10/  10]  Valid score: 0.933333\n",
      "=====parameter별 valid, test score=====\n",
      "Validation 평균: 0.643333\n",
      "epoch 1, training bce: 0.691322, training acc: 0.638021\n",
      "Test epoch: 1, bce: 0.688873, acc: 0.555556, auc: 0.800000, time: 1.030\n",
      "epoch 2, training bce: 0.591837, training acc: 0.796007\n",
      "Test epoch: 2, bce: 0.692154, acc: 0.555556, auc: 0.700000, time: 1.029\n",
      "epoch 3, training bce: 0.538487, training acc: 0.796007\n",
      "Test epoch: 3, bce: 0.692925, acc: 0.555556, auc: 0.650000, time: 1.027\n",
      "epoch 4, training bce: 0.504328, training acc: 0.796007\n",
      "Test epoch: 4, bce: 0.692520, acc: 0.555556, auc: 0.650000, time: 1.028\n",
      "epoch 5, training bce: 0.479085, training acc: 0.796007\n",
      "Test epoch: 5, bce: 0.691981, acc: 0.555556, auc: 0.650000, time: 1.043\n",
      "epoch 6, training bce: 0.456880, training acc: 0.796007\n",
      "Test epoch: 6, bce: 0.691806, acc: 0.555556, auc: 0.650000, time: 1.040\n",
      "epoch 7, training bce: 0.437088, training acc: 0.851562\n",
      "Test epoch: 7, bce: 0.691752, acc: 0.555556, auc: 0.600000, time: 1.027\n",
      "epoch 8, training bce: 0.418671, training acc: 0.851562\n",
      "Test epoch: 8, bce: 0.691813, acc: 0.555556, auc: 0.600000, time: 1.037\n",
      "epoch 9, training bce: 0.401063, training acc: 0.851562\n",
      "Test epoch: 9, bce: 0.691925, acc: 0.555556, auc: 0.600000, time: 1.027\n",
      "epoch 10, training bce: 0.384562, training acc: 0.851562\n",
      "Test epoch: 10, bce: 0.692179, acc: 0.555556, auc: 0.600000, time: 1.031\n",
      "[  1/  10]  Valid score: 0.800000\n",
      "epoch 1, training bce: 0.682221, training acc: 0.685764\n",
      "Test epoch: 1, bce: 0.696326, acc: 0.555556, auc: 0.400000, time: 1.035\n",
      "epoch 2, training bce: 0.575761, training acc: 0.835938\n",
      "Test epoch: 2, bce: 0.698650, acc: 0.555556, auc: 0.400000, time: 1.033\n",
      "epoch 3, training bce: 0.526166, training acc: 0.835938\n",
      "Test epoch: 3, bce: 0.698584, acc: 0.555556, auc: 0.400000, time: 1.049\n",
      "epoch 4, training bce: 0.496169, training acc: 0.835938\n",
      "Test epoch: 4, bce: 0.697737, acc: 0.555556, auc: 0.400000, time: 1.061\n",
      "epoch 5, training bce: 0.472008, training acc: 0.843750\n",
      "Test epoch: 5, bce: 0.697246, acc: 0.555556, auc: 0.400000, time: 1.024\n",
      "epoch 6, training bce: 0.450685, training acc: 0.843750\n",
      "Test epoch: 6, bce: 0.697239, acc: 0.555556, auc: 0.400000, time: 1.025\n",
      "epoch 7, training bce: 0.431749, training acc: 0.843750\n",
      "Test epoch: 7, bce: 0.697593, acc: 0.555556, auc: 0.400000, time: 1.032\n",
      "epoch 8, training bce: 0.414652, training acc: 0.851562\n",
      "Test epoch: 8, bce: 0.698019, acc: 0.555556, auc: 0.400000, time: 1.031\n",
      "epoch 9, training bce: 0.399132, training acc: 0.851562\n",
      "Test epoch: 9, bce: 0.698479, acc: 0.555556, auc: 0.400000, time: 1.030\n",
      "epoch 10, training bce: 0.384669, training acc: 0.851562\n",
      "Test epoch: 10, bce: 0.699057, acc: 0.555556, auc: 0.400000, time: 1.026\n",
      "[  2/  10]  Valid score: 0.400000\n",
      "epoch 1, training bce: 0.687287, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.698720, acc: 0.625000, auc: 0.266667, time: 1.021\n",
      "epoch 2, training bce: 0.574070, training acc: 0.820312\n",
      "Test epoch: 2, bce: 0.699009, acc: 0.625000, auc: 0.400000, time: 1.029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, training bce: 0.528354, training acc: 0.820312\n",
      "Test epoch: 3, bce: 0.699654, acc: 0.625000, auc: 0.400000, time: 1.035\n",
      "epoch 4, training bce: 0.499365, training acc: 0.835938\n",
      "Test epoch: 4, bce: 0.699387, acc: 0.625000, auc: 0.400000, time: 1.027\n",
      "epoch 5, training bce: 0.476177, training acc: 0.835938\n",
      "Test epoch: 5, bce: 0.698350, acc: 0.625000, auc: 0.400000, time: 1.025\n",
      "epoch 6, training bce: 0.455278, training acc: 0.835938\n",
      "Test epoch: 6, bce: 0.696843, acc: 0.625000, auc: 0.400000, time: 1.023\n",
      "epoch 7, training bce: 0.437257, training acc: 0.835938\n",
      "Test epoch: 7, bce: 0.695235, acc: 0.625000, auc: 0.400000, time: 1.051\n",
      "epoch 8, training bce: 0.421049, training acc: 0.835938\n",
      "Test epoch: 8, bce: 0.693439, acc: 0.625000, auc: 0.466667, time: 1.048\n",
      "epoch 9, training bce: 0.405892, training acc: 0.843750\n",
      "Test epoch: 9, bce: 0.691390, acc: 0.625000, auc: 0.466667, time: 1.060\n",
      "epoch 10, training bce: 0.390896, training acc: 0.843750\n",
      "Test epoch: 10, bce: 0.689431, acc: 0.625000, auc: 0.466667, time: 1.049\n",
      "[  3/  10]  Valid score: 0.466667\n",
      "epoch 1, training bce: 0.685062, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.692515, acc: 0.625000, auc: 0.400000, time: 1.048\n",
      "epoch 2, training bce: 0.589407, training acc: 0.728125\n",
      "Test epoch: 2, bce: 0.696883, acc: 0.625000, auc: 0.333333, time: 1.030\n",
      "epoch 3, training bce: 0.538563, training acc: 0.778125\n",
      "Test epoch: 3, bce: 0.698852, acc: 0.625000, auc: 0.266667, time: 1.030\n",
      "epoch 4, training bce: 0.505531, training acc: 0.785937\n",
      "Test epoch: 4, bce: 0.698700, acc: 0.625000, auc: 0.266667, time: 1.025\n",
      "epoch 5, training bce: 0.479987, training acc: 0.785937\n",
      "Test epoch: 5, bce: 0.697649, acc: 0.625000, auc: 0.266667, time: 1.027\n",
      "epoch 6, training bce: 0.458038, training acc: 0.785937\n",
      "Test epoch: 6, bce: 0.696471, acc: 0.625000, auc: 0.333333, time: 1.039\n",
      "epoch 7, training bce: 0.437140, training acc: 0.785937\n",
      "Test epoch: 7, bce: 0.695379, acc: 0.625000, auc: 0.333333, time: 1.053\n",
      "epoch 8, training bce: 0.417434, training acc: 0.793750\n",
      "Test epoch: 8, bce: 0.694513, acc: 0.625000, auc: 0.333333, time: 1.089\n",
      "epoch 9, training bce: 0.398634, training acc: 0.793750\n",
      "Test epoch: 9, bce: 0.694058, acc: 0.625000, auc: 0.333333, time: 1.056\n",
      "epoch 10, training bce: 0.380223, training acc: 0.851562\n",
      "Test epoch: 10, bce: 0.693940, acc: 0.625000, auc: 0.333333, time: 1.029\n",
      "[  4/  10]  Valid score: 0.400000\n",
      "epoch 1, training bce: 0.671843, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.695278, acc: 0.625000, auc: 0.533333, time: 1.032\n",
      "epoch 2, training bce: 0.574360, training acc: 0.820312\n",
      "Test epoch: 2, bce: 0.696172, acc: 0.625000, auc: 0.466667, time: 1.024\n",
      "epoch 3, training bce: 0.525289, training acc: 0.828125\n",
      "Test epoch: 3, bce: 0.696417, acc: 0.625000, auc: 0.400000, time: 1.021\n",
      "epoch 4, training bce: 0.493687, training acc: 0.828125\n",
      "Test epoch: 4, bce: 0.695907, acc: 0.625000, auc: 0.466667, time: 1.030\n",
      "epoch 5, training bce: 0.469749, training acc: 0.835938\n",
      "Test epoch: 5, bce: 0.694470, acc: 0.625000, auc: 0.466667, time: 1.040\n",
      "epoch 6, training bce: 0.449393, training acc: 0.843750\n",
      "Test epoch: 6, bce: 0.692833, acc: 0.625000, auc: 0.466667, time: 1.050\n",
      "epoch 7, training bce: 0.431623, training acc: 0.843750\n",
      "Test epoch: 7, bce: 0.691349, acc: 0.625000, auc: 0.466667, time: 1.030\n",
      "epoch 8, training bce: 0.415095, training acc: 0.843750\n",
      "Test epoch: 8, bce: 0.690042, acc: 0.625000, auc: 0.466667, time: 1.033\n",
      "epoch 9, training bce: 0.399126, training acc: 0.843750\n",
      "Test epoch: 9, bce: 0.688881, acc: 0.625000, auc: 0.466667, time: 1.038\n",
      "epoch 10, training bce: 0.383465, training acc: 0.843750\n",
      "Test epoch: 10, bce: 0.687591, acc: 0.625000, auc: 0.466667, time: 1.038\n",
      "[  5/  10]  Valid score: 0.533333\n",
      "epoch 1, training bce: 0.707073, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.692687, acc: 0.625000, auc: 0.266667, time: 1.041\n",
      "epoch 2, training bce: 0.612626, training acc: 0.704688\n",
      "Test epoch: 2, bce: 0.685667, acc: 0.625000, auc: 0.733333, time: 1.032\n",
      "epoch 3, training bce: 0.556742, training acc: 0.812500\n",
      "Test epoch: 3, bce: 0.675936, acc: 0.625000, auc: 0.733333, time: 1.046\n",
      "epoch 4, training bce: 0.523132, training acc: 0.820312\n",
      "Test epoch: 4, bce: 0.667536, acc: 0.625000, auc: 0.800000, time: 1.023\n",
      "epoch 5, training bce: 0.497501, training acc: 0.828125\n",
      "Test epoch: 5, bce: 0.660348, acc: 0.625000, auc: 0.800000, time: 1.039\n",
      "epoch 6, training bce: 0.477091, training acc: 0.828125\n",
      "Test epoch: 6, bce: 0.654000, acc: 0.625000, auc: 0.800000, time: 1.025\n",
      "epoch 7, training bce: 0.458355, training acc: 0.828125\n",
      "Test epoch: 7, bce: 0.648550, acc: 0.625000, auc: 0.800000, time: 1.045\n",
      "epoch 8, training bce: 0.440925, training acc: 0.828125\n",
      "Test epoch: 8, bce: 0.643400, acc: 0.625000, auc: 0.800000, time: 1.029\n",
      "epoch 9, training bce: 0.424753, training acc: 0.828125\n",
      "Test epoch: 9, bce: 0.638856, acc: 0.625000, auc: 0.800000, time: 1.024\n",
      "epoch 10, training bce: 0.409388, training acc: 0.828125\n",
      "Test epoch: 10, bce: 0.634697, acc: 0.625000, auc: 0.800000, time: 1.020\n",
      "[  6/  10]  Valid score: 0.800000\n",
      "epoch 1, training bce: 0.678534, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.705750, acc: 0.625000, auc: 0.733333, time: 1.059\n",
      "epoch 2, training bce: 0.573199, training acc: 0.770312\n",
      "Test epoch: 2, bce: 0.708916, acc: 0.625000, auc: 0.800000, time: 1.038\n",
      "epoch 3, training bce: 0.522134, training acc: 0.828125\n",
      "Test epoch: 3, bce: 0.709391, acc: 0.625000, auc: 0.800000, time: 1.035\n",
      "epoch 4, training bce: 0.492197, training acc: 0.828125\n",
      "Test epoch: 4, bce: 0.708511, acc: 0.625000, auc: 0.800000, time: 1.023\n",
      "epoch 5, training bce: 0.469090, training acc: 0.828125\n",
      "Test epoch: 5, bce: 0.706827, acc: 0.625000, auc: 0.800000, time: 1.024\n",
      "epoch 6, training bce: 0.448876, training acc: 0.835938\n",
      "Test epoch: 6, bce: 0.704773, acc: 0.625000, auc: 0.800000, time: 1.027\n",
      "epoch 7, training bce: 0.430859, training acc: 0.835938\n",
      "Test epoch: 7, bce: 0.702547, acc: 0.625000, auc: 0.800000, time: 1.026\n",
      "epoch 8, training bce: 0.414412, training acc: 0.843750\n",
      "Test epoch: 8, bce: 0.700123, acc: 0.625000, auc: 0.800000, time: 1.025\n",
      "epoch 9, training bce: 0.398744, training acc: 0.843750\n",
      "Test epoch: 9, bce: 0.697683, acc: 0.625000, auc: 0.733333, time: 1.041\n",
      "epoch 10, training bce: 0.383782, training acc: 0.843750\n",
      "Test epoch: 10, bce: 0.695416, acc: 0.625000, auc: 0.733333, time: 1.022\n",
      "[  7/  10]  Valid score: 0.800000\n",
      "epoch 1, training bce: 0.691029, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.681925, acc: 0.625000, auc: 1.000000, time: 1.043\n",
      "epoch 2, training bce: 0.580008, training acc: 0.728125\n",
      "Test epoch: 2, bce: 0.681489, acc: 0.625000, auc: 1.000000, time: 1.043\n",
      "epoch 3, training bce: 0.523446, training acc: 0.785937\n",
      "Test epoch: 3, bce: 0.679313, acc: 0.625000, auc: 1.000000, time: 1.043\n",
      "epoch 4, training bce: 0.489254, training acc: 0.785937\n",
      "Test epoch: 4, bce: 0.676146, acc: 0.625000, auc: 0.933333, time: 1.027\n",
      "epoch 5, training bce: 0.462315, training acc: 0.785937\n",
      "Test epoch: 5, bce: 0.672297, acc: 0.625000, auc: 0.933333, time: 1.042\n",
      "epoch 6, training bce: 0.439290, training acc: 0.793750\n",
      "Test epoch: 6, bce: 0.668049, acc: 0.625000, auc: 1.000000, time: 1.021\n",
      "epoch 7, training bce: 0.417753, training acc: 0.793750\n",
      "Test epoch: 7, bce: 0.663643, acc: 0.625000, auc: 1.000000, time: 1.025\n",
      "epoch 8, training bce: 0.397863, training acc: 0.793750\n",
      "Test epoch: 8, bce: 0.659184, acc: 0.625000, auc: 1.000000, time: 1.028\n",
      "epoch 9, training bce: 0.378580, training acc: 0.843750\n",
      "Test epoch: 9, bce: 0.654644, acc: 0.625000, auc: 1.000000, time: 1.030\n",
      "epoch 10, training bce: 0.359193, training acc: 0.843750\n",
      "Test epoch: 10, bce: 0.650089, acc: 0.625000, auc: 1.000000, time: 1.045\n",
      "[  8/  10]  Valid score: 1.000000\n",
      "epoch 1, training bce: 0.721612, training acc: 0.639062\n",
      "Test epoch: 1, bce: 0.712786, acc: 0.625000, auc: 0.533333, time: 1.025\n",
      "epoch 2, training bce: 0.610870, training acc: 0.785937\n",
      "Test epoch: 2, bce: 0.712033, acc: 0.750000, auc: 0.533333, time: 1.029\n",
      "epoch 3, training bce: 0.559065, training acc: 0.785937\n",
      "Test epoch: 3, bce: 0.709540, acc: 0.625000, auc: 0.533333, time: 1.024\n",
      "epoch 4, training bce: 0.526977, training acc: 0.785937\n",
      "Test epoch: 4, bce: 0.707605, acc: 0.625000, auc: 0.533333, time: 1.030\n",
      "epoch 5, training bce: 0.501426, training acc: 0.785937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test epoch: 5, bce: 0.706072, acc: 0.625000, auc: 0.600000, time: 1.025\n",
      "epoch 6, training bce: 0.479891, training acc: 0.843750\n",
      "Test epoch: 6, bce: 0.704196, acc: 0.625000, auc: 0.600000, time: 1.048\n",
      "epoch 7, training bce: 0.458970, training acc: 0.851562\n",
      "Test epoch: 7, bce: 0.702117, acc: 0.625000, auc: 0.600000, time: 1.030\n",
      "epoch 8, training bce: 0.439858, training acc: 0.859375\n",
      "Test epoch: 8, bce: 0.699883, acc: 0.625000, auc: 0.600000, time: 1.032\n",
      "epoch 9, training bce: 0.421297, training acc: 0.859375\n",
      "Test epoch: 9, bce: 0.697636, acc: 0.625000, auc: 0.600000, time: 1.024\n",
      "epoch 10, training bce: 0.403244, training acc: 0.859375\n",
      "Test epoch: 10, bce: 0.695708, acc: 0.625000, auc: 0.600000, time: 1.034\n",
      "[  9/  10]  Valid score: 0.600000\n",
      "epoch 1, training bce: 0.686372, training acc: 0.604688\n",
      "Test epoch: 1, bce: 0.673764, acc: 0.625000, auc: 1.000000, time: 1.058\n",
      "epoch 2, training bce: 0.582931, training acc: 0.693750\n",
      "Test epoch: 2, bce: 0.674986, acc: 0.625000, auc: 0.933333, time: 1.037\n",
      "epoch 3, training bce: 0.533970, training acc: 0.743750\n",
      "Test epoch: 3, bce: 0.675615, acc: 0.625000, auc: 0.866667, time: 1.026\n",
      "epoch 4, training bce: 0.501497, training acc: 0.751563\n",
      "Test epoch: 4, bce: 0.675698, acc: 0.625000, auc: 0.866667, time: 1.022\n",
      "epoch 5, training bce: 0.474965, training acc: 0.809375\n",
      "Test epoch: 5, bce: 0.674932, acc: 0.625000, auc: 0.800000, time: 1.029\n",
      "epoch 6, training bce: 0.451651, training acc: 0.809375\n",
      "Test epoch: 6, bce: 0.673413, acc: 0.625000, auc: 0.800000, time: 1.025\n",
      "epoch 7, training bce: 0.429656, training acc: 0.809375\n",
      "Test epoch: 7, bce: 0.671288, acc: 0.625000, auc: 0.800000, time: 1.024\n",
      "epoch 8, training bce: 0.408946, training acc: 0.859375\n",
      "Test epoch: 8, bce: 0.668495, acc: 0.625000, auc: 0.800000, time: 1.025\n",
      "epoch 9, training bce: 0.389308, training acc: 0.859375\n",
      "Test epoch: 9, bce: 0.665442, acc: 0.625000, auc: 0.800000, time: 1.022\n",
      "epoch 10, training bce: 0.370408, training acc: 0.867188\n",
      "Test epoch: 10, bce: 0.662294, acc: 0.625000, auc: 0.800000, time: 1.026\n",
      "[ 10/  10]  Valid score: 1.000000\n",
      "=====parameter별 valid, test score=====\n",
      "Validation 평균: 0.680000\n",
      "epoch 1, training bce: 0.691455, training acc: 0.638021\n",
      "Test epoch: 1, bce: 0.698401, acc: 0.555556, auc: 0.400000, time: 1.027\n",
      "epoch 2, training bce: 0.579880, training acc: 0.717014\n",
      "Test epoch: 2, bce: 0.698499, acc: 0.555556, auc: 0.350000, time: 1.025\n",
      "epoch 3, training bce: 0.531354, training acc: 0.796007\n",
      "Test epoch: 3, bce: 0.699378, acc: 0.555556, auc: 0.350000, time: 1.025\n",
      "epoch 4, training bce: 0.497831, training acc: 0.796007\n",
      "Test epoch: 4, bce: 0.699963, acc: 0.555556, auc: 0.450000, time: 1.022\n",
      "epoch 5, training bce: 0.471619, training acc: 0.796007\n",
      "Test epoch: 5, bce: 0.700409, acc: 0.555556, auc: 0.500000, time: 1.027\n",
      "epoch 6, training bce: 0.449017, training acc: 0.796007\n",
      "Test epoch: 6, bce: 0.700658, acc: 0.555556, auc: 0.500000, time: 1.023\n",
      "epoch 7, training bce: 0.429283, training acc: 0.796007\n",
      "Test epoch: 7, bce: 0.700804, acc: 0.555556, auc: 0.500000, time: 1.029\n",
      "epoch 8, training bce: 0.410930, training acc: 0.851562\n",
      "Test epoch: 8, bce: 0.701094, acc: 0.555556, auc: 0.450000, time: 1.064\n",
      "epoch 9, training bce: 0.393173, training acc: 0.851562\n",
      "Test epoch: 9, bce: 0.701182, acc: 0.555556, auc: 0.500000, time: 1.027\n",
      "epoch 10, training bce: 0.376011, training acc: 0.851562\n",
      "Test epoch: 10, bce: 0.701006, acc: 0.555556, auc: 0.500000, time: 1.031\n",
      "[  1/  10]  Valid score: 0.500000\n",
      "epoch 1, training bce: 0.697158, training acc: 0.677951\n",
      "Test epoch: 1, bce: 0.703022, acc: 0.555556, auc: 0.450000, time: 1.073\n",
      "epoch 2, training bce: 0.572631, training acc: 0.835938\n",
      "Test epoch: 2, bce: 0.705230, acc: 0.555556, auc: 0.300000, time: 1.044\n",
      "epoch 3, training bce: 0.511351, training acc: 0.843750\n",
      "Test epoch: 3, bce: 0.705727, acc: 0.555556, auc: 0.300000, time: 1.029\n",
      "epoch 4, training bce: 0.477871, training acc: 0.851562\n",
      "Test epoch: 4, bce: 0.704986, acc: 0.555556, auc: 0.300000, time: 1.028\n",
      "epoch 5, training bce: 0.450812, training acc: 0.851562\n",
      "Test epoch: 5, bce: 0.703995, acc: 0.555556, auc: 0.350000, time: 1.034\n",
      "epoch 6, training bce: 0.427262, training acc: 0.851562\n",
      "Test epoch: 6, bce: 0.703472, acc: 0.555556, auc: 0.400000, time: 1.035\n",
      "epoch 7, training bce: 0.405938, training acc: 0.851562\n",
      "Test epoch: 7, bce: 0.703362, acc: 0.555556, auc: 0.400000, time: 1.024\n",
      "epoch 8, training bce: 0.386847, training acc: 0.851562\n",
      "Test epoch: 8, bce: 0.703540, acc: 0.555556, auc: 0.400000, time: 1.031\n",
      "epoch 9, training bce: 0.369396, training acc: 0.851562\n",
      "Test epoch: 9, bce: 0.703750, acc: 0.555556, auc: 0.400000, time: 1.030\n",
      "epoch 10, training bce: 0.353108, training acc: 0.859375\n",
      "Test epoch: 10, bce: 0.703893, acc: 0.555556, auc: 0.400000, time: 1.033\n",
      "[  2/  10]  Valid score: 0.450000\n",
      "epoch 1, training bce: 0.678250, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.689699, acc: 0.625000, auc: 0.533333, time: 1.028\n",
      "epoch 2, training bce: 0.577657, training acc: 0.820312\n",
      "Test epoch: 2, bce: 0.700476, acc: 0.625000, auc: 0.533333, time: 1.028\n",
      "epoch 3, training bce: 0.534791, training acc: 0.835938\n",
      "Test epoch: 3, bce: 0.703527, acc: 0.625000, auc: 0.533333, time: 1.025\n",
      "epoch 4, training bce: 0.508582, training acc: 0.835938\n",
      "Test epoch: 4, bce: 0.704972, acc: 0.625000, auc: 0.533333, time: 1.034\n",
      "epoch 5, training bce: 0.486507, training acc: 0.835938\n",
      "Test epoch: 5, bce: 0.704424, acc: 0.625000, auc: 0.533333, time: 1.023\n",
      "epoch 6, training bce: 0.466341, training acc: 0.835938\n",
      "Test epoch: 6, bce: 0.703117, acc: 0.625000, auc: 0.533333, time: 1.051\n",
      "epoch 7, training bce: 0.448566, training acc: 0.835938\n",
      "Test epoch: 7, bce: 0.701411, acc: 0.625000, auc: 0.533333, time: 1.021\n",
      "epoch 8, training bce: 0.431492, training acc: 0.835938\n",
      "Test epoch: 8, bce: 0.699613, acc: 0.625000, auc: 0.533333, time: 1.041\n",
      "epoch 9, training bce: 0.415532, training acc: 0.835938\n",
      "Test epoch: 9, bce: 0.697716, acc: 0.625000, auc: 0.533333, time: 1.031\n",
      "epoch 10, training bce: 0.400565, training acc: 0.835938\n",
      "Test epoch: 10, bce: 0.695332, acc: 0.625000, auc: 0.533333, time: 1.051\n",
      "[  3/  10]  Valid score: 0.533333\n",
      "epoch 1, training bce: 0.667122, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.693595, acc: 0.625000, auc: 0.200000, time: 1.024\n",
      "epoch 2, training bce: 0.570735, training acc: 0.728125\n",
      "Test epoch: 2, bce: 0.698486, acc: 0.625000, auc: 0.133333, time: 1.028\n",
      "epoch 3, training bce: 0.513039, training acc: 0.785937\n",
      "Test epoch: 3, bce: 0.699768, acc: 0.625000, auc: 0.066667, time: 1.029\n",
      "epoch 4, training bce: 0.478407, training acc: 0.785937\n",
      "Test epoch: 4, bce: 0.700034, acc: 0.625000, auc: 0.066667, time: 1.034\n",
      "epoch 5, training bce: 0.450235, training acc: 0.793750\n",
      "Test epoch: 5, bce: 0.699783, acc: 0.625000, auc: 0.133333, time: 1.028\n",
      "epoch 6, training bce: 0.426661, training acc: 0.793750\n",
      "Test epoch: 6, bce: 0.699292, acc: 0.625000, auc: 0.133333, time: 1.026\n",
      "epoch 7, training bce: 0.403746, training acc: 0.793750\n",
      "Test epoch: 7, bce: 0.698624, acc: 0.625000, auc: 0.133333, time: 1.024\n",
      "epoch 8, training bce: 0.382148, training acc: 0.851562\n",
      "Test epoch: 8, bce: 0.697779, acc: 0.625000, auc: 0.133333, time: 1.029\n",
      "epoch 9, training bce: 0.361107, training acc: 0.851562\n",
      "Test epoch: 9, bce: 0.696983, acc: 0.625000, auc: 0.133333, time: 1.027\n",
      "epoch 10, training bce: 0.340243, training acc: 0.859375\n",
      "Test epoch: 10, bce: 0.696158, acc: 0.625000, auc: 0.133333, time: 1.023\n",
      "[  4/  10]  Valid score: 0.200000\n",
      "epoch 1, training bce: 0.661421, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.687586, acc: 0.625000, auc: 0.600000, time: 1.026\n",
      "epoch 2, training bce: 0.564272, training acc: 0.770312\n",
      "Test epoch: 2, bce: 0.687906, acc: 0.625000, auc: 0.533333, time: 1.029\n",
      "epoch 3, training bce: 0.511621, training acc: 0.828125\n",
      "Test epoch: 3, bce: 0.688088, acc: 0.625000, auc: 0.533333, time: 1.043\n",
      "epoch 4, training bce: 0.480153, training acc: 0.828125\n",
      "Test epoch: 4, bce: 0.688270, acc: 0.625000, auc: 0.533333, time: 1.030\n",
      "epoch 5, training bce: 0.456746, training acc: 0.828125\n",
      "Test epoch: 5, bce: 0.687826, acc: 0.625000, auc: 0.533333, time: 1.019\n",
      "epoch 6, training bce: 0.437242, training acc: 0.835938\n",
      "Test epoch: 6, bce: 0.687042, acc: 0.625000, auc: 0.533333, time: 1.023\n",
      "epoch 7, training bce: 0.419881, training acc: 0.843750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test epoch: 7, bce: 0.686272, acc: 0.625000, auc: 0.533333, time: 1.028\n",
      "epoch 8, training bce: 0.403352, training acc: 0.843750\n",
      "Test epoch: 8, bce: 0.685378, acc: 0.625000, auc: 0.466667, time: 1.032\n",
      "epoch 9, training bce: 0.387214, training acc: 0.843750\n",
      "Test epoch: 9, bce: 0.684474, acc: 0.625000, auc: 0.466667, time: 1.030\n",
      "epoch 10, training bce: 0.371412, training acc: 0.843750\n",
      "Test epoch: 10, bce: 0.683520, acc: 0.625000, auc: 0.466667, time: 1.016\n",
      "[  5/  10]  Valid score: 0.600000\n",
      "epoch 1, training bce: 0.684014, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.667835, acc: 0.625000, auc: 0.733333, time: 1.056\n",
      "epoch 2, training bce: 0.587098, training acc: 0.754687\n",
      "Test epoch: 2, bce: 0.662310, acc: 0.625000, auc: 0.666667, time: 1.031\n",
      "epoch 3, training bce: 0.540872, training acc: 0.820312\n",
      "Test epoch: 3, bce: 0.656856, acc: 0.625000, auc: 0.666667, time: 1.020\n",
      "epoch 4, training bce: 0.510859, training acc: 0.820312\n",
      "Test epoch: 4, bce: 0.652098, acc: 0.625000, auc: 0.800000, time: 1.325\n",
      "epoch 5, training bce: 0.484696, training acc: 0.820312\n",
      "Test epoch: 5, bce: 0.647423, acc: 0.625000, auc: 0.800000, time: 1.048\n",
      "epoch 6, training bce: 0.461738, training acc: 0.828125\n",
      "Test epoch: 6, bce: 0.642597, acc: 0.625000, auc: 0.800000, time: 1.030\n",
      "epoch 7, training bce: 0.441269, training acc: 0.828125\n",
      "Test epoch: 7, bce: 0.637508, acc: 0.625000, auc: 0.800000, time: 1.049\n",
      "epoch 8, training bce: 0.421945, training acc: 0.828125\n",
      "Test epoch: 8, bce: 0.632286, acc: 0.625000, auc: 0.800000, time: 1.028\n",
      "epoch 9, training bce: 0.404003, training acc: 0.828125\n",
      "Test epoch: 9, bce: 0.627656, acc: 0.625000, auc: 0.800000, time: 1.026\n",
      "epoch 10, training bce: 0.387056, training acc: 0.828125\n",
      "Test epoch: 10, bce: 0.623922, acc: 0.625000, auc: 0.800000, time: 1.024\n",
      "[  6/  10]  Valid score: 0.800000\n",
      "epoch 1, training bce: 0.672525, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.694933, acc: 0.625000, auc: 0.533333, time: 1.030\n",
      "epoch 2, training bce: 0.578074, training acc: 0.770312\n",
      "Test epoch: 2, bce: 0.700377, acc: 0.625000, auc: 0.600000, time: 1.052\n",
      "epoch 3, training bce: 0.528198, training acc: 0.828125\n",
      "Test epoch: 3, bce: 0.702551, acc: 0.625000, auc: 0.733333, time: 1.034\n",
      "epoch 4, training bce: 0.499624, training acc: 0.828125\n",
      "Test epoch: 4, bce: 0.703150, acc: 0.625000, auc: 0.800000, time: 1.033\n",
      "epoch 5, training bce: 0.477249, training acc: 0.835938\n",
      "Test epoch: 5, bce: 0.702376, acc: 0.625000, auc: 0.800000, time: 1.043\n",
      "epoch 6, training bce: 0.458226, training acc: 0.835938\n",
      "Test epoch: 6, bce: 0.701054, acc: 0.625000, auc: 0.800000, time: 1.025\n",
      "epoch 7, training bce: 0.441605, training acc: 0.835938\n",
      "Test epoch: 7, bce: 0.699360, acc: 0.625000, auc: 0.800000, time: 1.024\n",
      "epoch 8, training bce: 0.426372, training acc: 0.835938\n",
      "Test epoch: 8, bce: 0.697508, acc: 0.625000, auc: 0.800000, time: 1.030\n",
      "epoch 9, training bce: 0.411219, training acc: 0.843750\n",
      "Test epoch: 9, bce: 0.695552, acc: 0.625000, auc: 0.800000, time: 1.031\n",
      "epoch 10, training bce: 0.396291, training acc: 0.843750\n",
      "Test epoch: 10, bce: 0.693501, acc: 0.625000, auc: 0.733333, time: 1.036\n",
      "[  7/  10]  Valid score: 0.800000\n",
      "epoch 1, training bce: 0.686674, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.691441, acc: 0.625000, auc: 0.866667, time: 1.023\n",
      "epoch 2, training bce: 0.582059, training acc: 0.785937\n",
      "Test epoch: 2, bce: 0.691427, acc: 0.625000, auc: 0.800000, time: 1.029\n",
      "epoch 3, training bce: 0.535746, training acc: 0.785937\n",
      "Test epoch: 3, bce: 0.689063, acc: 0.625000, auc: 0.800000, time: 1.027\n",
      "epoch 4, training bce: 0.502488, training acc: 0.793750\n",
      "Test epoch: 4, bce: 0.685174, acc: 0.625000, auc: 0.866667, time: 1.024\n",
      "epoch 5, training bce: 0.475390, training acc: 0.793750\n",
      "Test epoch: 5, bce: 0.680432, acc: 0.625000, auc: 0.866667, time: 1.033\n",
      "epoch 6, training bce: 0.452273, training acc: 0.793750\n",
      "Test epoch: 6, bce: 0.675322, acc: 0.625000, auc: 0.866667, time: 1.085\n",
      "epoch 7, training bce: 0.430209, training acc: 0.793750\n",
      "Test epoch: 7, bce: 0.670181, acc: 0.625000, auc: 0.866667, time: 1.046\n",
      "epoch 8, training bce: 0.409021, training acc: 0.843750\n",
      "Test epoch: 8, bce: 0.665417, acc: 0.625000, auc: 0.866667, time: 1.025\n",
      "epoch 9, training bce: 0.388794, training acc: 0.843750\n",
      "Test epoch: 9, bce: 0.660745, acc: 0.625000, auc: 0.933333, time: 1.049\n",
      "epoch 10, training bce: 0.368996, training acc: 0.851562\n",
      "Test epoch: 10, bce: 0.656034, acc: 0.625000, auc: 0.933333, time: 1.043\n",
      "[  8/  10]  Valid score: 0.933333\n",
      "epoch 1, training bce: 0.688746, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.709204, acc: 0.625000, auc: 0.400000, time: 1.025\n",
      "epoch 2, training bce: 0.600883, training acc: 0.720313\n",
      "Test epoch: 2, bce: 0.699399, acc: 0.625000, auc: 0.533333, time: 1.055\n",
      "epoch 3, training bce: 0.551334, training acc: 0.778125\n",
      "Test epoch: 3, bce: 0.696190, acc: 0.750000, auc: 0.533333, time: 1.030\n",
      "epoch 4, training bce: 0.519207, training acc: 0.785937\n",
      "Test epoch: 4, bce: 0.693186, acc: 0.750000, auc: 0.533333, time: 1.050\n",
      "epoch 5, training bce: 0.493386, training acc: 0.785937\n",
      "Test epoch: 5, bce: 0.690546, acc: 0.750000, auc: 0.533333, time: 1.045\n",
      "epoch 6, training bce: 0.471116, training acc: 0.785937\n",
      "Test epoch: 6, bce: 0.687976, acc: 0.750000, auc: 0.533333, time: 1.023\n",
      "epoch 7, training bce: 0.450780, training acc: 0.793750\n",
      "Test epoch: 7, bce: 0.685168, acc: 0.750000, auc: 0.533333, time: 1.044\n",
      "epoch 8, training bce: 0.431289, training acc: 0.793750\n",
      "Test epoch: 8, bce: 0.682309, acc: 0.750000, auc: 0.533333, time: 1.026\n",
      "epoch 9, training bce: 0.412325, training acc: 0.801562\n",
      "Test epoch: 9, bce: 0.679661, acc: 0.750000, auc: 0.600000, time: 1.037\n",
      "epoch 10, training bce: 0.393743, training acc: 0.851562\n",
      "Test epoch: 10, bce: 0.677228, acc: 0.750000, auc: 0.600000, time: 1.045\n",
      "[  9/  10]  Valid score: 0.600000\n",
      "epoch 1, training bce: 0.698717, training acc: 0.604688\n",
      "Test epoch: 1, bce: 0.671739, acc: 0.625000, auc: 1.000000, time: 1.045\n",
      "epoch 2, training bce: 0.589999, training acc: 0.678125\n",
      "Test epoch: 2, bce: 0.672227, acc: 0.625000, auc: 1.000000, time: 1.022\n",
      "epoch 3, training bce: 0.530023, training acc: 0.751563\n",
      "Test epoch: 3, bce: 0.671493, acc: 0.625000, auc: 1.000000, time: 1.037\n",
      "epoch 4, training bce: 0.491809, training acc: 0.801562\n",
      "Test epoch: 4, bce: 0.670739, acc: 0.625000, auc: 1.000000, time: 1.062\n",
      "epoch 5, training bce: 0.461026, training acc: 0.809375\n",
      "Test epoch: 5, bce: 0.669484, acc: 0.625000, auc: 0.866667, time: 1.021\n",
      "epoch 6, training bce: 0.434596, training acc: 0.809375\n",
      "Test epoch: 6, bce: 0.667996, acc: 0.625000, auc: 0.866667, time: 1.027\n",
      "epoch 7, training bce: 0.410994, training acc: 0.867188\n",
      "Test epoch: 7, bce: 0.666264, acc: 0.625000, auc: 0.866667, time: 1.019\n",
      "epoch 8, training bce: 0.389274, training acc: 0.867188\n",
      "Test epoch: 8, bce: 0.664411, acc: 0.625000, auc: 0.800000, time: 1.023\n",
      "epoch 9, training bce: 0.368243, training acc: 0.867188\n",
      "Test epoch: 9, bce: 0.662538, acc: 0.625000, auc: 0.800000, time: 1.025\n",
      "epoch 10, training bce: 0.347587, training acc: 0.882812\n",
      "Test epoch: 10, bce: 0.660378, acc: 0.625000, auc: 0.800000, time: 1.021\n",
      "[ 10/  10]  Valid score: 1.000000\n",
      "=====parameter별 valid, test score=====\n",
      "Validation 평균: 0.641667\n",
      "-------------------------------Testing Begining-------------------------------\n",
      "Max hy: (4, 0.02, 0.01)\n",
      "epoch 1, training bce: 0.671959, training acc: 0.638021\n",
      "Test epoch: 1, bce: 0.725428, acc: 0.555556, auc: 0.550000, time: 1.029\n",
      "epoch 2, training bce: 0.600586, training acc: 0.709201\n",
      "Test epoch: 2, bce: 0.719191, acc: 0.555556, auc: 0.600000, time: 1.081\n",
      "epoch 3, training bce: 0.554740, training acc: 0.709201\n",
      "Test epoch: 3, bce: 0.743988, acc: 0.555556, auc: 0.550000, time: 1.025\n",
      "epoch 4, training bce: 0.511221, training acc: 0.717014\n",
      "Test epoch: 4, bce: 0.747302, acc: 0.555556, auc: 0.550000, time: 1.026\n",
      "epoch 5, training bce: 0.473563, training acc: 0.717014\n",
      "Test epoch: 5, bce: 0.760825, acc: 0.555556, auc: 0.550000, time: 1.024\n",
      "epoch 6, training bce: 0.438000, training acc: 0.724826\n",
      "Test epoch: 6, bce: 0.784734, acc: 0.555556, auc: 0.550000, time: 1.027\n",
      "epoch 7, training bce: 0.402006, training acc: 0.788194\n",
      "Test epoch: 7, bce: 0.822938, acc: 0.555556, auc: 0.500000, time: 1.027\n",
      "epoch 8, training bce: 0.366386, training acc: 0.796007\n",
      "Test epoch: 8, bce: 0.878049, acc: 0.555556, auc: 0.450000, time: 1.074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, training bce: 0.328558, training acc: 0.796007\n",
      "Test epoch: 9, bce: 0.949665, acc: 0.666667, auc: 0.450000, time: 1.034\n",
      "epoch 10, training bce: 0.288979, training acc: 0.851562\n",
      "Test epoch: 10, bce: 1.022000, acc: 0.666667, auc: 0.450000, time: 1.033\n",
      "Loading Weights from Epoch: 2\n",
      "[  1/  10]  Valid score: 0.600000\n",
      "epoch 1, training bce: 0.665744, training acc: 0.685764\n",
      "Test epoch: 1, bce: 0.692763, acc: 0.555556, auc: 0.550000, time: 1.026\n",
      "epoch 2, training bce: 0.591536, training acc: 0.693576\n",
      "Test epoch: 2, bce: 0.702136, acc: 0.555556, auc: 0.450000, time: 1.023\n",
      "epoch 3, training bce: 0.544391, training acc: 0.756944\n",
      "Test epoch: 3, bce: 0.705078, acc: 0.555556, auc: 0.400000, time: 1.055\n",
      "epoch 4, training bce: 0.510336, training acc: 0.756944\n",
      "Test epoch: 4, bce: 0.707114, acc: 0.555556, auc: 0.300000, time: 1.045\n",
      "epoch 5, training bce: 0.481070, training acc: 0.756944\n",
      "Test epoch: 5, bce: 0.719890, acc: 0.555556, auc: 0.300000, time: 1.027\n",
      "epoch 6, training bce: 0.452610, training acc: 0.756944\n",
      "Test epoch: 6, bce: 0.766297, acc: 0.555556, auc: 0.400000, time: 1.027\n",
      "epoch 7, training bce: 0.424137, training acc: 0.764757\n",
      "Test epoch: 7, bce: 0.830509, acc: 0.555556, auc: 0.400000, time: 1.036\n",
      "epoch 8, training bce: 0.396312, training acc: 0.764757\n",
      "Test epoch: 8, bce: 0.894911, acc: 0.555556, auc: 0.400000, time: 1.051\n",
      "epoch 9, training bce: 0.367943, training acc: 0.835938\n",
      "Test epoch: 9, bce: 0.951878, acc: 0.555556, auc: 0.400000, time: 1.031\n",
      "epoch 10, training bce: 0.337708, training acc: 0.843750\n",
      "Test epoch: 10, bce: 0.999479, acc: 0.555556, auc: 0.400000, time: 1.055\n",
      "Loading Weights from Epoch: 1\n",
      "[  2/  10]  Valid score: 0.550000\n",
      "epoch 1, training bce: 0.697528, training acc: 0.712500\n",
      "Test epoch: 1, bce: 1.476255, acc: 0.375000, auc: 0.400000, time: 1.024\n",
      "epoch 2, training bce: 0.626297, training acc: 0.754687\n",
      "Test epoch: 2, bce: 1.342382, acc: 0.500000, auc: 0.333333, time: 1.021\n",
      "epoch 3, training bce: 0.580325, training acc: 0.812500\n",
      "Test epoch: 3, bce: 1.118738, acc: 0.500000, auc: 0.200000, time: 1.025\n",
      "epoch 4, training bce: 0.553549, training acc: 0.812500\n",
      "Test epoch: 4, bce: 0.956307, acc: 0.375000, auc: 0.200000, time: 1.021\n",
      "epoch 5, training bce: 0.529779, training acc: 0.812500\n",
      "Test epoch: 5, bce: 0.877805, acc: 0.375000, auc: 0.200000, time: 1.020\n",
      "epoch 6, training bce: 0.509032, training acc: 0.812500\n",
      "Test epoch: 6, bce: 0.845495, acc: 0.375000, auc: 0.200000, time: 1.054\n",
      "epoch 7, training bce: 0.489628, training acc: 0.812500\n",
      "Test epoch: 7, bce: 0.829636, acc: 0.375000, auc: 0.400000, time: 1.021\n",
      "epoch 8, training bce: 0.469604, training acc: 0.812500\n",
      "Test epoch: 8, bce: 0.821582, acc: 0.375000, auc: 0.400000, time: 1.025\n",
      "epoch 9, training bce: 0.449728, training acc: 0.812500\n",
      "Test epoch: 9, bce: 0.815670, acc: 0.500000, auc: 0.400000, time: 1.024\n",
      "epoch 10, training bce: 0.430128, training acc: 0.812500\n",
      "Test epoch: 10, bce: 0.804037, acc: 0.500000, auc: 0.400000, time: 1.022\n",
      "Loading Weights from Epoch: 1\n",
      "[  3/  10]  Valid score: 0.400000\n",
      "epoch 1, training bce: 0.701373, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.946337, acc: 0.500000, auc: 0.533333, time: 1.036\n",
      "epoch 2, training bce: 0.639431, training acc: 0.646875\n",
      "Test epoch: 2, bce: 0.909154, acc: 0.375000, auc: 0.266667, time: 1.033\n",
      "epoch 3, training bce: 0.589605, training acc: 0.654687\n",
      "Test epoch: 3, bce: 0.838564, acc: 0.625000, auc: 0.266667, time: 1.022\n",
      "epoch 4, training bce: 0.547719, training acc: 0.654687\n",
      "Test epoch: 4, bce: 0.816098, acc: 0.500000, auc: 0.266667, time: 1.022\n",
      "epoch 5, training bce: 0.507814, training acc: 0.662500\n",
      "Test epoch: 5, bce: 0.819112, acc: 0.625000, auc: 0.066667, time: 1.044\n",
      "epoch 6, training bce: 0.470434, training acc: 0.662500\n",
      "Test epoch: 6, bce: 0.847136, acc: 0.625000, auc: 0.000000, time: 1.024\n",
      "epoch 7, training bce: 0.436063, training acc: 0.670312\n",
      "Test epoch: 7, bce: 0.888378, acc: 0.625000, auc: 0.000000, time: 1.052\n",
      "epoch 8, training bce: 0.402827, training acc: 0.778125\n",
      "Test epoch: 8, bce: 0.933458, acc: 0.625000, auc: 0.000000, time: 1.038\n",
      "epoch 9, training bce: 0.367636, training acc: 0.778125\n",
      "Test epoch: 9, bce: 0.982075, acc: 0.625000, auc: 0.000000, time: 1.017\n",
      "epoch 10, training bce: 0.330165, training acc: 0.778125\n",
      "Test epoch: 10, bce: 1.033571, acc: 0.625000, auc: 0.000000, time: 1.022\n",
      "Loading Weights from Epoch: 1\n",
      "[  4/  10]  Valid score: 0.533333\n",
      "epoch 1, training bce: 0.687305, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.702166, acc: 0.500000, auc: 0.666667, time: 1.025\n",
      "epoch 2, training bce: 0.630991, training acc: 0.689063\n",
      "Test epoch: 2, bce: 0.701059, acc: 0.500000, auc: 0.666667, time: 1.024\n",
      "epoch 3, training bce: 0.586102, training acc: 0.712500\n",
      "Test epoch: 3, bce: 0.708999, acc: 0.500000, auc: 0.533333, time: 1.028\n",
      "epoch 4, training bce: 0.552692, training acc: 0.762500\n",
      "Test epoch: 4, bce: 0.718108, acc: 0.500000, auc: 0.533333, time: 1.027\n",
      "epoch 5, training bce: 0.520653, training acc: 0.812500\n",
      "Test epoch: 5, bce: 0.729628, acc: 0.500000, auc: 0.533333, time: 1.024\n",
      "epoch 6, training bce: 0.490969, training acc: 0.812500\n",
      "Test epoch: 6, bce: 0.748212, acc: 0.500000, auc: 0.533333, time: 1.075\n",
      "epoch 7, training bce: 0.462359, training acc: 0.820312\n",
      "Test epoch: 7, bce: 0.773846, acc: 0.500000, auc: 0.533333, time: 1.042\n",
      "epoch 8, training bce: 0.434652, training acc: 0.828125\n",
      "Test epoch: 8, bce: 0.800596, acc: 0.500000, auc: 0.466667, time: 1.024\n",
      "epoch 9, training bce: 0.408038, training acc: 0.828125\n",
      "Test epoch: 9, bce: 0.821664, acc: 0.500000, auc: 0.466667, time: 1.029\n",
      "epoch 10, training bce: 0.381681, training acc: 0.828125\n",
      "Test epoch: 10, bce: 0.830751, acc: 0.500000, auc: 0.466667, time: 1.032\n",
      "Loading Weights from Epoch: 1\n",
      "[  5/  10]  Valid score: 0.666667\n",
      "epoch 1, training bce: 0.692099, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.707218, acc: 0.500000, auc: 0.733333, time: 1.051\n",
      "epoch 2, training bce: 0.622303, training acc: 0.696875\n",
      "Test epoch: 2, bce: 0.614162, acc: 0.875000, auc: 0.733333, time: 1.058\n",
      "epoch 3, training bce: 0.574569, training acc: 0.696875\n",
      "Test epoch: 3, bce: 0.555069, acc: 0.875000, auc: 0.800000, time: 1.027\n",
      "epoch 4, training bce: 0.539962, training acc: 0.696875\n",
      "Test epoch: 4, bce: 0.513079, acc: 0.875000, auc: 0.800000, time: 1.027\n",
      "epoch 5, training bce: 0.507141, training acc: 0.696875\n",
      "Test epoch: 5, bce: 0.477688, acc: 0.875000, auc: 0.866667, time: 1.025\n",
      "epoch 6, training bce: 0.476668, training acc: 0.696875\n",
      "Test epoch: 6, bce: 0.446777, acc: 0.875000, auc: 0.866667, time: 1.022\n",
      "epoch 7, training bce: 0.447554, training acc: 0.796875\n",
      "Test epoch: 7, bce: 0.425761, acc: 0.750000, auc: 0.866667, time: 1.042\n",
      "epoch 8, training bce: 0.419913, training acc: 0.796875\n",
      "Test epoch: 8, bce: 0.412770, acc: 0.750000, auc: 0.866667, time: 1.026\n",
      "epoch 9, training bce: 0.392527, training acc: 0.812500\n",
      "Test epoch: 9, bce: 0.405412, acc: 0.750000, auc: 0.866667, time: 1.031\n",
      "epoch 10, training bce: 0.362524, training acc: 0.820312\n",
      "Test epoch: 10, bce: 0.404260, acc: 0.750000, auc: 0.866667, time: 1.046\n",
      "Loading Weights from Epoch: 5\n",
      "[  6/  10]  Valid score: 0.866667\n",
      "epoch 1, training bce: 0.679744, training acc: 0.689063\n",
      "Test epoch: 1, bce: 0.660218, acc: 0.625000, auc: 0.600000, time: 1.043\n",
      "epoch 2, training bce: 0.621397, training acc: 0.689063\n",
      "Test epoch: 2, bce: 0.673022, acc: 0.625000, auc: 0.466667, time: 1.021\n",
      "epoch 3, training bce: 0.571967, training acc: 0.689063\n",
      "Test epoch: 3, bce: 0.684143, acc: 0.625000, auc: 0.466667, time: 1.021\n",
      "epoch 4, training bce: 0.538467, training acc: 0.689063\n",
      "Test epoch: 4, bce: 0.673174, acc: 0.625000, auc: 0.600000, time: 1.021\n",
      "epoch 5, training bce: 0.508794, training acc: 0.696875\n",
      "Test epoch: 5, bce: 0.658340, acc: 0.625000, auc: 0.666667, time: 1.019\n",
      "epoch 6, training bce: 0.483218, training acc: 0.712500\n",
      "Test epoch: 6, bce: 0.641360, acc: 0.625000, auc: 0.733333, time: 1.024\n",
      "epoch 7, training bce: 0.460315, training acc: 0.762500\n",
      "Test epoch: 7, bce: 0.629265, acc: 0.625000, auc: 0.733333, time: 1.025\n",
      "epoch 8, training bce: 0.438839, training acc: 0.762500\n",
      "Test epoch: 8, bce: 0.624328, acc: 0.500000, auc: 0.733333, time: 1.022\n",
      "epoch 9, training bce: 0.418072, training acc: 0.762500\n",
      "Test epoch: 9, bce: 0.626138, acc: 0.500000, auc: 0.733333, time: 1.018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, training bce: 0.397175, training acc: 0.812500\n",
      "Test epoch: 10, bce: 0.632880, acc: 0.500000, auc: 0.733333, time: 1.027\n",
      "Loading Weights from Epoch: 6\n",
      "[  7/  10]  Valid score: 0.733333\n",
      "epoch 1, training bce: 0.692782, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.620750, acc: 0.625000, auc: 0.800000, time: 1.026\n",
      "epoch 2, training bce: 0.631102, training acc: 0.646875\n",
      "Test epoch: 2, bce: 0.620479, acc: 0.625000, auc: 0.866667, time: 1.031\n",
      "epoch 3, training bce: 0.570728, training acc: 0.662500\n",
      "Test epoch: 3, bce: 0.623240, acc: 0.625000, auc: 0.933333, time: 1.027\n",
      "epoch 4, training bce: 0.531605, training acc: 0.662500\n",
      "Test epoch: 4, bce: 0.614555, acc: 0.625000, auc: 0.800000, time: 1.045\n",
      "epoch 5, training bce: 0.498581, training acc: 0.662500\n",
      "Test epoch: 5, bce: 0.596366, acc: 0.625000, auc: 0.733333, time: 1.029\n",
      "epoch 6, training bce: 0.469090, training acc: 0.720313\n",
      "Test epoch: 6, bce: 0.576345, acc: 0.625000, auc: 0.866667, time: 1.025\n",
      "epoch 7, training bce: 0.439926, training acc: 0.720313\n",
      "Test epoch: 7, bce: 0.557463, acc: 0.625000, auc: 0.800000, time: 1.025\n",
      "epoch 8, training bce: 0.410158, training acc: 0.720313\n",
      "Test epoch: 8, bce: 0.540245, acc: 0.625000, auc: 0.866667, time: 1.036\n",
      "epoch 9, training bce: 0.377158, training acc: 0.778125\n",
      "Test epoch: 9, bce: 0.522347, acc: 0.625000, auc: 0.866667, time: 1.037\n",
      "epoch 10, training bce: 0.341099, training acc: 0.778125\n",
      "Test epoch: 10, bce: 0.496962, acc: 0.625000, auc: 0.933333, time: 1.062\n",
      "Loading Weights from Epoch: 3\n",
      "[  8/  10]  Valid score: 0.933333\n",
      "epoch 1, training bce: 0.682630, training acc: 0.646875\n",
      "Test epoch: 1, bce: 0.891402, acc: 0.625000, auc: 0.600000, time: 1.623\n",
      "epoch 2, training bce: 0.628626, training acc: 0.654687\n",
      "Test epoch: 2, bce: 0.842776, acc: 0.500000, auc: 0.533333, time: 1.036\n",
      "epoch 3, training bce: 0.580177, training acc: 0.662500\n",
      "Test epoch: 3, bce: 0.769424, acc: 0.625000, auc: 0.533333, time: 1.044\n",
      "epoch 4, training bce: 0.546681, training acc: 0.662500\n",
      "Test epoch: 4, bce: 0.720391, acc: 0.625000, auc: 0.533333, time: 1.027\n",
      "epoch 5, training bce: 0.518333, training acc: 0.662500\n",
      "Test epoch: 5, bce: 0.686565, acc: 0.625000, auc: 0.533333, time: 1.087\n",
      "epoch 6, training bce: 0.492231, training acc: 0.712500\n",
      "Test epoch: 6, bce: 0.674025, acc: 0.625000, auc: 0.533333, time: 1.032\n",
      "epoch 7, training bce: 0.466584, training acc: 0.720313\n",
      "Test epoch: 7, bce: 0.676662, acc: 0.625000, auc: 0.600000, time: 1.024\n",
      "epoch 8, training bce: 0.439842, training acc: 0.770312\n",
      "Test epoch: 8, bce: 0.688488, acc: 0.625000, auc: 0.600000, time: 1.066\n",
      "epoch 9, training bce: 0.411318, training acc: 0.770312\n",
      "Test epoch: 9, bce: 0.699717, acc: 0.625000, auc: 0.600000, time: 1.032\n",
      "epoch 10, training bce: 0.379608, training acc: 0.770312\n",
      "Test epoch: 10, bce: 0.710459, acc: 0.625000, auc: 0.600000, time: 1.044\n",
      "Loading Weights from Epoch: 1\n",
      "[  9/  10]  Valid score: 0.600000\n",
      "epoch 1, training bce: 0.690895, training acc: 0.604688\n",
      "Test epoch: 1, bce: 0.683282, acc: 0.750000, auc: 0.866667, time: 1.047\n",
      "epoch 2, training bce: 0.636279, training acc: 0.662500\n",
      "Test epoch: 2, bce: 0.637240, acc: 0.625000, auc: 0.866667, time: 1.024\n",
      "epoch 3, training bce: 0.591795, training acc: 0.662500\n",
      "Test epoch: 3, bce: 0.608175, acc: 0.750000, auc: 0.866667, time: 1.023\n",
      "epoch 4, training bce: 0.554188, training acc: 0.678125\n",
      "Test epoch: 4, bce: 0.584574, acc: 0.750000, auc: 0.866667, time: 1.034\n",
      "epoch 5, training bce: 0.514914, training acc: 0.685937\n",
      "Test epoch: 5, bce: 0.557474, acc: 0.750000, auc: 0.866667, time: 1.026\n",
      "epoch 6, training bce: 0.474845, training acc: 0.685937\n",
      "Test epoch: 6, bce: 0.533826, acc: 0.750000, auc: 0.866667, time: 1.043\n",
      "epoch 7, training bce: 0.432998, training acc: 0.701562\n",
      "Test epoch: 7, bce: 0.517580, acc: 0.750000, auc: 0.866667, time: 1.038\n",
      "epoch 8, training bce: 0.388842, training acc: 0.801562\n",
      "Test epoch: 8, bce: 0.505760, acc: 0.750000, auc: 0.866667, time: 1.023\n",
      "epoch 9, training bce: 0.343610, training acc: 0.801562\n",
      "Test epoch: 9, bce: 0.494563, acc: 0.750000, auc: 0.866667, time: 1.036\n",
      "epoch 10, training bce: 0.297062, training acc: 0.859375\n",
      "Test epoch: 10, bce: 0.484453, acc: 0.625000, auc: 0.866667, time: 1.053\n",
      "Loading Weights from Epoch: 1\n",
      "[ 10/  10]  Valid score: 0.866667\n",
      "Validation 평균: 0.675000\n"
     ]
    }
   ],
   "source": [
    "addiction = model(train_data_processed, NUM_FOLDS, Num_feat, features_all, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e38e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
